<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>王子泰</title>
  
  <subtitle>哭也欢乐，悲也潇洒</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://wang22ti.com/"/>
  <updated>2018-11-22T13:06:43.543Z</updated>
  <id>http://wang22ti.com/</id>
  
  <author>
    <name>wang22ti</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>论文阅读-Neural_Collaborative_Filtering</title>
    <link href="http://wang22ti.com/2018/11/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Neural-Collaborative-Filtering/"/>
    <id>http://wang22ti.com/2018/11/11/论文阅读-Neural-Collaborative-Filtering/</id>
    <published>2018-11-11T02:38:10.000Z</published>
    <updated>2018-11-22T13:06:43.543Z</updated>
    
    <content type="html"><![CDATA[<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><h2 id="直接"><a href="#直接" class="headerlink" title="直接"></a>直接</h2><p><a href="https://arxiv.org/pdf/1708.05031.pdf" target="_blank" rel="noopener">Neural Collaborative Filtering</a></p><p><a href="https://www.comp.nus.edu.sg/~xiangnan/" target="_blank" rel="noopener">Xiangnan He’s Homwpage</a></p><h2 id="间接"><a href="#间接" class="headerlink" title="间接"></a>间接</h2><p><a href="https://www.cnblogs.com/HolyShine/p/6728999.html" target="_blank" rel="noopener">【翻译】Neural Collaborative Filtering—神经协同过滤</a></p><p><a href="https://www.cnblogs.com/wzyj/p/8974782.html" target="_blank" rel="noopener">【推荐系统】neural_collaborative_filtering（源码解析）</a></p><p><a href="https://zhuanlan.zhihu.com/p/31122667" target="_blank" rel="noopener">Neural Collaborative Filtering文章概要</a></p><p><a href="https://www.cnblogs.com/baihuaxiu/p/6617389.html" target="_blank" rel="noopener">[机器学习]推荐系统之协同过滤算法</a></p><p><a href="https://blog.csdn.net/yinruiyang94/article/details/78906370" target="_blank" rel="noopener">implicit feedback</a></p><p><a href="https://baike.baidu.com/item/Jaccard%E7%B3%BB%E6%95%B0/6784913?fr=aladdin" target="_blank" rel="noopener">Jaccard系数</a></p><p><a href="https://blog.csdn.net/u011926899/article/details/52669180" target="_blank" rel="noopener">NDCG评价指标讲解</a></p><h1 id="关键问题"><a href="#关键问题" class="headerlink" title="关键问题"></a>关键问题</h1><ol><li><p>文章简述？</p><ul><li>证明MF具有局限性</li><li>提出NCF的基本结构</li><li>提出NCF的具体实现——GMF、MLP以及由它们融合的NeuMF</li><li>实验与结论，包括预训练的技巧</li></ul></li><li><p>文章研究的对象?</p><p>以implicit feedback为基础的协同过滤</p><blockquote><p>which indirectly reflects users’ preference through behaviours like watching videos, purchasing products and clicking items. Compared to explicit feedback (i.e., ratings and reviews), implicit feedback can be tracked automatically and is thus much easier to collect for content providers. However, it is more challenging to utilize, since user satisfaction is not observed and there is a natural scarcity of negative feedback. In this paper, we explore the central theme of how to utilize DNNs to model noisy implicit feedback signals.</p></blockquote></li><li><p>MF的局限性？或者说为什么要用神经网络代替MF？</p><blockquote><p>Despite the effectiveness of MF for collaborative filtering, it is well-known that its performance can be hindered by the simple choice of the interaction function — inner product. </p></blockquote></li><li><p>为什么神经网络的引入可以克服MF的局限性？</p><blockquote><p>The neural network has been proven to be capable of approximating any continuous function</p></blockquote></li><li><p>NCF的结构？</p><p><img src="/2018/11/11/论文阅读-Neural-Collaborative-Filtering/../../../博士/方向探索/推荐算法/神经协同过滤/对于神经协同网络几个关键问题的思考/1.png" alt=""></p><p>数学的表达为：</p><script type="math/tex; mode=display">\hat { y } _ { u i } = f \left( \mathbf { P } ^ { T } \mathbf { v } _ { u } ^ { U } , \mathbf { Q } ^ { T } \mathbf { v } _ { i } ^ { I } | \mathbf { P } , \mathbf { Q } , \Theta _ { f } \right)</script></li></ol><blockquote><p> where $\mathbf { P } \in \mathbb { R } ^ { M \times K } \text { and } \mathbf { Q } \in \mathbb { R } ^ { N \times K }$, denoting the latent factor matrix for users and items, respectively; and  $\Theta _ { f }$ denotes the model parameters of the interaction function $f$. Since the function $f$ is defined as a multi-layer neural network, it can be formulated as</p></blockquote><script type="math/tex; mode=display">   f \left( \mathbf { P } ^ { T } \mathbf { v } _ { u } ^ { U } , \mathbf { Q } ^ { T } \mathbf { v } _ { i } ^ { I } \right) = \phi _ { o u t } \left( \phi _ { X } \left( \ldots \phi _ { 2 } \left( \mathbf { P } ^ { T } \mathbf { v } _ { u } ^ { U } , \mathbf { Q } ^ { T } \mathbf { v } _ { i } ^ { I } \right) \right) \ldots \right) )</script><blockquote><p>where $\phi _ { o u t } $ and $ \phi _ { x }$ respectively denote the mapping function for the output layer and $x$-th neural collaborative filtering (CF) layer, and there are $X$ neural $CF$ layers in total.</p></blockquote><ol><li><p>为什么在NCF的损失函数是交叉熵，或者说如何处理implicit sample？</p><blockquote><p>While the squared loss can be explained by assuming that observations are generated from a Gaussian distribution, we point out that it may not tally well with implicit data. This is because for implicit data, the target value $y_{ui}$ is a binarized 1 or 0 denoting whether u has interacted with i.</p><p>Considering the one-class nature of implicit feedback, wecan view the value of $y_{ui}$ as a label — 1 means item $i$ is relevant to u, and $0$ otherwise. The prediction score $\hat{y}_{ui}$ then represents how likely $i$ is relevant to $u$. To endow NCF with such a probabilistic explanation, we need to constrain the output $\hat{y}_{ui}$ in the range of $[0, 1]$, which can be easily achieved by using a probabilistic function (e.g., the Logistic or Probit function) as the activation function for the output layer $φ_{out}$.</p></blockquote></li><li><p>为什么说GMF是Generated MF？或者为什么说MF是GMF的一个特例？</p><p>在MF中有</p><script type="math/tex; mode=display">\phi _ { 1 } \left( \mathbf { p } _ { u } , \mathbf { q } _ { i } \right) = \mathbf { p } _ { u } \odot \mathbf { q } _ { i }</script><p>而在GMF可以写成</p><script type="math/tex; mode=display">\hat { y } _ { u i } = a _ { o u t } \left( \mathbf { h } ^ { T } \left( \mathbf { p } _ { u } \odot \mathbf { q } _ { i } \right) \right)</script><p>因此有</p><blockquote><p>if we use an identity function for $a_{out}$ and enforce $h$ to be a uniform vector of 1, we can exactly recover the MF model.</p></blockquote></li><li><p>为什么要将GMF和MLP融合？</p><blockquote><p>It unifies the strengths of linearity of MF and non-linearity of MLP for modelling the user–item latent structures.</p></blockquote></li><li><p>为什么要预训练？</p><blockquote><p>Due to the non-convexity of the objective function of NeuMF可能的进一步工作？</p></blockquote></li><li><p>实验结果？</p><ul><li>pre-trained NeuMF &gt; NeuMF &gt; MLP &gt; GMF &gt; others</li><li>更深的网络是有作用的</li></ul></li><li><p>进一步的工作？</p><blockquote><p> While a nonuniform sampling strategy (e.g., item popularity-biased [14, 12]) might further improve the performance, we leave the exploration as a future work.</p><p>In future, we will study pairwise learners for NCF models and extend NCF to model auxiliary information, such as user reviews [11], knowledge bases [45], and temporal signals [1]. While existing personalization models have primarily focused on individuals, it is interesting to develop models for groups of users, which help the decision-making for social groups [15, 42]. Moreover, we are particularly interested in building recommender systems for multi-media items, an interesting task but has received relatively less scrutiny in the recommendation community [3]. Multi-media items, such as images and videos, contain much richer visual semantics [16, 41] that can reflect users’ interest. To build a multi-media recommender system, we need to develop effective methods to learn from multi-view and multi-modal data [13, 40]. Another emerging direction is to explore the potential of recurrent neural networks and hashing method</p></blockquote></li></ol><h1 id="实验复现"><a href="#实验复现" class="headerlink" title="实验复现"></a>实验复现</h1><p>首先是git一波</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/hexiangnan/neural_collaborative_filtering.git</span><br></pre></td></tr></table></figure><p>看一下Read Me，环境用的是</p><blockquote><ul><li>Keras version:  ‘1.0.7’</li><li>Theano version: ‘0.8.0’</li></ul></blockquote><p>于是用anaconda新建一个环境，就叫ncf吧，结果运行起来就报了个错：</p><blockquote><p> ValueError: Only call sigmoid_cross_entropy_with_logits with named arguments (labels=…, logits=…, …)</p></blockquote><p>原因是它并没有告诉我安装什么版本的tensorflow，，还好issue里有<a href="https://github.com/hexiangnan/neural_collaborative_filtering/issues/15" target="_blank" rel="noopener">解答</a>（我的环境是tensorflow=1.8.0, python=3.6）。接着还会报一个</p><blockquote><p>ImportError: cannot import name ‘control_flow_ops’</p></blockquote><p>的错误，还好还是有<a href="https://github.com/tensorflow/tensorflow/issues/7020" target="_blank" rel="noopener">大神</a>能搞定。</p><p>至此，GMF.py终于跑起来了，然而跑MLP.py的时候又报了新的错：</p><blockquote><p>TypeError: Expected int32, got list containing Tensors of type ‘_Message’ instead.</p></blockquote><p>emmmm，还是新的tensorflow的api出现的了问题，<a href="https://github.com/carpedm20/DCGAN-tensorflow/issues/99" target="_blank" rel="noopener">大神</a>已经指出了直接调用tensorflow的情况。在keras中，还需要根据报错路径修改文件<code>tensorflow_backend.py</code>中的<code>concatenate</code>函数：</p><blockquote><p>from </p><p><code>return tf.concat(as, tensors)</code></p><p>to</p><p><code>return tf.concat(tensors, ax)</code></p></blockquote><p>实际上，有这样问题的不止一处，但是这样已经跑起来了！</p><p>NeuMF.py是上面两个网络的综合，所以并没有出现新的错误。</p><p>事实证明，应该先看一下issue，用python2.7的，，，</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;参考文献&quot;&gt;&lt;a href=&quot;#参考文献&quot; class=&quot;headerlink&quot; title=&quot;参考文献&quot;&gt;&lt;/a&gt;参考文献&lt;/h1&gt;&lt;h2 id=&quot;直接&quot;&gt;&lt;a href=&quot;#直接&quot; class=&quot;headerlink&quot; title=&quot;直接&quot;&gt;&lt;/a&gt;直接&lt;/h
      
    
    </summary>
    
      <category term="推荐算法" scheme="http://wang22ti.com/categories/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>《模式识别》实验6-BP神经网络</title>
    <link href="http://wang22ti.com/2018/11/11/%E3%80%8A%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E3%80%8B%E5%AE%9E%E9%AA%8C6-BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://wang22ti.com/2018/11/11/《模式识别》实验6-BP神经网络/</id>
    <published>2018-11-11T02:29:21.000Z</published>
    <updated>2018-11-13T02:57:14.927Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实验目的"><a href="#实验目的" class="headerlink" title="实验目的"></a>实验目的</h1><ol><li><p>了解人工神经网络的生物学基础</p></li><li><p>掌握神经网络网络的前向传播和反向传播的数学原理</p></li><li><p>用matlab实现一个简单的全连接神经网络</p></li></ol><h1 id="实验原理"><a href="#实验原理" class="headerlink" title="实验原理"></a>实验原理</h1><p>​        可以参见我的个人博客wang22ti.com，其中博文《<a href="http://wang22ti.com/2018/07/31/mooc-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%B8%88-1-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">mooc-深度学习工程师-1-神经网络和深度学习</a>》对神经网络的数学原理有详细的介绍。</p><p>​        MATLAB软件包含MATLAB神经网络工具箱。它是以人工神经网络理论为基础，用MATLAB语言构造出了该理论所涉及的公式运算、矩阵操作和方程求解等大部分子程序以用于神经网络的设计和训练。用户只需要根据自己的需要调用相关的子程序，即可以完成包括网络结构设计、权值初始化、网络训练及结果输出等在内的一系列工作，免除编写复杂庞大程序的困扰。BP神经网络主要用到newff，sim和train3个神经网络函数：</p><ol><li><p>net=Newff (P, T, S, TF, BTF, BLF, PF, IPF, OPE, DDE)</p><p>创建一个BP神经网络，其中P为输入数据矩阵；T为输出数据矩阵；S为隐含层节点的数组；TF为节点的激活函数，比如logsig、purelin；BTF为训练函数，比如动态反向传播和动态自适应学习率的梯度下降算法traingdx；BLF为网络学习函数，比如BP学习规则learned。一般手动设置前6个参数，后面4个采用系统默认。</p></li><li><p>[net,tr]=train(NET, X, T, Pi, Ai)</p><p>用训练数据训练BP神经网络，其中NET为待训练网络；X为输入数据矩阵；T为输出数据矩阵；Pi为初始化输入层条件；Ai为初始化输出层条件；net为训练好的神经网络；tr为训练过程记录。一般手动设置前面3个参数。</p></li><li><p>y=sim(net, x)</p><p>用训练好的BP神经网络预测函数输出。其中net是训练好的神经网络；x为输入数据；y为网络预测数据。</p></li></ol><h1 id="实验内容与步骤"><a href="#实验内容与步骤" class="headerlink" title="实验内容与步骤"></a>实验内容与步骤</h1><p>​        有一批Iris花，已知这批Iris花可分为3个品种，现需要对其进行分类。不同品种的Iris花的花萼长度、花萼宽度、花瓣长度、花瓣宽度会有差异。我们现有一批已知品种的Iris花的花萼长度、花萼宽度、花瓣长度、花瓣宽度的数据。用已有的数据训练一个神经网络用作分类器。实验代码如下所示，前者是训练代码，后者是测试代码。</p><h2 id="train"><a href="#train" class="headerlink" title="train"></a>train</h2><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%将Iris数据集分为2组，每组各75个样本，每组中每种花各有25个样本。</span></span><br><span class="line"><span class="comment">%其中一组作为以上程序的训练样本，另外一组作为检验样本。</span></span><br><span class="line"><span class="comment">%为了方便训练，将3类花分别编号为1，2，3 。</span></span><br><span class="line"><span class="comment">%使用这些数据训练一个4输入（分别对应4个特征），3输出（分别对应该样本属于某一品种的可能性大小）的前向网络。</span></span><br><span class="line"></span><br><span class="line">clear all;</span><br><span class="line"><span class="comment">%读取训练数据</span></span><br><span class="line">[f1,f2,f3,f4,class] = textread(<span class="string">'trainData.txt'</span>,<span class="string">'%f%f%f%f%s'</span>,<span class="number">75</span>);</span><br><span class="line"><span class="comment">%特征值归一化</span></span><br><span class="line"><span class="comment">%构造输出矩阵</span></span><br><span class="line">[input,minI,maxI] = premnmx( [f1 , f2 , f3 , f4 ]')  ;</span><br><span class="line">s=<span class="built_in">length</span>(class);</span><br><span class="line">output=<span class="built_in">zeros</span>(s,<span class="number">3</span>);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : s </span><br><span class="line">    <span class="keyword">if</span> strcmp(class(<span class="built_in">i</span>),<span class="string">'Isetosa'</span>)</span><br><span class="line">        output( <span class="built_in">i</span> , <span class="number">1</span>) = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">elseif</span> strcmp(class(<span class="built_in">i</span>),<span class="string">'Iversicolor'</span>)</span><br><span class="line">        output( <span class="built_in">i</span> , <span class="number">2</span>) = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">else</span> </span><br><span class="line">        output( <span class="built_in">i</span> , <span class="number">3</span> ) = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="comment">%创建神经网络</span></span><br><span class="line">net = newff(minmax(input), [<span class="number">10</span>, <span class="number">3</span>], &#123;<span class="string">'logsig'</span>, <span class="string">'purelin'</span>&#125;, <span class="string">'traingdx'</span>) ; </span><br><span class="line"><span class="comment">%设置训练参数</span></span><br><span class="line">net.trainparam.show = <span class="number">50</span> ;          <span class="comment">%显示中间结果的周期；</span></span><br><span class="line">net.trainparam.epochs = <span class="number">1000</span> ;     <span class="comment">%最大的迭代次数；</span></span><br><span class="line">net.trainparam.goal = <span class="number">0.01</span>;          <span class="comment">%神经网络的训练的目标误差；</span></span><br><span class="line">net.trainParam.lr = <span class="number">0.005</span> ;           <span class="comment">%学习率；</span></span><br><span class="line"><span class="comment">%开始训练 </span></span><br><span class="line">[net, tr] = train( net, input , output' ) ;</span><br></pre></td></tr></table></figure><h2 id="test"><a href="#test" class="headerlink" title="test"></a>test</h2><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%读取测试数据</span></span><br><span class="line">[t1, t2, t3, t4, c] = textread(<span class="string">'testData.txt'</span> , <span class="string">'%f%f%f%f%s'</span>,<span class="number">75</span>);</span><br><span class="line"><span class="comment">%测试数据归一化</span></span><br><span class="line">testInput = tramnmx ( [t1,t2,t3,t4]' , minI, maxI ) ;</span><br><span class="line"><span class="comment">%仿真</span></span><br><span class="line">Y = sim( net , testInput );</span><br><span class="line"><span class="comment">%补充程序4</span></span><br><span class="line">s=<span class="built_in">length</span>(c);</span><br><span class="line">output1=<span class="built_in">zeros</span>(s,<span class="number">3</span>);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : s </span><br><span class="line">    <span class="keyword">if</span> strcmp(c(<span class="built_in">i</span>),<span class="string">'Isetosa'</span>)</span><br><span class="line">        output1( <span class="built_in">i</span> , <span class="number">1</span> ) = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">elseif</span> strcmp(c(<span class="built_in">i</span>),<span class="string">'Iversicolor'</span>)</span><br><span class="line">        output1( <span class="built_in">i</span> , <span class="number">2</span>  ) = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">else</span> </span><br><span class="line">        output1( <span class="built_in">i</span> , <span class="number">3</span>  ) = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="comment">%统计识别正确率</span></span><br><span class="line">[s1 , s2] = <span class="built_in">size</span>( Y ) ;</span><br><span class="line">hitNum = <span class="number">0</span> ;</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : s2</span><br><span class="line">    [m , Index] = max( Y( : ,  <span class="built_in">i</span> ) ) ;</span><br><span class="line">    <span class="keyword">if</span>( output1(<span class="built_in">i</span>,Index) == <span class="number">1</span>   ) </span><br><span class="line">        hitNum = hitNum + <span class="number">1</span> ; </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">sprintf(<span class="string">'识别率是 %3.3f%%'</span>,<span class="number">100</span> * hitNum / s2 )</span><br></pre></td></tr></table></figure><h1 id="实验结果与讨论"><a href="#实验结果与讨论" class="headerlink" title="实验结果与讨论"></a>实验结果与讨论</h1><p>​        运行训练代码会出现如下图所示的UI，可以实时查看神经网络的结构，训练过程中的数据，比如代数、时间、梯度、损失等等，十分简单易用。</p><p><img src="/2018/11/11/《模式识别》实验6-BP神经网络/《模式识别》实验5-最近邻与k近邻/1.png" alt=""></p><p>运行测试代码，得到在测试集上的识别率为96.000%，还是一个比较理想的结果。</p><p><img src="/2018/11/11/《模式识别》实验6-BP神经网络/《模式识别》实验5-最近邻与k近邻/2.png" alt=""></p><h1 id="实验总结"><a href="#实验总结" class="headerlink" title="实验总结"></a>实验总结</h1><p>​        本次实验聚焦于神经网络的原理分析与设计。神经网络是当前模式识别领域最火热的方法，在计算机视觉、自然语言处理等领域每天都有新的神经网络模型被提出。此前我一直使用python和tensorflow，这是我第一次使用matlab搭建神经网络，取得了意想不到的收获。相信这次实验对于我今后的科研工作会带来很大的帮助。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;实验目的&quot;&gt;&lt;a href=&quot;#实验目的&quot; class=&quot;headerlink&quot; title=&quot;实验目的&quot;&gt;&lt;/a&gt;实验目的&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;了解人工神经网络的生物学基础&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;掌握神经网络网络的前向传播和反向传播的数学
      
    
    </summary>
    
      <category term="模式识别" scheme="http://wang22ti.com/categories/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>《模式识别》实验5-最近邻与k近邻</title>
    <link href="http://wang22ti.com/2018/11/11/%E3%80%8A%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E3%80%8B%E5%AE%9E%E9%AA%8C5-%E6%9C%80%E8%BF%91%E9%82%BB%E4%B8%8Ek%E8%BF%91%E9%82%BB/"/>
    <id>http://wang22ti.com/2018/11/11/《模式识别》实验5-最近邻与k近邻/</id>
    <published>2018-11-11T02:28:47.000Z</published>
    <updated>2018-11-13T02:53:19.650Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实验目的"><a href="#实验目的" class="headerlink" title="实验目的"></a>实验目的</h1><ol><li><p>掌握最近邻分类算法。</p></li><li><p>掌握k近邻分类算法。</p></li></ol><h1 id="实验原理"><a href="#实验原理" class="headerlink" title="实验原理"></a>实验原理</h1><ol><li>最近邻法将与测试样本最近邻样本的类别作为决策的结果，对于一个C类的问题，每类有$N_i$个样本，则第$i$类$W_i$的判别函数为</li></ol><script type="math/tex; mode=display">g _ { i } ( x ) = \min _ { k } \left\| x - x _ { i } ^ { k } \right\| , k = 1 , \dots , N _ { i }</script><p>​        其中$| \cdot |$表示某种距离的度量，通常使用欧式距离。</p><ol><li>K近邻法是最近邻法的扩展，其基本规则是在所有$N$个样本中找到与测试样本的$k$个最近邻者，其中各类别所占个数表示为$\mathrm { k } _ { \mathrm { i } } , i = 1 , \dots , c$，定义判别函数为：</li></ol><script type="math/tex; mode=display">\mathrm { g } _ { \mathrm { i } } ( x ) = k _ { i } , i = 1,2 , \ldots , c</script><p>​    决策规则为</p><script type="math/tex; mode=display">\mathrm { j } = \operatorname { argmax } _ { i } g _ { i } ( x ) , i = 1 , \ldots , c</script><p>​        其中$k$一般采用奇数，避免因为票数相等而难以决策</p><h1 id="实验内容与步骤"><a href="#实验内容与步骤" class="headerlink" title="实验内容与步骤"></a>实验内容与步骤</h1><ol><li><p>每类产生50个样本作为训练样本；每类产生100个样本作为考试样本</p></li><li><p>按最近邻法用训练样本对考试样本分类，计算平均错误率</p></li><li><p>按最k近邻法（k=21）用训练样本对考试样本分类，计算平均错误率</p></li><li><p>标注错分点。</p></li></ol><h2 id="最近邻"><a href="#最近邻" class="headerlink" title="最近邻"></a>最近邻</h2><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% 最近邻</span></span><br><span class="line">clear all;</span><br><span class="line">close all;</span><br><span class="line">clc;</span><br><span class="line">warning off</span><br><span class="line">mu1=[<span class="number">-2</span> <span class="number">-2</span>];    <span class="comment">%均值向量</span></span><br><span class="line">mu2=[<span class="number">2</span> <span class="number">2</span>];</span><br><span class="line">sigma1=[<span class="number">1</span> <span class="number">0</span>;<span class="number">0</span> <span class="number">1</span>];    <span class="comment">%协方差矩阵</span></span><br><span class="line">sigma2=[<span class="number">1</span> <span class="number">0</span>;<span class="number">0</span> <span class="number">4</span>];</span><br><span class="line">N1=<span class="number">50</span>;  <span class="comment">%每一类50个训练样本</span></span><br><span class="line">N2=<span class="number">100</span>; <span class="comment">%每一类100个考试样本</span></span><br><span class="line">train1=mvnrnd(mu1,sigma1,N1);   <span class="comment">%产生多维正态随机数</span></span><br><span class="line">train2=mvnrnd(mu2,sigma2,N1);</span><br><span class="line">test1=mvnrnd(mu1,sigma1,N2);</span><br><span class="line">test2=mvnrnd(mu2,sigma2,N2);</span><br><span class="line">train=[train1;train2];  <span class="comment">%训练样本，前50行为第一类，后50行为第二类</span></span><br><span class="line">dist1=<span class="built_in">zeros</span>(N2,<span class="number">2</span>*N1); <span class="comment">%考试点到第一类训练点的距离矩阵</span></span><br><span class="line">dist2=<span class="built_in">zeros</span>(N2,<span class="number">2</span>*N1); <span class="comment">%考试点到第二类训练点的距离矩阵</span></span><br><span class="line">t1=<span class="number">1</span>;</span><br><span class="line">t2=<span class="number">1</span>;</span><br><span class="line">err_index1=[];</span><br><span class="line">err_index2=[];</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:N2 <span class="comment">%遍历所有考试点</span></span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span>=<span class="number">1</span>:<span class="number">2</span>*N1 <span class="comment">%遍历所有训练点</span></span><br><span class="line">        <span class="comment">%补充程序1  （第一类）考试点到所有训练点的距离</span></span><br><span class="line">        dist1(<span class="built_in">i</span>, <span class="built_in">j</span>) = ((test1(<span class="built_in">i</span>, <span class="number">1</span>) - train(<span class="built_in">j</span>, <span class="number">1</span>))^<span class="number">2</span> + (test1(<span class="built_in">i</span>, <span class="number">2</span>) - train(<span class="built_in">j</span>, <span class="number">2</span>))^<span class="number">2</span>)^<span class="number">0.5</span>;</span><br><span class="line">        <span class="comment">%补充程序2  （第二类）考试点到所有训练点的距离</span></span><br><span class="line">        dist2(<span class="built_in">i</span>, <span class="built_in">j</span>) = ((test2(<span class="built_in">i</span>, <span class="number">1</span>) - train(<span class="built_in">j</span>, <span class="number">1</span>))^<span class="number">2</span> + (test2(<span class="built_in">i</span>, <span class="number">2</span>) - train(<span class="built_in">j</span>, <span class="number">2</span>))^<span class="number">2</span>)^<span class="number">0.5</span>;       </span><br><span class="line">    <span class="keyword">end</span> <span class="comment">%计算每个考试点到所有训练点的欧式距离</span></span><br><span class="line">    col1=<span class="built_in">find</span>(dist1(<span class="built_in">i</span>,:)==min(dist1(<span class="built_in">i</span>,:)));   <span class="comment">%发现（第一类）考试点到两类训练点距离最小值的列号</span></span><br><span class="line">    <span class="keyword">if</span> col1&gt;N1<span class="comment">%补充程序3 （第一类）考试点到第二类训练点的距离最小</span></span><br><span class="line">        err_index1(t1)=<span class="built_in">i</span>;   <span class="comment">%（第一类）考试点错分为第二类的索引</span></span><br><span class="line">        t1=t1+<span class="number">1</span>;    <span class="comment">%（第一类）考试点错分数加1</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    col2=<span class="built_in">find</span>(dist2(<span class="built_in">i</span>,:)==min(dist2(<span class="built_in">i</span>,:)));   <span class="comment">%发现（第二类）考试点到两类训练点距离最小值的列号</span></span><br><span class="line">    <span class="keyword">if</span> col2&lt;=N1<span class="comment">%补充程序4（第二类）考试点到第一类训练点的距离最小</span></span><br><span class="line">        err_index2(t2)=<span class="built_in">i</span>;   <span class="comment">%（第二类）考试点错分为第一类的索引</span></span><br><span class="line">        t2=t2+<span class="number">1</span>;    <span class="comment">%（第二类）考试点错分数加1</span></span><br><span class="line">    <span class="keyword">end</span> <span class="comment">%找到距离该考试点最近的训练点，判断是否错分</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">pe1=<span class="built_in">size</span>(err_index1,<span class="number">2</span>)/N2;</span><br><span class="line">pe2=<span class="built_in">size</span>(err_index2,<span class="number">2</span>)/N2;</span><br><span class="line">pe=<span class="number">0.5</span>*(pe1+pe2); <span class="comment">%计算错误率</span></span><br><span class="line">figure(<span class="number">1</span>)</span><br><span class="line">hold on</span><br><span class="line">grid on</span><br><span class="line">plot(train1(:,<span class="number">1</span>),train1(:,<span class="number">2</span>),<span class="string">'*y'</span>);</span><br><span class="line">plot(train2(:,<span class="number">1</span>),train2(:,<span class="number">2</span>),<span class="string">'+g'</span>);</span><br><span class="line">plot(test1(:,<span class="number">1</span>),test1(:,<span class="number">2</span>),<span class="string">'*r'</span>);</span><br><span class="line">plot(test2(:,<span class="number">1</span>),test2(:,<span class="number">2</span>),<span class="string">'+b'</span>);</span><br><span class="line">plot(test1(err_index1,<span class="number">1</span>),test1(err_index1,<span class="number">2</span>),<span class="string">'or'</span>,<span class="string">'MarkerSize'</span>,<span class="number">12</span>);</span><br><span class="line">plot(test2(err_index2,<span class="number">1</span>),test2(err_index2,<span class="number">2</span>),<span class="string">'sb'</span>,<span class="string">'MarkerSize'</span>,<span class="number">12</span>);</span><br></pre></td></tr></table></figure><h2 id="k近邻"><a href="#k近邻" class="headerlink" title="k近邻"></a>k近邻</h2><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% K近邻</span></span><br><span class="line">clear all;</span><br><span class="line">close all;</span><br><span class="line">clear all;</span><br><span class="line">close all;</span><br><span class="line">clc;</span><br><span class="line">warning off</span><br><span class="line">mu1=[<span class="number">-2</span> <span class="number">-2</span>];    <span class="comment">%均值向量</span></span><br><span class="line">mu2=[<span class="number">2</span> <span class="number">2</span>];</span><br><span class="line">sigma1=[<span class="number">1</span> <span class="number">0</span>;<span class="number">0</span> <span class="number">1</span>];    <span class="comment">%协方差矩阵</span></span><br><span class="line">sigma2=[<span class="number">1</span> <span class="number">0</span>;<span class="number">0</span> <span class="number">4</span>];</span><br><span class="line">N1=<span class="number">50</span>;  <span class="comment">%每一类50个训练样本</span></span><br><span class="line">N2=<span class="number">100</span>; <span class="comment">%每一类100个考试样本</span></span><br><span class="line">train1=mvnrnd(mu1,sigma1,N1);   <span class="comment">%产生多维正态随机数</span></span><br><span class="line">train2=mvnrnd(mu2,sigma2,N1);</span><br><span class="line">test1=mvnrnd(mu1,sigma1,N2);</span><br><span class="line">test2=mvnrnd(mu2,sigma2,N2);</span><br><span class="line">train=[train1;train2];  <span class="comment">%训练样本，前50行为第一类，后50行为第二类</span></span><br><span class="line">dist1=<span class="built_in">zeros</span>(N2,<span class="number">2</span>*N1); <span class="comment">%考试点到第一类训练点的距离矩阵</span></span><br><span class="line">dist2=<span class="built_in">zeros</span>(N2,<span class="number">2</span>*N1); <span class="comment">%考试点到第二类训练点的距离矩阵</span></span><br><span class="line">t1=<span class="number">1</span>;</span><br><span class="line">t2=<span class="number">1</span>;</span><br><span class="line">err_index1=[];</span><br><span class="line">err_index2=[];</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:N2  <span class="comment">%遍历所有考试点</span></span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span>=<span class="number">1</span>:<span class="number">2</span>*N1 <span class="comment">%遍历所有训练点</span></span><br><span class="line">        dist1(<span class="built_in">i</span>,<span class="built_in">j</span>)=<span class="built_in">sqrt</span>((test1(<span class="built_in">i</span>,<span class="number">1</span>)-train(<span class="built_in">j</span>,<span class="number">1</span>))^<span class="number">2</span>+(test1(<span class="built_in">i</span>,<span class="number">2</span>)-train(<span class="built_in">j</span>,<span class="number">2</span>))^<span class="number">2</span>);  <span class="comment">%（第一类）考试点到所有训练点的距离</span></span><br><span class="line">        dist2(<span class="built_in">i</span>,<span class="built_in">j</span>)=<span class="built_in">sqrt</span>((test2(<span class="built_in">i</span>,<span class="number">1</span>)-train(<span class="built_in">j</span>,<span class="number">1</span>))^<span class="number">2</span>+(test2(<span class="built_in">i</span>,<span class="number">2</span>)-train(<span class="built_in">j</span>,<span class="number">2</span>))^<span class="number">2</span>);  <span class="comment">%（第二类）考试点到所有训练点的距离</span></span><br><span class="line">    <span class="keyword">end</span> <span class="comment">%计算每个考试点到所有训练点的欧式距离</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="comment">%k近邻（k=21）</span></span><br><span class="line">k=<span class="number">21</span>;</span><br><span class="line">thr=<span class="built_in">ceil</span>(k/<span class="number">2</span>);  <span class="comment">%向正方向取整</span></span><br><span class="line"><span class="comment">%补充程序1  对于每一行，将所有列按升序排列</span></span><br><span class="line">[sort_dist1, sort_index1] = sort(dist1, <span class="number">2</span>);</span><br><span class="line">[sort_dist2, sort_index2] = sort(dist2, <span class="number">2</span>);</span><br><span class="line"><span class="comment">%补充程序2   %先将得到的考试点与样本点的距离排序，同时记录排序的标号</span></span><br><span class="line"></span><br><span class="line">t1=<span class="number">1</span>;</span><br><span class="line">t2=<span class="number">1</span>;</span><br><span class="line">k_err_index1=[];</span><br><span class="line">k_err_index2=[];</span><br><span class="line">err_num1=<span class="built_in">size</span>(<span class="built_in">find</span>(sort_index1(<span class="built_in">i</span>,<span class="number">1</span>:k)&gt;N1),<span class="number">2</span>); </span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:N2 <span class="comment">%遍历所有考试点</span></span><br><span class="line">    err_num1=<span class="built_in">size</span>(<span class="built_in">find</span>(sort_index1(<span class="built_in">i</span>,<span class="number">1</span>:k)&gt;N1),<span class="number">2</span>);  <span class="comment">%找前k个最近值中分错了几个</span></span><br><span class="line">    <span class="keyword">if</span> err_num1 &gt; thr<span class="comment">%补充程序3 如果错分数目过半</span></span><br><span class="line">        k_err_index1(t1)=<span class="built_in">i</span>;</span><br><span class="line">        t1=t1+<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    err_num2=<span class="built_in">size</span>(<span class="built_in">find</span>(sort_index2(<span class="built_in">i</span>,<span class="number">1</span>:k)&lt;=N1),<span class="number">2</span>);</span><br><span class="line">    <span class="keyword">if</span> err_num2 &gt; thr<span class="comment">%补充程序4</span></span><br><span class="line">        k_err_index2(t2)=<span class="built_in">i</span>;</span><br><span class="line">        t2=t2+<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="comment">%取前k个最小距离，看其标号中来自哪一类的训练样本点较多，判断是否错分</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">k_pe1=<span class="built_in">size</span>(k_err_index1,<span class="number">2</span>)/N2;</span><br><span class="line">k_pe2=<span class="built_in">size</span>(k_err_index2,<span class="number">2</span>)/N2;</span><br><span class="line">k_pe=<span class="number">0.5</span>*(k_pe1+k_pe2); <span class="comment">%计算错误率</span></span><br><span class="line">figure(<span class="number">1</span>)</span><br><span class="line">hold on</span><br><span class="line">grid on</span><br><span class="line">plot(train1(:,<span class="number">1</span>),train1(:,<span class="number">2</span>),<span class="string">'*y'</span>);</span><br><span class="line">plot(train2(:,<span class="number">1</span>),train2(:,<span class="number">2</span>),<span class="string">'+g'</span>);</span><br><span class="line">plot(test1(:,<span class="number">1</span>),test1(:,<span class="number">2</span>),<span class="string">'*r'</span>);</span><br><span class="line">plot(test2(:,<span class="number">1</span>),test2(:,<span class="number">2</span>),<span class="string">'+b'</span>);</span><br><span class="line">plot(test1(k_err_index1,<span class="number">1</span>),test1(k_err_index1,<span class="number">2</span>),<span class="string">'or'</span>,<span class="string">'MarkerSize'</span>,<span class="number">12</span>);</span><br><span class="line">plot(test2(k_err_index2,<span class="number">1</span>),test2(k_err_index2,<span class="number">2</span>),<span class="string">'sb'</span>,<span class="string">'MarkerSize'</span>,<span class="number">12</span>);</span><br></pre></td></tr></table></figure><h1 id="实验结果与讨论"><a href="#实验结果与讨论" class="headerlink" title="实验结果与讨论"></a>实验结果与讨论</h1><ol><li><p>最近邻的实验结果如下图所示，可见大多数样本被正确标记，只有临界区的几个出现了错误。</p><p><img src="/2018/11/11/《模式识别》实验5-最近邻与k近邻/1.png" alt=""></p></li><li><p>K近邻的实验结果如下图所示，与最近邻的实验现象相仿。</p><p><img src="/2018/11/11/《模式识别》实验5-最近邻与k近邻/2.png" alt=""></p></li></ol><h1 id="实验总结"><a href="#实验总结" class="headerlink" title="实验总结"></a>实验总结</h1><p>​        本次实验聚焦于最近邻和K近邻两种分类器的设计。通过实验，我掌握了这两个同为一类的分类器的原理及其实现过程。这两个分类器都是无监督的分类，即便是在今后的科研学习中，只要合理定义“距离”，仍然十分有效。相信通过这次的学习，会对我今后的科研以及课程下一阶段的学习带来帮助。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;实验目的&quot;&gt;&lt;a href=&quot;#实验目的&quot; class=&quot;headerlink&quot; title=&quot;实验目的&quot;&gt;&lt;/a&gt;实验目的&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;掌握最近邻分类算法。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;掌握k近邻分类算法。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol
      
    
    </summary>
    
      <category term="模式识别" scheme="http://wang22ti.com/categories/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>笔记-keras</title>
    <link href="http://wang22ti.com/2018/11/11/%E7%AC%94%E8%AE%B0-keras/"/>
    <id>http://wang22ti.com/2018/11/11/笔记-keras/</id>
    <published>2018-11-11T02:27:41.000Z</published>
    <updated>2018-11-11T02:30:35.367Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="机器学习" scheme="http://wang22ti.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>华为Mate20Pro的使用总结</title>
    <link href="http://wang22ti.com/2018/11/08/%E5%8D%8E%E4%B8%BAMate20Pro%E7%9A%84%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/"/>
    <id>http://wang22ti.com/2018/11/08/华为Mate20Pro的使用总结/</id>
    <published>2018-11-08T07:21:17.000Z</published>
    <updated>2018-11-08T16:54:53.218Z</updated>
    
    <content type="html"><![CDATA[<p>讲道理的话，本不应该换手机的，米6用起来还是不错的。但是遇到了mate20pro这一部国产真旗舰，还是不由地高呼“真香”，于是入手了8+128G蓝色版本。详细的测评就不写了，网上铺天盖地，就写一些比较独特的吧。</p><h1 id="关于绿屏"><a href="#关于绿屏" class="headerlink" title="关于绿屏"></a>关于绿屏</h1><p>入手之前很担心这个问题，还好是京东方的屏幕，只要百度软件<code>device info hw</code>就可以看啦：</p><p><img src="/2018/11/08/华为Mate20Pro的使用总结/1.jpg" alt=""></p><p>在<code>Touchscreen</code>中BOE就是京东方啦。屏幕素质很不错，不过和三星的比还是略有差距，显示白色背景的时候曲面还是有一点点绿，无大碍，微信扫一扫没有翻车。有趣的是分辨率是智能调节的，比如图中显示的就是1080p。</p><h1 id="关于SIM卡槽"><a href="#关于SIM卡槽" class="headerlink" title="关于SIM卡槽"></a>关于SIM卡槽</h1><p>为了压缩空间，mate20pro的两张sim卡是背靠背放在一起的！这还是我第一次见，太刺激了~</p><h1 id="关于窗口小部件"><a href="#关于窗口小部件" class="headerlink" title="关于窗口小部件"></a>关于窗口小部件</h1><p>使用了邮箱和备忘录小部件之后，半天都没有刷新，贴吧上也有人有一样的情况：</p><p><a href="http://tieba.baidu.com/p/5322839441" target="_blank" rel="noopener">p10plus 备忘录 小部件不显示内容</a></p><p>实际上重启一下就好啦~不过还是软件做得不好</p><h1 id="关于隐藏应用"><a href="#关于隐藏应用" class="headerlink" title="关于隐藏应用"></a>关于隐藏应用</h1><p>之前用p9的时候，十分喜欢的功能是将不常用的应用，所谓眼不见为净嘿嘿，不过mate20pro竟然把这个功能给砍掉了emmmm，还好在论坛上有解决方案：</p><p>开启桌面隐藏的神器：<a href="https://club.huawei.com/thread-15930827-1-1.html" target="_blank" rel="noopener">Tweaker for Huawei_v2.4</a></p><h1 id="关于手机克隆的速度"><a href="#关于手机克隆的速度" class="headerlink" title="关于手机克隆的速度"></a>关于手机克隆的速度</h1><p>按照说明对米6进行克隆的时候，大多数情况特别特别慢，速度在5K到10M之间浮动，，，终于在一个偶然的机会，我发现当两个手机背对背拥抱的时候，就是使用NFC配对的时候，可以保持在非常高的速度，大概40-50M，几乎是米6的带宽极限了吧。</p><h1 id="关于自启动权限"><a href="#关于自启动权限" class="headerlink" title="关于自启动权限"></a>关于自启动权限</h1><p>emmmm，这个十分令人无语，因为捷波朗耳机的软件需要后台运行一个插件，我把设置翻了个遍也没找到这样的权限设置，最后在网上查了一下竟然在应用<code>手机管家</code>里面，，</p><h1 id="和米6的对比"><a href="#和米6的对比" class="headerlink" title="和米6的对比"></a>和米6的对比</h1><p>之所以和小米的上一代产品比，是因为真的没有钱再买米8了啊，，</p><p>优势：</p><ol><li>无敌的摄像水平，就不放样张了，见了都说好</li><li>960帧慢动作，AI摄影，AR表情，手持夜景，这些都是小米6没有的</li><li>40W的快充和4200毫安的电池，十分的安心</li><li>无线充电和反向无线充电实在是太酷了</li><li>优秀的屏幕A屏，极高的屏占比，看起来赏心悦目，还有息屏信息</li><li>蓝牙耳机几乎没有底噪</li><li>稳定的信号，比如在电梯里依然可以看视频！</li><li>完整的Google框架，翻墙很轻松</li><li>云电脑和无线投屏还是很赞的</li><li>指关节操作还是很赞的，而且比p9好用多了</li><li>支持长按图标呼出菜单，很方便</li><li>IP68防水，冲洗十分方便</li><li>抬腕亮屏+人脸解锁+屏下指纹，十分优秀的解锁方案</li><li>蓝色版本背后的纹理十分的好看，耐指纹</li><li>比米6陶瓷版要轻一些哈哈</li></ol><p>劣势主要集中在系统方面：</p><ol><li>没有负一屏比较鸡肋，没有我的购物、我的快递、我的收藏这些聚类</li><li>一些设定的逻辑混乱，就比如自启动的权限竟然不在设置当中</li><li>UI统一度有待加强，比如夜间模式中邮箱小部件完全没有改动</li><li>无法自动获取流量套餐情况</li><li>应用双开完全没法和小米比</li><li>同步功能不如小米完善</li><li>相册没有按文字查找的功能，人脸分析的水平有待提高</li><li>可选的主题没有小米丰富</li></ol><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>华为mate20pro满足了我对于2018乃至2019年上半年对于机皇的所有想象。再加上对于国产零部件的带动，可以说是用心做手机了。同时，还有很多细节是值得进一步改善的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;讲道理的话，本不应该换手机的，米6用起来还是不错的。但是遇到了mate20pro这一部国产真旗舰，还是不由地高呼“真香”，于是入手了8+128G蓝色版本。详细的测评就不写了，网上铺天盖地，就写一些比较独特的吧。&lt;/p&gt;
&lt;h1 id=&quot;关于绿屏&quot;&gt;&lt;a href=&quot;#关于绿
      
    
    </summary>
    
      <category term="杂记" scheme="http://wang22ti.com/categories/%E6%9D%82%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>一个极简的翻译前端</title>
    <link href="http://wang22ti.com/2018/11/06/%E4%B8%80%E4%B8%AA%E6%9E%81%E7%AE%80%E7%9A%84%E7%BF%BB%E8%AF%91%E5%89%8D%E7%AB%AF/"/>
    <id>http://wang22ti.com/2018/11/06/一个极简的翻译前端/</id>
    <published>2018-11-06T08:08:52.000Z</published>
    <updated>2018-11-12T07:03:18.337Z</updated>
    
    <content type="html"><![CDATA[<p>嗯……不知道为啥脑残地写了这个极简的翻译器</p><h1 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h1><p>就是调用百度翻译的API了，输入框没有东西的时候按回车窗口跑到鼠标处，输入框里是q的时候按回车退出。</p><p><img src="/2018/11/06/一个极简的翻译前端/1.png" alt=""></p><h1 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h1><p>主要用到了了<a href="http://wang22ti.com/2018/02/20/%E7%AC%94%E8%AE%B0-%E7%99%BE%E5%BA%A6%E7%BF%BB%E8%AF%91API/">调用百度翻译</a>，python前端包<code>tkinter</code>，以及检测鼠标键盘的包<code>pyautogui</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tkinter <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> pyautogui <span class="keyword">as</span> pag</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">baidu_traslate</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        q=<span class="string">'apple'</span>, toLang=<span class="string">'zh'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        fromLang=<span class="string">'auto'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        appid=<span class="string">'***********'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        secretKey=<span class="string">'***************'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> requests</span><br><span class="line">    <span class="keyword">import</span> random</span><br><span class="line">    <span class="keyword">import</span> hashlib</span><br><span class="line">    <span class="keyword">import</span> json</span><br><span class="line">    url = <span class="string">'http://api.fanyi.baidu.com/api/trans/vip/translate'</span></span><br><span class="line">    salt = random.randint(<span class="number">32768</span>, <span class="number">65536</span>)</span><br><span class="line"></span><br><span class="line">    sign = appid + q + str(salt) + secretKey</span><br><span class="line">    m = hashlib.md5(sign.encode(<span class="string">'utf-8'</span>))</span><br><span class="line">    sign = m.hexdigest()</span><br><span class="line"></span><br><span class="line">    url = url + \</span><br><span class="line">          <span class="string">'?appid='</span> + appid + \</span><br><span class="line">          <span class="string">'&amp;q='</span> + q + \</span><br><span class="line">          <span class="string">'&amp;from='</span> + fromLang + \</span><br><span class="line">          <span class="string">'&amp;to='</span> + toLang + \</span><br><span class="line">          <span class="string">'&amp;salt='</span> + str(salt) + <span class="string">'&amp;sign='</span> + sign</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        text = requests.get(url).text</span><br><span class="line">        ret_dict = json.loads(text)</span><br><span class="line">        ret_dict[<span class="string">'src'</span>] = ret_dict[<span class="string">'trans_result'</span>][<span class="number">0</span>][<span class="string">'src'</span>]</span><br><span class="line">        ret_dict[<span class="string">'dst'</span>] = ret_dict[<span class="string">'trans_result'</span>][<span class="number">0</span>][<span class="string">'dst'</span>]</span><br><span class="line">        ret_dict.pop(<span class="string">'trans_result'</span>)</span><br><span class="line">        <span class="keyword">return</span> ret_dict</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'dst'</span>: <span class="string">''</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyWindow</span><span class="params">(Frame)</span>:</span></span><br><span class="line"></span><br><span class="line">    x, y = <span class="number">1000</span>, <span class="number">80</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">operate</span><span class="params">(self, event)</span>:</span></span><br><span class="line">        src = self.input.get()</span><br><span class="line">        <span class="keyword">if</span> src == <span class="string">''</span>:</span><br><span class="line">            self.x, self.y = pag.position()</span><br><span class="line">            self.root.geometry(<span class="string">"+%d+%d"</span> % (self.x, self.y))</span><br><span class="line">        <span class="keyword">elif</span> src == <span class="string">'q'</span>:</span><br><span class="line">            sys.exit()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dst = baidu_traslate(src)[<span class="string">'dst'</span>]</span><br><span class="line">            <span class="comment"># print(dst)</span></span><br><span class="line">            self.ouput.delete(<span class="number">0</span>, <span class="string">'end'</span>)</span><br><span class="line">            self.ouput.insert(<span class="number">0</span>, dst)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, root=None)</span>:</span></span><br><span class="line">        self.root = root</span><br><span class="line">        self.root.wm_attributes(<span class="string">'-topmost'</span>, <span class="number">1</span>)</span><br><span class="line">        self.root.overrideredirect(<span class="keyword">True</span>)</span><br><span class="line">        self.root.geometry(<span class="string">"+%d+%d"</span> % (self.x, self.y))</span><br><span class="line">        Frame.__init__(self, self.root)</span><br><span class="line"></span><br><span class="line">        self.input = Entry(root)</span><br><span class="line">        self.input.bind(<span class="string">'&lt;Return&gt;'</span>, self.operate)</span><br><span class="line">        self.input.pack()</span><br><span class="line"></span><br><span class="line">        self.ouput = Entry(root)</span><br><span class="line">        self.ouput.pack()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root = MyWindow(Tk())</span><br><span class="line">root.mainloop()</span><br></pre></td></tr></table></figure><h1 id="关于打包"><a href="#关于打包" class="headerlink" title="关于打包"></a>关于打包</h1><p>第一次将python源代码打包为exe，用到的包是<code>pyinstaller</code>。用到的语句是</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyinstaller -F 源文件.py -w</span><br></pre></td></tr></table></figure><p>其中<code>-F</code>打包为一个exe文件，<code>-w</code>表示在运行的时候不需要dos界面。</p><p>需要注意的是即便是一个很小的程序，因为这个包会把所有python环境的包打包进来，就导致十分地累赘。正确的做法是在anaconda中新建一个纯净的环境，需要什么包就安装什么，然后再调用pyinstaller，这样该程序从300M下降到10M。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;嗯……不知道为啥脑残地写了这个极简的翻译器&lt;/p&gt;
&lt;h1 id=&quot;功能&quot;&gt;&lt;a href=&quot;#功能&quot; class=&quot;headerlink&quot; title=&quot;功能&quot;&gt;&lt;/a&gt;功能&lt;/h1&gt;&lt;p&gt;就是调用百度翻译的API了，输入框没有东西的时候按回车窗口跑到鼠标处，输入框里是q
      
    
    </summary>
    
      <category term="小程序" scheme="http://wang22ti.com/categories/%E5%B0%8F%E7%A8%8B%E5%BA%8F/"/>
    
    
  </entry>
  
  <entry>
    <title>《模式识别》实验4-线性分类器设计</title>
    <link href="http://wang22ti.com/2018/10/20/%E3%80%8A%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E3%80%8B%E5%AE%9E%E9%AA%8C4-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8%E8%AE%BE%E8%AE%A1/"/>
    <id>http://wang22ti.com/2018/10/20/《模式识别》实验4-线性分类器设计/</id>
    <published>2018-10-20T02:55:39.000Z</published>
    <updated>2018-11-12T09:25:36.916Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实验目的"><a href="#实验目的" class="headerlink" title="实验目的"></a>实验目的</h1><ol><li><p>了解基于Fisher准则的线性分类器和基于感知准则的线性分类器的数学原理。</p></li><li><p>实现上述两种线性分类器的设计，加深对其原理的理解。</p></li></ol><h1 id="实验原理"><a href="#实验原理" class="headerlink" title="实验原理"></a>实验原理</h1><ol><li><p>Fisher准则的基本思想是：对于两类问题，就是要找到一个最合适的投影轴，使两类样本在该轴上投影的交迭部分最少，从而使分类效果为最佳。故有基本参数：各类样本的均值向量$m _ { i } = \frac { 1 } { N _ { i } } \sum _ { \left\{ x \in X _ { i } \right\} } x$，样本类内离散度矩阵$\mathrm { S } _ { \mathrm { i } } = \sum _ { \left\{ x \in X _ { i } \right\} } \left( x - m _ { i } \right) \left( x - m _ { i } \right) ^ { T }$，总类内离散度矩阵$S _ { \mathrm { w } } = S _ { 1 } + S _ { 2 }$，样本间离散度矩阵$S _ { b } = \left( m _ { 1 } - m _ { 2 } \right) \left( m _ { 1 } - m _ { 2 } \right) ^ { T }$。则定义Fisher准则函数：$\mathrm { J } _ { \mathrm { F } } ( W ) = \frac { \left( \tilde { m } _ { 1 } - \tilde { m } _ { 2 } \right) ^ { 2 } } { \tilde { S } _ { 1 } ^ { 2 } + \tilde { S } _ { 2 } ^ { 2 } } , \tilde { m } _ { i } = w ^ { T } \mathrm { m } _ { \mathrm { i } } , \tilde { S } _ { i } ^ { 2 } = w ^ { T } S _ { i } w$，则有最优投影方向$\mathrm { W } ^ { * } = S _ { W } ^ { - 1 } \left( m _ { 1 } - m _ { 2 } \right)$，而一般$\mathrm { W } _ { 0 } = - \frac { \tilde { m } _ { 1 } - \tilde { m } _ { 2 } } { 2 }$。</p></li><li><p>定义感知准则函数$J ( a ) = \sum _ { \left\{ y \in y ^ { k } \right\} } - a ^ { T } y$，其中$y^k$为错分样本集合。因此对$a$求梯度有</p></li></ol><script type="math/tex; mode=display">\nabla \mathrm { J } = \frac { \partial J ( a ) } { \partial a } = \sum _ { \left\{ \mathrm { y } \in \mathrm { y } ^ { \mathrm { k } } \right\} } - y</script><p>​        所以有感知机迭代公式：</p><script type="math/tex; mode=display">a _ { K + 1 } = a _ { k } + \rho _ { k } \sum _ { \left\{ y \in y ^ { k } \right\} } y</script><h1 id="实验内容与步骤"><a href="#实验内容与步骤" class="headerlink" title="实验内容与步骤"></a>实验内容与步骤</h1><ol><li><p>把数据作为样本，根据Fisher准则选择投影方向，使原样本向量在该方向上的投影能兼顾类间分布尽可能分开，类内样本投影尽可能密集的要求，求出评价投影方向$W$的函数，并在图形表示出来，并求使$J_F(W)$取极大值的$W^*$ 。用matlab完成Fisher线性分类器的设计。根据上述的结果并判断(1,1.5,0.6), (1.2,1.0,0.55), (2.0,0.9,0.68), (1.2,1.5,0.89), (0.23,2.33,1.43)，属于哪个类别，并画出数据分类相应的结果图，画出其在上的投影。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line">clear all;</span><br><span class="line">x1 =[<span class="number">0.2331</span>    <span class="number">1.5207</span>    <span class="number">0.6499</span>    <span class="number">0.7757</span>    <span class="number">1.0524</span>    <span class="number">1.1974</span></span><br><span class="line">    <span class="number">0.2908</span>    <span class="number">0.2518</span>    <span class="number">0.6682</span>    <span class="number">0.5622</span>    <span class="number">0.9023</span>    <span class="number">0.1333</span></span><br><span class="line">    <span class="number">-0.5431</span>    <span class="number">0.9407</span>   <span class="number">-0.2126</span>    <span class="number">0.0507</span>   <span class="number">-0.0810</span>    <span class="number">0.7315</span></span><br><span class="line">    <span class="number">0.3345</span>    <span class="number">1.0650</span>   <span class="number">-0.0247</span>    <span class="number">0.1043</span>    <span class="number">0.3122</span>    <span class="number">0.6655</span></span><br><span class="line">    <span class="number">0.5838</span>    <span class="number">1.1653</span>    <span class="number">1.2653</span>    <span class="number">0.8137</span>   <span class="number">-0.3399</span>    <span class="number">0.5152</span></span><br><span class="line">    <span class="number">0.7226</span>   <span class="number">-0.2015</span>    <span class="number">0.4070</span>   <span class="number">-0.1717</span>   <span class="number">-1.0573</span>   <span class="number">-0.2099</span>];</span><br><span class="line">y1 =[<span class="number">2.3385</span>    <span class="number">2.1946</span>    <span class="number">1.6730</span>    <span class="number">1.6365</span>    <span class="number">1.7844</span>    <span class="number">2.0155</span></span><br><span class="line">    <span class="number">2.0681</span>    <span class="number">2.1213</span>    <span class="number">2.4797</span>    <span class="number">1.5118</span>    <span class="number">1.9692</span>    <span class="number">1.8340</span></span><br><span class="line">    <span class="number">1.8704</span>    <span class="number">2.2948</span>    <span class="number">1.7714</span>    <span class="number">2.3939</span>    <span class="number">1.5648</span>    <span class="number">1.9329</span></span><br><span class="line">    <span class="number">2.2027</span>    <span class="number">2.4568</span>    <span class="number">1.7523</span>    <span class="number">1.6991</span>    <span class="number">2.4883</span>    <span class="number">1.7259</span></span><br><span class="line">    <span class="number">2.0466</span>    <span class="number">2.0226</span>    <span class="number">2.3757</span>    <span class="number">1.7987</span>    <span class="number">2.0828</span>    <span class="number">2.0798</span></span><br><span class="line">    <span class="number">1.9449</span>    <span class="number">2.3801</span>    <span class="number">2.2373</span>    <span class="number">2.1614</span>    <span class="number">1.9235</span>    <span class="number">2.2604</span>];</span><br><span class="line">z1 =[<span class="number">0.5338</span>    <span class="number">0.8514</span>    <span class="number">1.0831</span>    <span class="number">0.4164</span>    <span class="number">1.1176</span>    <span class="number">0.5536</span></span><br><span class="line">    <span class="number">0.6071</span>    <span class="number">0.4439</span>    <span class="number">0.4928</span>    <span class="number">0.5901</span>    <span class="number">1.0927</span>    <span class="number">1.0756</span></span><br><span class="line">    <span class="number">1.0072</span>    <span class="number">0.4272</span>    <span class="number">0.4353</span>    <span class="number">0.9869</span>    <span class="number">0.4841</span>    <span class="number">1.0992</span></span><br><span class="line">    <span class="number">1.0299</span>    <span class="number">0.7127</span>    <span class="number">1.0124</span>    <span class="number">0.4576</span>    <span class="number">0.8544</span>    <span class="number">1.1275</span></span><br><span class="line">    <span class="number">0.7705</span>    <span class="number">0.4129</span>    <span class="number">1.0085</span>    <span class="number">0.7676</span>    <span class="number">0.8418</span>    <span class="number">0.8784</span></span><br><span class="line">    <span class="number">0.9751</span>    <span class="number">0.7840</span>    <span class="number">0.4158</span>    <span class="number">1.0315</span>    <span class="number">0.7533</span>    <span class="number">0.9548</span>];</span><br><span class="line"><span class="comment">%将x1、y1、z1变为列向量</span></span><br><span class="line">x1=x1(:);y1=y1(:);z1=z1(:);</span><br><span class="line"></span><br><span class="line"><span class="comment">%计算第一类的样本均值向量m1</span></span><br><span class="line">m1(<span class="number">1</span>)=mean(x1);</span><br><span class="line">m1(<span class="number">2</span>)=mean(y1);</span><br><span class="line">m1(<span class="number">3</span>)=mean(z1);</span><br><span class="line"></span><br><span class="line"><span class="comment">%计算第一类样本类内离散度矩阵S1，参考（4-17）</span></span><br><span class="line"></span><br><span class="line">S1=<span class="built_in">zeros</span>(<span class="number">3</span>,<span class="number">3</span>);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:<span class="number">36</span></span><br><span class="line">    <span class="comment">%程序补充（1）</span></span><br><span class="line">    tmp = [x1(i)-m1(<span class="number">1</span>) y1(i)-m1(<span class="number">2</span>) z1(i)-m1(<span class="number">3</span>)]';</span><br><span class="line">    S1 = S1 + tmp * tmp';   </span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">%w2的数据点坐标</span></span><br><span class="line">x2 =[<span class="number">1.4010</span>    <span class="number">1.2301</span>    <span class="number">2.0814</span>    <span class="number">1.1655</span>    <span class="number">1.3740</span>    <span class="number">1.1829</span></span><br><span class="line">    <span class="number">1.7632</span>    <span class="number">1.9739</span>    <span class="number">2.4152</span>    <span class="number">2.5890</span>    <span class="number">2.8472</span>    <span class="number">1.9539</span></span><br><span class="line">    <span class="number">1.2500</span>    <span class="number">1.2864</span>    <span class="number">1.2614</span>    <span class="number">2.0071</span>    <span class="number">2.1831</span>    <span class="number">1.7909</span></span><br><span class="line">    <span class="number">1.3322</span>    <span class="number">1.1466</span>    <span class="number">1.7087</span>    <span class="number">1.5920</span>    <span class="number">2.9353</span>    <span class="number">1.4664</span></span><br><span class="line">    <span class="number">2.9313</span>    <span class="number">1.8349</span>    <span class="number">1.8340</span>    <span class="number">2.5096</span>    <span class="number">2.7198</span>    <span class="number">2.3148</span></span><br><span class="line">    <span class="number">2.0353</span>    <span class="number">2.6030</span>    <span class="number">1.2327</span>    <span class="number">2.1465</span>    <span class="number">1.5673</span>    <span class="number">2.9414</span>];</span><br><span class="line"></span><br><span class="line">y2 =[<span class="number">1.0298</span>    <span class="number">0.9611</span>    <span class="number">0.9154</span>    <span class="number">1.4901</span>    <span class="number">0.8200</span>    <span class="number">0.9399</span></span><br><span class="line">    <span class="number">1.1405</span>    <span class="number">1.0678</span>    <span class="number">0.8050</span>    <span class="number">1.2889</span>    <span class="number">1.4601</span>    <span class="number">1.4334</span></span><br><span class="line">    <span class="number">0.7091</span>    <span class="number">1.2942</span>    <span class="number">1.3744</span>    <span class="number">0.9387</span>    <span class="number">1.2266</span>    <span class="number">1.1833</span></span><br><span class="line">    <span class="number">0.8798</span>    <span class="number">0.5592</span>    <span class="number">0.5150</span>    <span class="number">0.9983</span>    <span class="number">0.9120</span>    <span class="number">0.7126</span></span><br><span class="line">    <span class="number">1.2833</span>    <span class="number">1.1029</span>    <span class="number">1.2680</span>    <span class="number">0.7140</span>    <span class="number">1.2446</span>    <span class="number">1.3392</span></span><br><span class="line">    <span class="number">1.1808</span>    <span class="number">0.5503</span>    <span class="number">1.4708</span>    <span class="number">1.1435</span>    <span class="number">0.7679</span>    <span class="number">1.1288</span>];</span><br><span class="line">z2 =[<span class="number">0.6210</span>    <span class="number">1.3656</span>    <span class="number">0.5498</span>    <span class="number">0.6708</span>    <span class="number">0.8932</span>    <span class="number">1.4342</span></span><br><span class="line">    <span class="number">0.9508</span>    <span class="number">0.7324</span>    <span class="number">0.5784</span>    <span class="number">1.4943</span>    <span class="number">1.0915</span>    <span class="number">0.7644</span></span><br><span class="line">    <span class="number">1.2159</span>    <span class="number">1.3049</span>    <span class="number">1.1408</span>    <span class="number">0.9398</span>    <span class="number">0.6197</span>    <span class="number">0.6603</span></span><br><span class="line">    <span class="number">1.3928</span>    <span class="number">1.4084</span>    <span class="number">0.6909</span>    <span class="number">0.8400</span>    <span class="number">0.5381</span>    <span class="number">1.3729</span></span><br><span class="line">    <span class="number">0.7731</span>    <span class="number">0.7319</span>    <span class="number">1.3439</span>    <span class="number">0.8142</span>    <span class="number">0.9586</span>    <span class="number">0.7379</span></span><br><span class="line">    <span class="number">0.7548</span>    <span class="number">0.7393</span>    <span class="number">0.6739</span>    <span class="number">0.8651</span>    <span class="number">1.3699</span>    <span class="number">1.1458</span>];</span><br><span class="line">x2=x2(:);</span><br><span class="line">y2=y2(:);</span><br><span class="line">z2=z2(:);</span><br><span class="line"></span><br><span class="line"><span class="comment">%计算第二类的样本均值向量m2</span></span><br><span class="line">m2(<span class="number">1</span>)=mean(x2);</span><br><span class="line">m2(<span class="number">2</span>)=mean(y2);</span><br><span class="line">m2(<span class="number">3</span>)=mean(z2);</span><br><span class="line"></span><br><span class="line"><span class="comment">%计算第二类样本类内离散度矩阵S2</span></span><br><span class="line"></span><br><span class="line">S2=<span class="built_in">zeros</span>(<span class="number">3</span>,<span class="number">3</span>);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:<span class="number">36</span></span><br><span class="line">    <span class="comment">%程序补充（2）</span></span><br><span class="line">    tmp = [x2(i)-m2(<span class="number">1</span>) y2(i)-m2(<span class="number">2</span>) z2(i)-m2(<span class="number">3</span>)]';</span><br><span class="line">    S2 = S2 + tmp * tmp';  </span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">%总类内离散度矩阵Sw，参考（4-18）</span></span><br><span class="line"><span class="comment">%程序补充（3）</span></span><br><span class="line"></span><br><span class="line">Sw = S1 + S2;</span><br><span class="line"></span><br><span class="line"><span class="comment">%样本类间离散度矩阵Sb，参考（4-19）</span></span><br><span class="line">Sb=<span class="built_in">zeros</span>(<span class="number">3</span>,<span class="number">3</span>);</span><br><span class="line">Sb=(m1-m2)'*(m1-m2);</span><br><span class="line"></span><br><span class="line"><span class="comment">%最优解W</span></span><br><span class="line">W=Sw^<span class="number">-1</span>*(m1-m2)';   <span class="comment">%，参考（4-32）</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%将W变为单位向量以方便计算投影</span></span><br><span class="line">W=W/<span class="built_in">sqrt</span>(sum(W.^<span class="number">2</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">%计算一维Y空间中的各类样本均值M1及M2</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:<span class="number">36</span></span><br><span class="line">    y(<span class="built_in">i</span>)=W'*[x1(i) y1(i) z1(i)]';</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">M1=mean(y);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:<span class="number">36</span></span><br><span class="line">    y(<span class="built_in">i</span>)=W'*[x2(i) y2(i) z2(i)]';</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">M2=mean(y);</span><br><span class="line"></span><br><span class="line">W0=-(M1+M2)/<span class="number">2</span>; <span class="comment">%参考（4-33）</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%计算将样本投影到最佳方向上以后的新坐标</span></span><br><span class="line">X1=[x1*W(<span class="number">1</span>)+y1*W(<span class="number">2</span>)+z1*W(<span class="number">3</span>)]';</span><br><span class="line">X2=[x2*W(<span class="number">1</span>)+y2*W(<span class="number">2</span>)+z2*W(<span class="number">3</span>)]';   <span class="comment">%得到投影长度</span></span><br><span class="line">XX1=[W(<span class="number">1</span>)*X1;W(<span class="number">2</span>)*X1;W(<span class="number">3</span>)*X1];</span><br><span class="line">XX2=[W(<span class="number">1</span>)*X2;W(<span class="number">2</span>)*X2;W(<span class="number">3</span>)*X2];   <span class="comment">%得到新坐标</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">%绘制样本点</span></span><br><span class="line">figure;</span><br><span class="line">h1=plot3(x1,y1,z1,<span class="string">'r*'</span>);   <span class="comment">%第一类(红色星号)</span></span><br><span class="line">hold on</span><br><span class="line">h2=plot3(x2,y2,z2,<span class="string">'gp'</span>) ;  <span class="comment">%第二类(绿色五角星)</span></span><br><span class="line">title(<span class="string">'Fisher线性判别曲线'</span>);</span><br><span class="line">W1=<span class="number">5</span>*W;     <span class="comment">%使画的线更长</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%判别已给点的类 </span></span><br><span class="line">a1=[<span class="number">1</span>,<span class="number">1.5</span>,<span class="number">0.6</span>]';a2=[<span class="number">1.2</span>,<span class="number">1.0</span>,<span class="number">0.55</span>]';a3=[<span class="number">2.0</span>,<span class="number">0.9</span>,<span class="number">0.68</span>]';a4=[<span class="number">1.2</span>,<span class="number">1.5</span>,<span class="number">0.89</span>]';a5=[<span class="number">0.23</span>,<span class="number">2.33</span>,<span class="number">1.43</span>]';</span><br><span class="line">A=[a1 a2 a3 a4 a5];</span><br><span class="line">n=<span class="built_in">size</span>(A,<span class="number">2</span>);                      </span><br><span class="line"><span class="comment">%下面代码在改变样本时可不修改</span></span><br><span class="line"><span class="comment">%绘制待测数据投影到最佳方向上的点</span></span><br><span class="line"><span class="keyword">for</span> k=<span class="number">1</span>:n</span><br><span class="line">    A1=A(:,k)'*W;   <span class="comment">%得到投影长度</span></span><br><span class="line">    A11=W*A1;   <span class="comment">%得到待测数据投影新坐标</span></span><br><span class="line">    y=W'*A(:,k)+W0;    <span class="comment">%计算后与0相比以判断类别，大于0为第一类，小于0为第二类            </span></span><br><span class="line">    <span class="keyword">if</span> y&gt;<span class="number">0</span></span><br><span class="line">        h3=plot3(A(<span class="number">1</span>,k),A(<span class="number">2</span>,k),A(<span class="number">3</span>,k),<span class="string">'ro'</span>);   <span class="comment">%点为"ro"对应第一类(红色圆圈)</span></span><br><span class="line">        h3=plot3(A11(<span class="number">1</span>),A11(<span class="number">2</span>),A11(<span class="number">3</span>),<span class="string">'ro'</span>);  <span class="comment">%投影为"ro"对应第一类</span></span><br><span class="line">    <span class="keyword">else</span> </span><br><span class="line">        h4=plot3(A(<span class="number">1</span>,k),A(<span class="number">2</span>,k),A(<span class="number">3</span>,k),<span class="string">'bh'</span>);   <span class="comment">%点为"ch"对应第二类(蓝色六角星)</span></span><br><span class="line">        h4=plot3(A11(<span class="number">1</span>),A11(<span class="number">2</span>),A11(<span class="number">3</span>),<span class="string">'bh'</span>);  <span class="comment">%投影为"ch"对应第二类</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">legend([h1,h2,h3,h4],<span class="string">'第一类样本点'</span>,<span class="string">'第二类样本点'</span>,<span class="string">'属于第一类的点'</span>,<span class="string">'属于第二类的点'</span>);</span><br><span class="line"><span class="comment">%画出最佳方向 </span></span><br><span class="line">line([-W1(<span class="number">1</span>),W1(<span class="number">1</span>)],[-W1(<span class="number">2</span>),W1(<span class="number">2</span>)],[-W1(<span class="number">3</span>),W1(<span class="number">3</span>)],<span class="string">'color'</span>,<span class="string">'k'</span>); </span><br><span class="line">view([<span class="number">-37.5</span>,<span class="number">30</span>]);   <span class="comment">%设置视点的函数view(方位角,仰角)</span></span><br><span class="line">axis([<span class="number">-2</span>,<span class="number">3</span>,<span class="number">-1</span>,<span class="number">3</span>,<span class="number">-0.5</span>,<span class="number">1.5</span>]); <span class="comment">%设置坐标轴可见范围</span></span><br><span class="line">grid on</span><br><span class="line">hold off</span><br></pre></td></tr></table></figure></li><li><p>实验所用样本数据如表1给出（其中每个数据为两维， x1表示第一维的值、x2表示第二维的值），编程实现y1、y2类，和y2、y3类的分类。分析分类器算法的性能，写出实现感知器算法的程序，具体要求：1、将程序应用在y1和y2的训练数据上。记下收敛的步数。2、将程序应用在y2和y3类上，同样记下收敛的步数。</p></li></ol><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">clear all;</span><br><span class="line">clc;</span><br><span class="line"><span class="comment">%original data</span></span><br><span class="line"><span class="comment">%产生第一类、第二类和第三类原始数据，分别赋给w1、w2 和w3 变量</span></span><br><span class="line">w1=[<span class="number">0.1</span> <span class="number">6.8</span> <span class="number">-3.5</span> <span class="number">2.0</span> <span class="number">4.1</span> <span class="number">3.1</span> <span class="number">-0.8</span> <span class="number">0.9</span> <span class="number">5.0</span> <span class="number">3.9</span>;<span class="number">1.1</span> <span class="number">7.1</span> <span class="number">-4.1</span> <span class="number">2.7</span> <span class="number">2.8</span> <span class="number">5.0</span> <span class="number">-1.3</span> <span class="number">1.2</span> <span class="number">6.4</span> <span class="number">4.0</span>];</span><br><span class="line">w2=[<span class="number">7.1</span> <span class="number">-1.4</span> <span class="number">4.5</span> <span class="number">6.3</span> <span class="number">4.2</span> <span class="number">1.4</span> <span class="number">2.4</span> <span class="number">2.5</span> <span class="number">8.4</span> <span class="number">4.1</span>;<span class="number">4.2</span> <span class="number">-4.3</span> <span class="number">0.0</span> <span class="number">1.6</span> <span class="number">1.9</span> <span class="number">-3.2</span> <span class="number">-4.0</span> <span class="number">-6.1</span> <span class="number">3.7</span> <span class="number">-2.2</span>];</span><br><span class="line">w3=[<span class="number">-3.0</span> <span class="number">0.5</span> <span class="number">2.9</span> <span class="number">-0.1</span> <span class="number">-4.0</span> <span class="number">-1.3</span> <span class="number">-3.4</span> <span class="number">-4.1</span> <span class="number">-5.1</span> <span class="number">1.9</span>;<span class="number">-2.9</span> <span class="number">8.7</span> <span class="number">2.1</span> <span class="number">5.2</span> <span class="number">2.2</span> <span class="number">3.7</span> <span class="number">6.2</span> <span class="number">3.4</span> <span class="number">1.6</span> <span class="number">5.1</span>];</span><br><span class="line"><span class="comment">%normalized</span></span><br><span class="line"><span class="comment">%分别产生第一类、第二类和第三类增广样本向量集ww1、ww2 和ww3</span></span><br><span class="line">ww1=[ones(<span class="number">1</span>,size(w1,<span class="number">2</span>)); w1];</span><br><span class="line">ww2=[ones(<span class="number">1</span>,size(w2,<span class="number">2</span>)); w2];</span><br><span class="line">ww3=[ones(<span class="number">1</span>,size(w3,<span class="number">2</span>)); w3];</span><br><span class="line"><span class="comment">%产生第一类和第二类样本向量的规范化增广样本向量集w12</span></span><br><span class="line">w12=[ww1,-ww2];<span class="comment">%4-42</span></span><br><span class="line"><span class="comment">%%w13=[ww1,-ww3];</span></span><br><span class="line"><span class="comment">%%w23=[ww2,-ww3];</span></span><br><span class="line">y=<span class="built_in">zeros</span>(<span class="number">1</span>,<span class="built_in">size</span>(w12,<span class="number">2</span>)); <span class="comment">%产生1x20 的行向量，赋给y，初值全为0</span></span><br><span class="line">v=[<span class="number">1</span>;<span class="number">1</span>;<span class="number">1</span>]; <span class="comment">%给权向量v 赋初值</span></span><br><span class="line">k=<span class="number">0</span>; <span class="comment">%k 为迭代次数，v(0)= [1;1;1]</span></span><br><span class="line"><span class="keyword">while</span> any(y&lt;=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:<span class="built_in">size</span>(y,<span class="number">2</span>)</span><br><span class="line">        <span class="comment">%补充程序(1)</span></span><br><span class="line">        y(<span class="built_in">i</span>) = v'*w12(:,<span class="built_in">i</span>);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="comment">%补充程序(2)</span></span><br><span class="line">    v = v+(sum( (w12(:,<span class="built_in">find</span>(y&lt;<span class="number">0</span>)))'))'</span><br><span class="line">    k=k+<span class="number">1</span>;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">v <span class="comment">%显示最终求得的权向量v 的值</span></span><br><span class="line">k <span class="comment">%迭代次数值</span></span><br><span class="line">figure(<span class="number">1</span>)</span><br><span class="line">plot(w1(<span class="number">1</span>,:),w1(<span class="number">2</span>,:),<span class="string">'r.'</span>)</span><br><span class="line">hold on</span><br><span class="line">plot(w2(<span class="number">1</span>,:),w2(<span class="number">2</span>,:),<span class="string">'b*'</span>)</span><br><span class="line">xmin=min(min(w1(<span class="number">1</span>,:)),min(w2(<span class="number">1</span>,:)));</span><br><span class="line">xmax=max(max(w1(<span class="number">1</span>,:)),max(w2(<span class="number">1</span>,:)));</span><br><span class="line">ymin=min(min(w1(<span class="number">2</span>,:)),min(w2(<span class="number">2</span>,:)));</span><br><span class="line">ymax=max(max(w1(<span class="number">2</span>,:)),max(w2(<span class="number">2</span>,:)));</span><br><span class="line">xindex=xmin<span class="number">-1</span>:(xmax-xmin)/<span class="number">100</span>:xmax+<span class="number">1</span>;</span><br><span class="line">yindex=-v(<span class="number">2</span>)*xindex/v(<span class="number">3</span>)-v(<span class="number">1</span>)/v(<span class="number">3</span>); <span class="comment">%由v*xindex=0推，参考书上（4-44）</span></span><br><span class="line">plot(xindex,yindex)</span><br><span class="line"></span><br><span class="line"><span class="comment">%从v=0 开始，将程序应用在ω2 和ω3 类上，同样记下收敛的步数。</span></span><br><span class="line">w23=[ww2,-ww3];</span><br><span class="line">yy=<span class="built_in">zeros</span>(<span class="number">1</span>,<span class="built_in">size</span>(w23,<span class="number">2</span>)); <span class="comment">%产生1x20 的行向量，赋给y，初值全为0</span></span><br><span class="line">vv=[<span class="number">1</span>;<span class="number">1</span>;<span class="number">1</span>]; <span class="comment">%给权向量v 赋初值</span></span><br><span class="line">kk=<span class="number">0</span>; <span class="comment">%k 为迭代次数，v(0)= [1;1;1]</span></span><br><span class="line"><span class="keyword">while</span> any(yy&lt;=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:<span class="built_in">size</span>(yy,<span class="number">2</span>)</span><br><span class="line">        <span class="comment">%补充程序(3)</span></span><br><span class="line">        yy(<span class="built_in">i</span>) = vv'*w23(:,<span class="built_in">i</span>);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="comment">%补充程序(4)</span></span><br><span class="line">    vv = vv+(sum( (w23(:,<span class="built_in">find</span>(yy&lt;<span class="number">0</span>)))'))'</span><br><span class="line">    kk=kk+<span class="number">1</span>;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">vv <span class="comment">%显示最终求得的权向量v 的值</span></span><br><span class="line">kk <span class="comment">%迭代次数值</span></span><br><span class="line">figure(<span class="number">2</span>)</span><br><span class="line">plot(w2(<span class="number">1</span>,:),w2(<span class="number">2</span>,:),<span class="string">'r.'</span>)</span><br><span class="line">hold on</span><br><span class="line">plot(w3(<span class="number">1</span>,:),w3(<span class="number">2</span>,:),<span class="string">'b*'</span>)</span><br><span class="line">xxmin=min(min(w2(<span class="number">1</span>,:)),min(w3(<span class="number">1</span>,:)));</span><br><span class="line">xxmax=max(max(w2(<span class="number">1</span>,:)),max(w3(<span class="number">1</span>,:)));</span><br><span class="line">yymin=min(min(w2(<span class="number">2</span>,:)),min(w3(<span class="number">2</span>,:)));</span><br><span class="line">yymax=max(max(w2(<span class="number">2</span>,:)),max(w3(<span class="number">2</span>,:)));</span><br><span class="line">xxindex=xmin<span class="number">-1</span>:(xxmax-xxmin)/<span class="number">100</span>:xxmax+<span class="number">1</span>;</span><br><span class="line">yyindex=-vv(<span class="number">2</span>)*xxindex/vv(<span class="number">3</span>)-vv(<span class="number">1</span>)/vv(<span class="number">3</span>);</span><br><span class="line">plot(xxindex,yyindex);</span><br></pre></td></tr></table></figure><h1 id="实验结果与讨论"><a href="#实验结果与讨论" class="headerlink" title="实验结果与讨论"></a>实验结果与讨论</h1><ol><li>Fisher准则的线性分类器如下图所示，其中5个点有2个被分为第一类，剩下3个被分为第二类。</li></ol><p><img src="/2018/10/20/《模式识别》实验4-线性分类器设计/1.png" alt=""></p><ol><li><p>两次分类结果如下图所示，其中y1和y2的分类在28轮收敛，其中y2和y3的分类在23轮收敛。</p><p><img src="/2018/10/20/《模式识别》实验4-线性分类器设计/2.png" alt=""></p><p><img src="/2018/10/20/《模式识别》实验4-线性分类器设计/3.png" alt=""></p></li></ol><h1 id="实验总结"><a href="#实验总结" class="headerlink" title="实验总结"></a>实验总结</h1><p>​        本次实验聚焦于线性分类器的设计，通过实践完成基于Fisher准则和基于感知准则的两个线性分类器的设计，我真正领会了其数学原理，进一步加深了对它们的理解，这必将有利于我进一步的学习。值得注意的是，本次所使用的数据均线性可分，对于线性不可分以及高维数据的情况，需要进一步讨论。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;实验目的&quot;&gt;&lt;a href=&quot;#实验目的&quot; class=&quot;headerlink&quot; title=&quot;实验目的&quot;&gt;&lt;/a&gt;实验目的&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;了解基于Fisher准则的线性分类器和基于感知准则的线性分类器的数学原理。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;
      
    
    </summary>
    
      <category term="模式识别" scheme="http://wang22ti.com/categories/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>《模式识别》实验3-概率密度函数估计</title>
    <link href="http://wang22ti.com/2018/10/20/%E3%80%8A%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E3%80%8B%E5%AE%9E%E9%AA%8C3-%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6%E5%87%BD%E6%95%B0%E4%BC%B0%E8%AE%A1/"/>
    <id>http://wang22ti.com/2018/10/20/《模式识别》实验3-概率密度函数估计/</id>
    <published>2018-10-20T02:55:03.000Z</published>
    <updated>2018-11-12T08:36:41.704Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实验目的"><a href="#实验目的" class="headerlink" title="实验目的"></a>实验目的</h1><ol><li><p>了解常用的概率密度函数估计的方法：参数估计法和非参数估计法，其中后者包括parzen窗和Kn近邻两种方法。</p></li><li><p>掌握正态分布的参数估计的两种求解方法：极大似然估计和贝叶斯估计。</p></li></ol><h1 id="实验原理"><a href="#实验原理" class="headerlink" title="实验原理"></a>实验原理</h1><h2 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h2><p>对于正态分布的极大似然估计，有</p><script type="math/tex; mode=display">\mathrm { P } ( \mathrm { x } ; \mu , \sigma ) = \frac { 1 } { \sqrt { 2 \pi } \sigma } e ^ { - \frac { ( x - \mu ) ^ { 2 } } { 2 \sigma ^ { 2 } } }</script><p>当有多个独立同分布的样本之后有</p><script type="math/tex; mode=display">\mathrm { P } ( \mathbf { x } ; \mu , \sigma ) = \prod _ { i = 1 } ^ { N } \frac { 1 } { \sqrt { 2 \pi } \sigma } e ^ { - \frac { \left( x _ { i } - \mu \right) ^ { 2 } } { 2 \sigma ^ { 2 } } } = \frac { 1 } { ( 2 \pi \sigma ) ^ { \frac { N } { 2 } } } e ^ { \sum _ { i = 1 } ^ { N } - \frac { \left( x _ { i } - \mu \right) ^ { 2 } } { 2 \sigma ^ { 2 } } }</script><p>两边取对数有</p><script type="math/tex; mode=display">\ln \mathrm { P } ( \mathbf { x } ; \mu , \sigma ) = - \frac { N } { 2 } \ln 2 \pi \sigma + \sum _ { i = 1 } ^ { N } - \frac { \left( x _ { i } - \mu \right) ^ { 2 } } { 2 \sigma ^ { 2 } }</script><p>求导有</p><script type="math/tex; mode=display">\hat { \mu } = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } x _ { i } , \quad \hat { \Sigma } = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } ( x - \hat { \mu } ) ( x - \hat { \mu } ) ^ { T }</script><h2 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h2><p>在贝叶斯估计中，参数也是随机变量，则有 </p><script type="math/tex; mode=display">\hat { \mu } = \frac { \sigma _ { 0 } ^ { 2 } } { N \sigma _ { 0 } ^ { 2 } + \sigma ^ { 2 } } \sum _ { i = 1 } ^ { N } x _ { i } + \frac { \sigma _ { 0 } ^ { 2 } } { N \sigma _ { 0 } ^ { 2 } + \sigma ^ { 2 } } \mu _ { 0 } , \quad \widehat { \sigma ^ { 2 } } = \frac { \sigma _ { 0 } ^ { 2 } \sigma ^ { 2 } } { N \sigma _ { 0 } ^ { 2 } + \sigma ^ { 2 } }</script><p>其中$\mu$的先验概率服从$\mathrm { N } \left( \mu _ { 0 } , \sigma _ { 0 } ^ { 2 } \right)$，总体分布密度服从$\mathrm { N } \left( \mu , \sigma ^ { 2 } \right)$</p><h2 id="非参数估计"><a href="#非参数估计" class="headerlink" title="非参数估计"></a>非参数估计</h2><p>在非参数估计中有</p><script type="math/tex; mode=display">\hat { p } ( x ) = \frac { k } { V N }</script><p>其中$N$为样本总数，$V$为区域体积，$K$为落入区域的样本数，同时有</p><script type="math/tex; mode=display">\mathrm { k } = \sum _ { i = 1 } ^ { N } \varphi \left( \frac { \left| x - x _ { i } \right| } { h } \right)</script><p>其中$h$为窗口长度，$\varphi$为判别是否在区域内的函数。如果区域太小，则概率密度函数不连续；如果区域太大，则概率密度函数不准确。所以要选取恰当的区域体积，Parzen窗口法令</p><script type="math/tex; mode=display">\mathrm { V } = \frac { h } { \sqrt { N } }</script><p>其中$h$为常数。在K近邻法则令</p><script type="math/tex; mode=display">\mathrm { K } = \mathrm { k } \sqrt { N }</script><p>选取合适的$V$使得区域恰好包含K个近邻。</p><h1 id="实验内容与步骤"><a href="#实验内容与步骤" class="headerlink" title="实验内容与步骤"></a>实验内容与步骤</h1><h2 id="极大似然估计-1"><a href="#极大似然估计-1" class="headerlink" title="极大似然估计"></a>极大似然估计</h2><p>利用最大似然参数估计方法计算单变量正态分布情况下的参数：均值$\mu$和方差$\sigma^2$，并利用MATLAB画出给出的样本数据和求得的概率密度函数；样本数据：x={0.42 -0.2 1.3 0.39 -1.6 0.029 -0.23 0.27 -1.9 0.87}</p><h3 id="sd-mle-m"><a href="#sd-mle-m" class="headerlink" title="sd_mle.m"></a>sd_mle.m</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[u, s]</span>=<span class="title">sd_mle</span><span class="params">(x)</span></span></span><br><span class="line"><span class="comment">%单维样本的最大似然估计（3-24，3-25</span></span><br><span class="line">len=<span class="built_in">numel</span>(x);           <span class="comment">%求数据个数N</span></span><br><span class="line">u=sum(x)/len;           <span class="comment">%求均值的估计</span></span><br><span class="line">s=sum((x-u)'*(x-u))/len;     <span class="comment">%求方差的估计</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="ztsr-m"><a href="#ztsr-m" class="headerlink" title="ztsr.m"></a>ztsr.m</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">clear all;</span><br><span class="line">clc;</span><br><span class="line">w1=[<span class="number">0.42</span> <span class="number">-0.2</span> <span class="number">1.3</span> <span class="number">0.39</span> <span class="number">-1.6</span> <span class="number">-0.029</span> <span class="number">-0.23</span> <span class="number">0.27</span> <span class="number">-1.9</span> <span class="number">0.87</span>]';</span><br><span class="line"><span class="comment">%--------问题a---------</span></span><br><span class="line"><span class="comment">%类别w1中特征1的最大似然估计</span></span><br><span class="line">[u1, s]=sd_mle(w1); <span class="comment">%最大似然估计</span></span><br><span class="line"><span class="built_in">disp</span>(u1);</span><br><span class="line"><span class="built_in">disp</span>(s);</span><br><span class="line"><span class="comment">%画出采样数据</span></span><br><span class="line">figure;</span><br><span class="line">hold on;</span><br><span class="line"><span class="built_in">i</span>(<span class="number">1</span>:<span class="number">10</span>,<span class="number">1</span>)=<span class="number">0.2</span>;</span><br><span class="line">scatter(w1,<span class="built_in">i</span>,<span class="string">'ob'</span>); <span class="comment">%画点函数</span></span><br><span class="line"><span class="built_in">j</span>=<span class="built_in">linspace</span>(<span class="number">-0.1</span>,<span class="number">0.6</span>,<span class="number">100</span>);</span><br><span class="line"><span class="comment">%画出估计值线</span></span><br><span class="line">plot(u1,<span class="built_in">j</span>,<span class="string">'-b'</span>);</span><br><span class="line"><span class="comment">%画出正态曲线</span></span><br><span class="line">x=<span class="built_in">linspace</span>(<span class="number">-10</span>,<span class="number">10</span>,<span class="number">200</span>);</span><br><span class="line">y=<span class="number">1</span> /(<span class="built_in">sqrt</span>(<span class="number">2</span>*<span class="built_in">pi</span>*s))*<span class="built_in">exp</span>(-(x-u1).^<span class="number">2</span>/(<span class="number">2</span>*s));</span><br><span class="line">plot(x,y,<span class="string">'g'</span>);</span><br><span class="line">title(<span class="string">'一维正态最大似然参数估计'</span>);</span><br><span class="line">grid on;</span><br><span class="line">hold off;</span><br></pre></td></tr></table></figure><h2 id="贝叶斯估计-1"><a href="#贝叶斯估计-1" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h2><p>利用贝叶斯参数估计的方法计算一维正态概率密度函数的参数均值$\mu$，并利MATLAB函数画出样本数据和估计出的概率密度函数图。样本数据：x={0.42 -0.2 1.3 0.39 -1.6 0.029 -0.23 0.27 -1.9 0.87}。概率密度函数的方差：s=0.9062；均值的$\mu$先验分布也是正态分布，参数$\mu _ { 0 } = - 0.0709 , \sigma _ { 0 } ^ { 2 } = 1$。</p><h3 id="ubeyes-m"><a href="#ubeyes-m" class="headerlink" title="ubeyes.m"></a>ubeyes.m</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[u,ss]</span>=<span class="title">ubeyes</span><span class="params">(x,s,u0,s0)</span></span></span><br><span class="line">N=<span class="built_in">numel</span>(x);</span><br><span class="line">u=s0 * sum(x) / (N*s0+s) + s*u0/(N*s0+s);</span><br><span class="line">ss=s*s0/(N*s0+s);</span><br></pre></td></tr></table></figure><h3 id="beyes-m"><a href="#beyes-m" class="headerlink" title="beyes.m"></a>beyes.m</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%一维正态的贝叶斯参数估计，均值u是待估计参数，方差s已知</span></span><br><span class="line">clear all;</span><br><span class="line">clc;</span><br><span class="line"><span class="comment">%w2为观测数据</span></span><br><span class="line">w2=[<span class="number">0.42</span> <span class="number">-0.2</span> <span class="number">1.3</span> <span class="number">0.39</span> <span class="number">-1.6</span> <span class="number">-0.029</span> <span class="number">-0.23</span> <span class="number">0.27</span> <span class="number">-1.9</span> <span class="number">0.87</span>]';</span><br><span class="line">s=<span class="number">0.9062</span>; <span class="comment">%样本概率密度函数的方差已知</span></span><br><span class="line">u0=<span class="number">-0.0709</span>;s0=<span class="number">1</span>;  <span class="comment">%均值u的先验分布也是正态分布，均值，方差已知</span></span><br><span class="line">[u,ss]=ubeyes(w2,s,u0,s0); <span class="comment">%ss 为估计均值后的方差变化</span></span><br><span class="line"><span class="comment">%画出采样数据</span></span><br><span class="line">figure;</span><br><span class="line"><span class="built_in">i</span>(<span class="number">1</span>:<span class="number">10</span>,<span class="number">1</span>)=<span class="number">0.2</span>;</span><br><span class="line">scatter(w2,<span class="built_in">i</span>);  <span class="comment">%画线函数</span></span><br><span class="line">hold on;</span><br><span class="line"><span class="comment">%画出正态曲线</span></span><br><span class="line">x=<span class="built_in">linspace</span>(<span class="number">-10</span>,<span class="number">10</span>,<span class="number">200</span>);</span><br><span class="line">y1=<span class="number">1.</span>/(<span class="built_in">sqrt</span>(<span class="number">2</span>*<span class="built_in">pi</span>*s))*<span class="built_in">exp</span>(-(x-u).^<span class="number">2</span>/(<span class="number">2</span>*s));</span><br><span class="line">y2=<span class="number">1.</span>/(<span class="built_in">sqrt</span>(<span class="number">2</span>*<span class="built_in">pi</span>*(s+ss)))*<span class="built_in">exp</span>(-(x-u).^<span class="number">2</span>/(<span class="number">2</span>*(s+ss)));</span><br><span class="line">plot(x,y1,<span class="string">'r'</span>); <span class="comment">%红色线是确定的方差下的概率密度函数</span></span><br><span class="line">plot(x,y2,<span class="string">'g'</span>); <span class="comment">%青色线是估计后变化了的方差下的概率密度函数</span></span><br><span class="line">legend(<span class="string">'样本数据'</span>,<span class="string">'确定方差下的概率密度函数'</span>,<span class="string">'变化了的方差下的概率密度函数'</span>);</span><br><span class="line"><span class="built_in">j</span>=<span class="built_in">linspace</span>(<span class="number">-0.1</span>,<span class="number">0.6</span>,<span class="number">100</span>);</span><br><span class="line"><span class="comment">%画出估计值线</span></span><br><span class="line">plot(u,<span class="built_in">j</span>,<span class="string">'r'</span>);</span><br><span class="line">title(<span class="string">'贝叶斯'</span>);</span><br><span class="line">grid on;</span><br><span class="line">hold off;</span><br></pre></td></tr></table></figure><h2 id="Parzen窗法"><a href="#Parzen窗法" class="headerlink" title="Parzen窗法"></a>Parzen窗法</h2><p>利用Parzen窗法，估计单一正态分布和两个均匀分布，样本数N分别为1、16、256、 无穷大，用一个比较大的数据代替，窗口宽度分别为：0.25、1、4，画出实验结果图。实验代码如下</p><h3 id="Parzen-m"><a href="#Parzen-m" class="headerlink" title="Parzen.m"></a>Parzen.m</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">p</span> = <span class="title">Parzen</span><span class="params">(x, X, h1, N)</span>  % <span class="title">x</span>为横坐标; <span class="title">X</span>为样本; <span class="title">h1</span>用来调节窗宽<span class="title">h</span>; <span class="title">N</span>为样本个数. </span></span><br><span class="line"><span class="comment">% Parzen窗口法的原理实现</span></span><br><span class="line">h = h1/<span class="built_in">sqrt</span>(N);                    <span class="comment">% (1) h为窗宽.</span></span><br><span class="line">Kn = <span class="built_in">zeros</span>(<span class="number">1</span>, <span class="number">100</span>); </span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:N </span><br><span class="line">Kn = Kn +  normpdf((x-X(<span class="built_in">i</span>))/h);   <span class="comment">% (2) 用正态窗函数作Parzen窗估计,计算落入Vn内的样本数</span></span><br><span class="line"><span class="keyword">end</span>                     <span class="comment">% normpdf：正态概率密度函数</span></span><br><span class="line">p = Kn/(h*N);                    <span class="comment">% (3) 密度估计</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="myparzen01-m"><a href="#myparzen01-m" class="headerlink" title="myparzen01.m"></a>myparzen01.m</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%%%%%% Parzen窗口法估计标准正态分布实验 %%%%%%</span></span><br><span class="line">clc;</span><br><span class="line">close all;</span><br><span class="line">clear all;</span><br><span class="line">M = <span class="number">128</span>^<span class="number">2</span>;    <span class="comment">%调整M的值可以改进精度   </span></span><br><span class="line">x = <span class="built_in">linspace</span>(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">100</span>); <span class="comment">%功能：用于产生-3,3之间的100点行线性的矢量。</span></span><br><span class="line">X = <span class="built_in">randn</span>(M, <span class="number">1</span>); <span class="comment">%产生标准正态分布</span></span><br><span class="line">figure;</span><br><span class="line">p = Parzen(x, X, <span class="number">0.25</span>, <span class="number">1</span>);     </span><br><span class="line">subplot(<span class="number">4</span>,<span class="number">3</span>,<span class="number">1</span>);plot(x, p);title(<span class="string">'N =1, h1 = 0.25'</span>);axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]);grid on;</span><br><span class="line">p = Parzen(x, X, <span class="number">1</span>, <span class="number">1</span>);        </span><br><span class="line">subplot(<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>);plot(x, p);title(<span class="string">'N =1, h1 = 1'</span>);axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]); grid on;</span><br><span class="line">p = Parzen(x, X, <span class="number">4</span>, <span class="number">1</span>);        </span><br><span class="line">subplot(<span class="number">4</span>,<span class="number">3</span>,<span class="number">3</span>);plot(x, p);title(<span class="string">'N =1, h1 = 4'</span>);axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]); grid on;</span><br><span class="line">p = Parzen(x, X, <span class="number">0.25</span>, <span class="number">16</span>);   </span><br><span class="line">subplot(<span class="number">4</span>,<span class="number">3</span>,<span class="number">4</span>);plot(x, p);title(<span class="string">'N =16, h1 = 0.25'</span>);axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]); grid on;</span><br><span class="line">p = Parzen(x, X, <span class="number">1</span>, <span class="number">16</span>);      </span><br><span class="line">subplot(<span class="number">4</span>,<span class="number">3</span>,<span class="number">5</span>);plot(x, p);title(<span class="string">'N =16, h1 = 1'</span>);axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]); grid on;</span><br><span class="line">p = Parzen(x, X, <span class="number">4</span>, <span class="number">16</span>);       </span><br><span class="line">subplot(<span class="number">4</span>,<span class="number">3</span>,<span class="number">6</span>);plot(x, p);title(<span class="string">'N =16, h1 = 4'</span>);axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]); grid on;</span><br><span class="line">p = Parzen(x, X, <span class="number">0.25</span>, <span class="number">256</span>);   </span><br><span class="line">subplot(<span class="number">4</span>,<span class="number">3</span>,<span class="number">7</span>);plot(x, p);title(<span class="string">'N =256, h1 = 0.25'</span>);axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]); grid on;</span><br><span class="line">p = Parzen(x, X, <span class="number">1</span>, <span class="number">256</span>);      </span><br><span class="line">subplot(<span class="number">4</span>,<span class="number">3</span>,<span class="number">8</span>);plot(x, p);title(<span class="string">'N =256, h1 = 1'</span>);axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]); grid on;</span><br><span class="line">p = Parzen(x, X, <span class="number">4</span>,<span class="number">256</span>);      </span><br><span class="line">subplot(<span class="number">4</span>,<span class="number">3</span>,<span class="number">9</span>);plot(x, p);title(<span class="string">'N =256, h1 = 4'</span>);axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]); grid on;</span><br><span class="line">p = Parzen(x, X, <span class="number">0.25</span>, M);   </span><br><span class="line">subplot(<span class="number">4</span>,<span class="number">3</span>,<span class="number">10</span>);plot(x, p);title(<span class="string">'N =128^2, h1 = 0.25'</span>);axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]); grid on;</span><br><span class="line">p = Parzen(x, X, <span class="number">1</span>, M);      </span><br><span class="line">subplot(<span class="number">4</span>,<span class="number">3</span>,<span class="number">11</span>);plot(x, p);title(<span class="string">'N =128^2, h1 = 1'</span>);axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]); grid on;</span><br><span class="line">p = Parzen(x, X, <span class="number">4</span>, M);      </span><br><span class="line">subplot(<span class="number">4</span>,<span class="number">3</span>,<span class="number">12</span>);plot(x, p);title(<span class="string">'N =128^2, h1 = 4'</span>);axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]); grid on;</span><br><span class="line">grid on;</span><br></pre></td></tr></table></figure><h3 id="myparzen02-m"><a href="#myparzen02-m" class="headerlink" title="myparzen02.m"></a>myparzen02.m</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%%%%%% Parzen窗口法估计两个均匀分布实验 %%%%%%</span></span><br><span class="line">clc;</span><br><span class="line">close all;</span><br><span class="line">clear all;</span><br><span class="line">M = <span class="number">128</span>^<span class="number">2</span>;    <span class="comment">%调整M的值可以改进精度   </span></span><br><span class="line">x = <span class="built_in">linspace</span>(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">100</span>); <span class="comment">%功能：用于产生-3,3之间的100点行线性的矢量。</span></span><br><span class="line">n = M;</span><br><span class="line"><span class="built_in">i</span>=<span class="number">1</span>;</span><br><span class="line"><span class="keyword">while</span> <span class="built_in">i</span> &lt;= n <span class="comment">%产生均匀分布双峰概密随机数，x分布在-2.5到-2之间概率为0.5，x分布在0到2之间概率为0.5；</span></span><br><span class="line">    R(<span class="built_in">i</span>)=<span class="built_in">rand</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">if</span> R(<span class="built_in">i</span>)&gt;<span class="number">0.5</span></span><br><span class="line">        p(<span class="number">1</span>,<span class="built_in">i</span>)=<span class="number">2</span>*<span class="built_in">rand</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">elseif</span> R(<span class="built_in">i</span>)&lt;<span class="number">0.5</span></span><br><span class="line">        p(<span class="number">1</span>,<span class="built_in">i</span>)=<span class="number">0.5</span>*<span class="built_in">rand</span>(<span class="number">1</span>)<span class="number">-2.5</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        null;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="built_in">i</span>=<span class="built_in">i</span>+<span class="number">1</span>;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">X=p;</span><br><span class="line">figure;</span><br><span class="line">p = Parzen(x, X, <span class="number">0.25</span>, <span class="number">1</span>);     </span><br><span class="line">subplot(<span class="number">4</span>,<span class="number">3</span>,<span class="number">1</span>);plot(x, p);title(<span class="string">'N =1, h1 = 0.25'</span>);axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]);grid on;</span><br><span class="line">p = Parzen(x, X, <span class="number">1</span>, <span class="number">1</span>);        </span><br><span class="line">subplot(<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>);plot(x, p);title(<span class="string">'N =1, h1 = 1'</span>);axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]); grid on;</span><br><span class="line">p = Parzen(x, X, <span class="number">4</span>, <span class="number">1</span>);        </span><br><span class="line">subplot(<span class="number">4</span>,<span class="number">3</span>,<span class="number">3</span>);plot(x, p);title(<span class="string">'N =1, h1 = 4'</span>);axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]); grid on;</span><br><span class="line">p = Parzen(x, X, <span class="number">0.25</span>, <span class="number">16</span>);   </span><br><span class="line">subplot(<span class="number">4</span>,<span class="number">3</span>,<span class="number">4</span>);plot(x, p);title(<span class="string">'N =16, h1 = 0.25'</span>);axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]); grid on;</span><br><span class="line">p = Parzen(x, X, <span class="number">1</span>, <span class="number">16</span>);      </span><br><span class="line">subplot(<span class="number">4</span>,<span class="number">3</span>,<span class="number">5</span>);plot(x, p);title(<span class="string">'N =16, h1 = 1'</span>);axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]); grid on;</span><br><span class="line">p = Parzen(x, X, <span class="number">4</span>, <span class="number">16</span>);       </span><br><span class="line">subplot(<span class="number">4</span>,<span class="number">3</span>,<span class="number">6</span>);plot(x, p);title(<span class="string">'N =16, h1 = 4'</span>);axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]); grid on;</span><br><span class="line"></span><br><span class="line">p = Parzen(x, X, <span class="number">0.25</span>, <span class="number">256</span>);   </span><br><span class="line">subplot(<span class="number">4</span>,<span class="number">3</span>,<span class="number">7</span>);plot(x, p);title(<span class="string">'N =256, h1 = 0.25'</span>);axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]); grid on;</span><br><span class="line">p = Parzen(x, X, <span class="number">1</span>, <span class="number">256</span>);      </span><br><span class="line">subplot(<span class="number">4</span>,<span class="number">3</span>,<span class="number">8</span>);plot(x, p);title(<span class="string">'N =256, h1 = 1'</span>);axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]); grid on;</span><br><span class="line">p = Parzen(x, X, <span class="number">4</span>,<span class="number">256</span>);      </span><br><span class="line">subplot(<span class="number">4</span>,<span class="number">3</span>,<span class="number">9</span>);plot(x, p);title(<span class="string">'N =256, h1 = 4'</span>);axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]); grid on;</span><br><span class="line">p = Parzen(x, X, <span class="number">0.25</span>, M);   </span><br><span class="line">subplot(<span class="number">4</span>,<span class="number">3</span>,<span class="number">10</span>);plot(x, p);title(<span class="string">'N =128^2, h1 = 0.25'</span>);axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]); grid on;</span><br><span class="line">p = Parzen(x, X, <span class="number">1</span>, M);      </span><br><span class="line">subplot(<span class="number">4</span>,<span class="number">3</span>,<span class="number">11</span>);plot(x, p);title(<span class="string">'N =128^2, h1 = 1'</span>);axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]); grid on;</span><br><span class="line">p = Parzen(x, X, <span class="number">4</span>, M);      </span><br><span class="line">subplot(<span class="number">4</span>,<span class="number">3</span>,<span class="number">12</span>);plot(x, p);title(<span class="string">'N =128^2, h1 = 4'</span>);axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>]); grid on;</span><br><span class="line">grid on;</span><br></pre></td></tr></table></figure><h2 id="Kn近邻法"><a href="#Kn近邻法" class="headerlink" title="Kn近邻法"></a>Kn近邻法</h2><p>利用Kn近邻法，估计单一正态分布和两个均匀分布，样本数N分别为1、16、256、 无穷大，用一个比较大的数据代替，画出实验结果图。</p><h3 id="knjl01-m"><a href="#knjl01-m" class="headerlink" title="knjl01.m"></a>knjl01.m</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">p</span>=<span class="title">knjl01</span><span class="params">(x,N)</span> </span></span><br><span class="line"><span class="comment">% 根据Kn近邻法对下列四处代码补全</span></span><br><span class="line">xb=<span class="built_in">randn</span>(N,<span class="number">1</span>);<span class="comment">%产生标准正态分布</span></span><br><span class="line"><span class="comment">%Kn近邻法的原理实现</span></span><br><span class="line">Kn=<span class="built_in">sqrt</span>(N);                         <span class="comment">% (1) 计算Kn</span></span><br><span class="line">p=<span class="built_in">zeros</span>(<span class="number">1</span>,<span class="number">100</span>); </span><br><span class="line"><span class="keyword">for</span> <span class="built_in">j</span>=<span class="number">1</span>:<span class="number">100</span>  </span><br><span class="line">    d=sort(<span class="built_in">abs</span>(xb-x(<span class="built_in">j</span>)));      <span class="comment">% (2) 用当前所处位置的x值减去样本点的值，并对其绝对值按升序排序</span></span><br><span class="line">    Vn=d(<span class="built_in">ceil</span>(Kn)) * <span class="number">2</span>;                     <span class="comment">%（3）计算包含kn个样本点所需的体积</span></span><br><span class="line">    p(<span class="built_in">j</span>)=Kn/(N * Vn);                   <span class="comment">%（4）计算该位置的密度</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="myjl01-m"><a href="#myjl01-m" class="headerlink" title="myjl01.m"></a>myjl01.m</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%%%%%% Kn近邻法估计标准正态分布实验 %%%%%%</span></span><br><span class="line">clc;  </span><br><span class="line">clear all; </span><br><span class="line">close all; </span><br><span class="line">x=<span class="built_in">linspace</span>(<span class="number">-3</span>,<span class="number">3</span>,<span class="number">100</span>); </span><br><span class="line">N=<span class="number">1</span>;  </span><br><span class="line">p1=knjl01(x,N);  </span><br><span class="line">figure;</span><br><span class="line">subplot(<span class="number">1</span>,<span class="number">4</span>,<span class="number">1</span>),plot(x,p1); </span><br><span class="line">title(<span class="string">'N=1'</span>); </span><br><span class="line">N=<span class="number">16</span>;  </span><br><span class="line">p2=knjl01(x,N);  </span><br><span class="line">subplot(<span class="number">1</span>,<span class="number">4</span>,<span class="number">2</span>),plot(x,p2); </span><br><span class="line">title(<span class="string">'N=16'</span>); </span><br><span class="line">N=<span class="number">256</span>;  </span><br><span class="line">p3=knjl01(x,N);  </span><br><span class="line">subplot(<span class="number">1</span>,<span class="number">4</span>,<span class="number">3</span>);plot(x,p3); </span><br><span class="line">title(<span class="string">'N=256'</span>); </span><br><span class="line">N=<span class="number">128</span>^<span class="number">2</span>;  </span><br><span class="line">p4=knjl01(x,N);  </span><br><span class="line">subplot(<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>);plot(x,p4); </span><br><span class="line">title(<span class="string">'N=128^2'</span>);</span><br></pre></td></tr></table></figure><h3 id="knjl02-m"><a href="#knjl02-m" class="headerlink" title="knjl02.m"></a>knjl02.m</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">p</span>=<span class="title">knjl02</span><span class="params">(x,N)</span> </span></span><br><span class="line"><span class="comment">% 根据Kn近邻法对下列四处代码补全</span></span><br><span class="line">xb=<span class="built_in">zeros</span>(N,<span class="number">1</span>);</span><br><span class="line">n=N; </span><br><span class="line"><span class="built_in">i</span>=<span class="number">1</span>;  </span><br><span class="line"><span class="keyword">while</span> <span class="built_in">i</span>&lt;=n <span class="comment">%产生均匀分布双峰概密随机数，x分布在-2.5到-2之间的概率为0.5;x分布在0到2之间的概率为0.5 </span></span><br><span class="line">    R(<span class="built_in">i</span>)=<span class="built_in">rand</span>(<span class="number">1</span>); </span><br><span class="line">    <span class="keyword">if</span> R(<span class="built_in">i</span>)&gt;<span class="number">0.5</span>  </span><br><span class="line">        q(<span class="number">1</span>,<span class="built_in">i</span>)= <span class="number">2</span>*<span class="built_in">rand</span>(<span class="number">1</span>);                <span class="comment">%概率密度函数变后，改动这里</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        q(<span class="number">1</span>,<span class="built_in">i</span>)= <span class="number">0.5</span>*<span class="built_in">rand</span>() <span class="number">-2.5</span>;                 <span class="comment">%概率密度函数变化后，改动这里 </span></span><br><span class="line">    <span class="keyword">end</span> </span><br><span class="line">    <span class="built_in">i</span>=<span class="built_in">i</span>+<span class="number">1</span>; </span><br><span class="line"><span class="keyword">end</span> </span><br><span class="line">xb=q; </span><br><span class="line"><span class="comment">%Kn近邻法的原理实现</span></span><br><span class="line">Kn=<span class="built_in">sqrt</span>(N);                         <span class="comment">% (1) 计算Kn</span></span><br><span class="line">p=<span class="built_in">zeros</span>(<span class="number">1</span>,<span class="number">100</span>); </span><br><span class="line"><span class="keyword">for</span> <span class="built_in">j</span>=<span class="number">1</span>:<span class="number">100</span>  </span><br><span class="line">    d=sort(<span class="built_in">abs</span>(xb-x(<span class="built_in">j</span>)));      <span class="comment">% (2) 用当前所处位置的x值减去样本点的值，并对其绝对值按升序排序</span></span><br><span class="line">    Vn=d(<span class="built_in">ceil</span>(Kn)) * <span class="number">2</span>;                     <span class="comment">%（3）计算包含kn个样本点所需的体积</span></span><br><span class="line">    p(<span class="built_in">j</span>)=Kn/(N * Vn);                   <span class="comment">%（4）计算该位置的密度</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="myjl02-m"><a href="#myjl02-m" class="headerlink" title="myjl02.m"></a>myjl02.m</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%%%%%% Kn近邻法估计两个均匀分布实验 %%%%%%</span></span><br><span class="line">clc;  </span><br><span class="line">clear all; </span><br><span class="line">close all; </span><br><span class="line">x=<span class="built_in">linspace</span>(<span class="number">-3</span>,<span class="number">3</span>,<span class="number">100</span>); </span><br><span class="line">N=<span class="number">1</span>;  </span><br><span class="line">p1=knjl02(x,N);  </span><br><span class="line">figure;</span><br><span class="line">subplot(<span class="number">1</span>,<span class="number">4</span>,<span class="number">1</span>),plot(x,p1); </span><br><span class="line">title(<span class="string">'N=1'</span>); </span><br><span class="line">N=<span class="number">16</span>;  </span><br><span class="line">p2=knjl02(x,N);  </span><br><span class="line">subplot(<span class="number">1</span>,<span class="number">4</span>,<span class="number">2</span>),plot(x,p2); </span><br><span class="line">title(<span class="string">'N=16'</span>); </span><br><span class="line">N=<span class="number">256</span>;  </span><br><span class="line">p3=knjl02(x,N);  </span><br><span class="line">subplot(<span class="number">1</span>,<span class="number">4</span>,<span class="number">3</span>);plot(x,p3); </span><br><span class="line">title(<span class="string">'N=256'</span>); </span><br><span class="line">N=<span class="number">128</span>^<span class="number">2</span>;  </span><br><span class="line">p4=knjl02(x,N);  </span><br><span class="line">subplot(<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>);plot(x,p4); </span><br><span class="line">title(<span class="string">'N=128^2'</span>);</span><br></pre></td></tr></table></figure><h1 id="实验结果与讨论"><a href="#实验结果与讨论" class="headerlink" title="实验结果与讨论"></a>实验结果与讨论</h1><h2 id="极大似然估计-2"><a href="#极大似然估计-2" class="headerlink" title="极大似然估计"></a>极大似然估计</h2><p>正态分布的极大似然估计的结果如下图所示</p><p><img src="/2018/10/20/《模式识别》实验3-概率密度函数估计/1.png" alt=""></p><h2 id="贝叶斯估计-2"><a href="#贝叶斯估计-2" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h2><p>正态分布的贝叶斯估计的结果如下图所示<br><img src="/2018/10/20/《模式识别》实验3-概率密度函数估计/2.png" alt=""></p><h2 id="Parzen窗法-1"><a href="#Parzen窗法-1" class="headerlink" title="Parzen窗法"></a>Parzen窗法</h2><p>在不同样本数量和不同窗口长度情况下，Parzen窗的实验结果如下，前者是正态分布，后者是分段函数。</p><p><img src="/2018/10/20/《模式识别》实验3-概率密度函数估计/3.png" alt=""></p><p><img src="/2018/10/20/《模式识别》实验3-概率密度函数估计/4.png" alt=""></p><h2 id="Kn近邻法-1"><a href="#Kn近邻法-1" class="headerlink" title="Kn近邻法"></a>Kn近邻法</h2><p>K近邻的实验结果如下，前者是正态分布，后者是分段函数。</p><p><img src="/2018/10/20/《模式识别》实验3-概率密度函数估计/5.png" alt=""></p><p><img src="/2018/10/20/《模式识别》实验3-概率密度函数估计/6.png" alt=""></p><h1 id="实验总结"><a href="#实验总结" class="headerlink" title="实验总结"></a>实验总结</h1><p>​        本次实验聚焦于概率密度估计，通过实践完成参数估计的极大似然估计、贝叶斯估计，以及非参数估计的Parzen窗方法和K近邻方法，我真正领会了其数学原理，进一步加深了对它们的理解，这必将有利于我进一步的学习。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;实验目的&quot;&gt;&lt;a href=&quot;#实验目的&quot; class=&quot;headerlink&quot; title=&quot;实验目的&quot;&gt;&lt;/a&gt;实验目的&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;了解常用的概率密度函数估计的方法：参数估计法和非参数估计法，其中后者包括parzen窗和Kn近邻两种方法。
      
    
    </summary>
    
      <category term="模式识别" scheme="http://wang22ti.com/categories/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>《人工智能》作业集</title>
    <link href="http://wang22ti.com/2018/10/20/%E3%80%8A%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E3%80%8B%E4%BD%9C%E4%B8%9A%E9%9B%86/"/>
    <id>http://wang22ti.com/2018/10/20/《人工智能》作业集/</id>
    <published>2018-10-20T02:54:01.000Z</published>
    <updated>2018-11-13T03:19:49.583Z</updated>
    
    <content type="html"><![CDATA[<p>虽然王公仆老师布置的作业和考试并没有直接关系，但是实事求是的说，王老师《人工智能》课程的内容还是比较科学合理，能够勾起大家学习兴趣的。以下便是历次作业题及其解答。</p><h1 id="用遗传算法求解旅行商问题"><a href="#用遗传算法求解旅行商问题" class="headerlink" title="用遗传算法求解旅行商问题"></a>用遗传算法求解旅行商问题</h1><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p><img src="/2018/10/20/《人工智能》作业集/1-1.png" alt=""></p><h2 id="源代码1"><a href="#源代码1" class="headerlink" title="源代码1"></a>源代码1</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distance</span><span class="params">(x1, x2, y1, y2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> ((x1 - x2) ** <span class="number">2</span> + (y1 - y2) ** <span class="number">2</span>) ** <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sample</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate_cost</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.cost = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, s <span class="keyword">in</span> enumerate(self.sample):</span><br><span class="line">            j = np.argmax(s)</span><br><span class="line">            self.cost += self.distances[i][j]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_sample</span><span class="params">(self)</span>:</span></span><br><span class="line">        now = np.argmax(self.sample[<span class="number">0</span>])</span><br><span class="line">        num_check = <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> now != <span class="number">0</span>:</span><br><span class="line">            now = np.argmax(self.sample[now])</span><br><span class="line">            num_check += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span> <span class="keyword">if</span> num_check == self.num_points <span class="keyword">else</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_points, distances)</span>:</span></span><br><span class="line">        self.num_points = num_points</span><br><span class="line">        self.distances = distances</span><br><span class="line">        self.sample = np.eye(num_points)</span><br><span class="line">        <span class="keyword">while</span> self.check_sample() <span class="keyword">is</span> <span class="keyword">False</span>:</span><br><span class="line">            i, j = [random.randint(<span class="number">0</span>, self.num_points - <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">2</span>)]</span><br><span class="line">            self.sample[[i, j], :] = self.sample[[j, i], :]</span><br><span class="line">        self.calculate_cost()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">change_sample</span><span class="params">(self)</span>:</span></span><br><span class="line">        i, j = [random.randint(<span class="number">0</span>, self.num_points - <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">2</span>)]</span><br><span class="line">        self.sample[[i, j], :] = self.sample[[j, i], :]</span><br><span class="line">        <span class="keyword">while</span> self.check_sample() <span class="keyword">is</span> <span class="keyword">False</span>:</span><br><span class="line">            i, j = [random.randint(<span class="number">0</span>, self.num_points - <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">2</span>)]</span><br><span class="line">            self.sample[[i, j], :] = self.sample[[j, i], :]</span><br><span class="line">        self.calculate_cost()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Samples</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, x, y, num_samples=<span class="number">100</span>)</span>:</span></span><br><span class="line">        self.inf = <span class="number">1000</span></span><br><span class="line">        self.x, self.y, self.num_samples = x, y, num_samples</span><br><span class="line">        self.num_points = len(self.x)</span><br><span class="line">        self.points = list(zip(self.x, self.y))</span><br><span class="line">        self.distances = np.array([</span><br><span class="line">            [distance(point1[<span class="number">0</span>], point2[<span class="number">0</span>], point1[<span class="number">1</span>], point2[<span class="number">1</span>]) <span class="keyword">if</span> point1 != point2 <span class="keyword">else</span> self.inf <span class="keyword">for</span> point2 <span class="keyword">in</span></span><br><span class="line">             self.points]</span><br><span class="line">            <span class="keyword">for</span> point1 <span class="keyword">in</span> self.points</span><br><span class="line">        ])</span><br><span class="line">        self.samples = []</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.num_samples):</span><br><span class="line">            self.samples.append(Sample(self.num_points, self.distances))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, num_iterator=<span class="number">300</span>, rate_kill=<span class="number">0.05</span>, rate_change=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        num_kill = int(rate_kill * self.num_samples)</span><br><span class="line">        num_live = self.num_samples - num_kill</span><br><span class="line">        num_change = int(rate_change * self.num_samples)</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_iterator):</span><br><span class="line">            self.samples = self.samples[:num_live]</span><br><span class="line">            <span class="keyword">for</span> __ <span class="keyword">in</span> range(num_kill):</span><br><span class="line">                self.samples.append(deepcopy(self.samples[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(num_change):</span><br><span class="line">                self.samples[random.randint(<span class="number">0</span>, self.num_samples - <span class="number">1</span>)].change_sample()</span><br><span class="line"></span><br><span class="line">            self.samples.sort(key=<span class="keyword">lambda</span> s: s.cost)</span><br><span class="line">            <span class="comment"># print(_, self.samples[0].cost)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">show_result</span><span class="params">(self)</span>:</span></span><br><span class="line">        best_sample = self.samples[<span class="number">0</span>]</span><br><span class="line">        plt.figure()</span><br><span class="line">        plt.scatter(self.x, self.y)</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(self.x, self.y):</span><br><span class="line">            plt.text(x, y, (x, y), fontdict=&#123;<span class="string">'size'</span>: <span class="number">16</span>, <span class="string">'color'</span>: <span class="string">'r'</span>&#125;)</span><br><span class="line">        print(<span class="string">'least cost: '</span>, best_sample.cost)</span><br><span class="line">        <span class="keyword">for</span> i, point <span class="keyword">in</span> enumerate(best_sample.sample):</span><br><span class="line">            j = np.argmax(point)</span><br><span class="line">            plt.plot([self.x[i], self.x[j]], [self.y[i], self.y[j]])</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">points_x = [<span class="number">40</span>, <span class="number">24</span>, <span class="number">17</span>, <span class="number">22</span>, <span class="number">51</span>, <span class="number">87</span>, <span class="number">68</span>, <span class="number">84</span>, <span class="number">66</span>, <span class="number">61</span>, ]</span><br><span class="line">points_y = [<span class="number">44</span>, <span class="number">14</span>, <span class="number">22</span>, <span class="number">76</span>, <span class="number">94</span>, <span class="number">65</span>, <span class="number">52</span>, <span class="number">36</span>, <span class="number">25</span>, <span class="number">26</span>, ]</span><br><span class="line">s = Samples(points_x, points_y)</span><br><span class="line">s.train()</span><br><span class="line">s.show_result()</span><br></pre></td></tr></table></figure><h2 id="源代码2"><a href="#源代码2" class="headerlink" title="源代码2"></a>源代码2</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distance</span><span class="params">(x1, x2, y1, y2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> ((x1 - x2) ** <span class="number">2</span> + (y1 - y2) ** <span class="number">2</span>) ** <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认从index为0的点出发</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sample</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate_cost</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.cost = self.distances[<span class="number">0</span>][self.sample[<span class="number">0</span>]]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.sample) - <span class="number">1</span>):</span><br><span class="line">            a, b = self.sample[i], self.sample[i + <span class="number">1</span>]</span><br><span class="line">            self.cost += self.distances[a][b]</span><br><span class="line">        self.cost += self.distances[self.sample[<span class="number">-1</span>]][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_points, distances)</span>:</span></span><br><span class="line">        self.num_points = num_points</span><br><span class="line">        self.distances = distances</span><br><span class="line">        self.sample = list(range(<span class="number">1</span>, self.num_points))</span><br><span class="line">        random.shuffle(self.sample)</span><br><span class="line">        self.calculate_cost()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">change_sample</span><span class="params">(self)</span>:</span></span><br><span class="line">        i, j = [random.randint(<span class="number">0</span>, len(self.sample) - <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">2</span>)]</span><br><span class="line">        self.sample[i], self.sample[j] = self.sample[j], self.sample[i]</span><br><span class="line">        self.calculate_cost()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Samples</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, x, y, num_samples=<span class="number">100</span>)</span>:</span></span><br><span class="line">        self.inf = <span class="number">1000</span></span><br><span class="line">        self.x, self.y, self.num_samples = x, y, num_samples</span><br><span class="line">        self.num_points = len(self.x)</span><br><span class="line">        self.points = list(zip(self.x, self.y))</span><br><span class="line">        self.distances = np.array([</span><br><span class="line">            [distance(point1[<span class="number">0</span>], point2[<span class="number">0</span>], point1[<span class="number">1</span>], point2[<span class="number">1</span>]) <span class="keyword">if</span> point1 != point2 <span class="keyword">else</span> self.inf <span class="keyword">for</span> point2 <span class="keyword">in</span></span><br><span class="line">             self.points]</span><br><span class="line">            <span class="keyword">for</span> point1 <span class="keyword">in</span> self.points</span><br><span class="line">        ])</span><br><span class="line">        self.samples = []</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.num_samples):</span><br><span class="line">            self.samples.append(Sample(self.num_points, self.distances))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, num_iterator=<span class="number">1000</span>, rate_kill=<span class="number">0.05</span>, rate_change=<span class="number">0.3</span>)</span>:</span></span><br><span class="line">        num_kill = int(rate_kill * self.num_samples)</span><br><span class="line">        num_live = self.num_samples - num_kill</span><br><span class="line">        num_change = int(rate_change * self.num_samples)</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_iterator):</span><br><span class="line">            self.samples = self.samples[:num_live]</span><br><span class="line">            <span class="keyword">for</span> __ <span class="keyword">in</span> range(num_kill):</span><br><span class="line">                self.samples.append(deepcopy(self.samples[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(num_change):</span><br><span class="line">                self.samples[random.randint(<span class="number">0</span>, self.num_samples - <span class="number">1</span>)].change_sample()</span><br><span class="line"></span><br><span class="line">            self.samples.sort(key=<span class="keyword">lambda</span> s: s.cost)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">show_result</span><span class="params">(self)</span>:</span></span><br><span class="line">        best_sample = self.samples[<span class="number">0</span>]</span><br><span class="line">        plt.figure()</span><br><span class="line">        plt.scatter(self.x, self.y)</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(self.x, self.y):</span><br><span class="line">            plt.text(x, y, (x, y), fontdict=&#123;<span class="string">'size'</span>: <span class="number">16</span>, <span class="string">'color'</span>: <span class="string">'r'</span>&#125;)</span><br><span class="line">        print(<span class="string">'least cost: '</span>, best_sample.cost)</span><br><span class="line">        plt.plot([self.x[<span class="number">0</span>], self.x[best_sample.sample[<span class="number">0</span>]]], [self.y[<span class="number">0</span>], self.y[best_sample.sample[<span class="number">0</span>]]])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(best_sample.sample)<span class="number">-1</span>):</span><br><span class="line">            a, b = best_sample.sample[i], best_sample.sample[i+<span class="number">1</span>]</span><br><span class="line">            plt.plot([self.x[a], self.x[b]], [self.y[a], self.y[b]])</span><br><span class="line">        plt.plot([self.x[<span class="number">0</span>], self.x[best_sample.sample[<span class="number">-1</span>]]], [self.y[<span class="number">0</span>], self.y[best_sample.sample[<span class="number">-1</span>]]])</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">points_x = [<span class="number">40</span>, <span class="number">24</span>, <span class="number">17</span>, <span class="number">22</span>, <span class="number">51</span>, <span class="number">87</span>, <span class="number">68</span>, <span class="number">84</span>, <span class="number">66</span>, <span class="number">61</span>, ]</span><br><span class="line">points_y = [<span class="number">44</span>, <span class="number">14</span>, <span class="number">22</span>, <span class="number">76</span>, <span class="number">94</span>, <span class="number">65</span>, <span class="number">52</span>, <span class="number">36</span>, <span class="number">25</span>, <span class="number">26</span>, ]</span><br><span class="line">s = Samples(points_x, points_y)</span><br><span class="line">s.train()</span><br><span class="line">s.show_result()</span><br></pre></td></tr></table></figure><h2 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h2><p><img src="/2018/10/20/《人工智能》作业集/1-2.png" alt=""></p><h1 id="一个简单的估计问题"><a href="#一个简单的估计问题" class="headerlink" title="一个简单的估计问题"></a>一个简单的估计问题</h1><h2 id="题目-1"><a href="#题目-1" class="headerlink" title="题目"></a>题目</h2><p><img src="/2018/10/20/《人工智能》作业集/2-1.png" alt=""></p><h2 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_samples</span><span class="params">(N=<span class="number">1000</span>, mean=<span class="number">0</span>, sigma=<span class="number">1</span>, A=<span class="number">5</span>)</span>:</span></span><br><span class="line">    np.random.seed(int(time.time()))</span><br><span class="line">    w = np.random.normal(mean, sigma, N)</span><br><span class="line">    x = A + w</span><br><span class="line">    hat_A = np.array([x[:k+<span class="number">1</span>].mean() <span class="keyword">for</span> k <span class="keyword">in</span> range(N)])</span><br><span class="line">    var_hat_A = [hat_A[:k+<span class="number">1</span>].var() <span class="keyword">for</span> k <span class="keyword">in</span> range(N)]</span><br><span class="line">    plt.scatter(range(<span class="number">1</span>, N+<span class="number">1</span>), var_hat_A, s=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">m, n = <span class="number">5</span>, <span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        plt.subplot(m, n, m * i + j + <span class="number">1</span>)</span><br><span class="line">        plot_samples()</span><br><span class="line">        time.sleep(np.random.randint(<span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="运行结果-1"><a href="#运行结果-1" class="headerlink" title="运行结果"></a>运行结果</h2><p><img src="/2018/10/20/《人工智能》作业集/2-2.png" alt=""></p><h1 id="论文阅读"><a href="#论文阅读" class="headerlink" title="论文阅读"></a>论文阅读</h1><h2 id="题目-2"><a href="#题目-2" class="headerlink" title="题目"></a>题目</h2><p><img src="/2018/10/20/《人工智能》作业集/3-1.png" alt=""></p><h2 id="观看视频"><a href="#观看视频" class="headerlink" title="观看视频"></a>观看视频</h2><p>随着数字图像的爆炸式增长，计算机视觉的发展已经成为必要。然而图像在计算机中的存储只是一堆矩阵，这让计算机理解图片变得很困难。传统的算法直接告诉计算机目标的特征，但是并不具有泛化能力，目标的形变、遮挡等等变化就会让算法完全失效。通过对于孩子认识物体过程的深入分析，李飞飞意识到问题不来自于算法，而来自于数据量。所以她领导开发了超大高质量数据集ImageNet；同时得益于硬件的发展，训练深度卷积神经网络，很快完成了图片中物体的识别，并进一步得到一个可以指出图片中物体简单关系的模型。模型还有3个可以改进的重点：一是防止对已经学习目标的过拟合，二是对没有相关训练集目标的识别，三是可以进一步提高模型的智力水平。</p><h2 id="阅读论文"><a href="#阅读论文" class="headerlink" title="阅读论文"></a>阅读论文</h2><p>传统的机器学习方法的局限性在于需要足够的工程技巧和领域专家的专业知识才能设计好的特征提取器。深度学习是表示学习的一种，通过逐级组装简单但是非线性的部件，并使用通用训练算法，由低级向高级自动提取特征。其中的非线性部件是指非线性激活函数，例如Relu、双曲正切函数、sigmoid函数，通用训练算法是指反向传播算法，归根到底是链式求导法则的一种应用。现在最常见的激活函数是Relu，因为它收敛的速度总是比sigmoid快；人们曾经担心反向传播算法容易陷入局部最小点，但实际上基本都是鞍点。</p><p>卷积神经网络是神经网络的一种，在计算机视觉领域有着很多应用，和全连接神经网络相比更加容易训练且更加具有泛化能力。其中，卷积层可以利用相邻值之间的关联性，还可以处理平移不变性；池化层是将相似的特征归为一类。</p><p>循环神经网络是另一种神经网络，对于序列数据处理的表现要好于卷积神经网络，比如在自然语言处理领域的文本预测、机器翻译。循环神经网络可以被看作非常深的前馈网络，其中所有层共享相同的权重，虽然这样设计的目的在于学习长期的依赖关系，但实际上并没有。解决这一问题的是LSTM（long short-term memory），其中加了一个连接到自己的“记忆细胞”，由其他单元学习到的乘法门控决定是否要将该值传入下一层。</p><p>未来的深度学习，非监督学习可能会更重要，计算机视觉系统会将卷积神经网络和带有强化学习的循环神经网络相结合，循环神经网络会广泛应用于自然语言处理。最终，人工智能的重大进步将来自表示学习和复杂推理的结合。</p><h1 id="K-Means"><a href="#K-Means" class="headerlink" title="K-Means"></a>K-Means</h1><h2 id="题目-3"><a href="#题目-3" class="headerlink" title="题目"></a>题目</h2><p><img src="/2018/10/20/《人工智能》作业集/4-1.jpg" alt=""></p><h2 id="源代码-1"><a href="#源代码-1" class="headerlink" title="源代码"></a>源代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distance</span><span class="params">(x1, x2, y1, y2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> ((x1 - x2) ** <span class="number">2</span> + (y1 - y2) ** <span class="number">2</span>) ** <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">points_x = [<span class="number">40</span>, <span class="number">24</span>, <span class="number">17</span>, <span class="number">22</span>, <span class="number">51</span>, <span class="number">87</span>, <span class="number">68</span>, <span class="number">84</span>, <span class="number">66</span>, <span class="number">61</span>, ]</span><br><span class="line">points_y = [<span class="number">44</span>, <span class="number">14</span>, <span class="number">22</span>, <span class="number">76</span>, <span class="number">94</span>, <span class="number">65</span>, <span class="number">52</span>, <span class="number">36</span>, <span class="number">25</span>, <span class="number">26</span>, ]</span><br><span class="line">points = list(zip(points_x, points_y))</span><br><span class="line">k = <span class="number">3</span></span><br><span class="line">colors = [<span class="string">'r'</span>, <span class="string">'g'</span>, <span class="string">'b'</span>]</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">m, n = <span class="number">3</span>, <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        results = &#123;point: <span class="number">-1</span> <span class="keyword">for</span> point <span class="keyword">in</span> points&#125;</span><br><span class="line">        centers = np.array([list(points[random.randint(<span class="number">0</span>, len(points_x) - <span class="number">1</span>)]) <span class="keyword">for</span> _ <span class="keyword">in</span> range(k)])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">            temp_x, temp_y = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(k)], [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(k)]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> point <span class="keyword">in</span> points:</span><br><span class="line">                distances_to_center = [distance(point[<span class="number">0</span>], center[<span class="number">0</span>], point[<span class="number">1</span>], center[<span class="number">1</span>]) <span class="keyword">for</span> center <span class="keyword">in</span> centers]</span><br><span class="line">                c = np.argmin(distances_to_center)</span><br><span class="line">                results[point] = c</span><br><span class="line">                temp_x[c].append(point[<span class="number">0</span>])</span><br><span class="line">                temp_y[c].append(point[<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> range(k):</span><br><span class="line">                centers[c][<span class="number">0</span>] = np.mean(temp_x[c])</span><br><span class="line">                centers[c][<span class="number">1</span>] = np.mean(temp_y[c])</span><br><span class="line"></span><br><span class="line">        plt.subplot(m, n, i * m + j + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> c, center <span class="keyword">in</span> enumerate(centers):</span><br><span class="line">            plt.scatter(center[<span class="number">0</span>], center[<span class="number">1</span>], c=colors[c], marker=<span class="string">'^'</span>, s=<span class="number">100</span>)</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(points_x, points_y):</span><br><span class="line">            plt.scatter(x, y, c=colors[results[x, y]])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="运行结果-2"><a href="#运行结果-2" class="headerlink" title="运行结果"></a>运行结果</h2><p>因为是随机选取初始中心点，所以每次分类结果并不相同。所以分别在k=2和k=3的前提下进行9次聚类，结果如下图所示，其中三角形表示聚类中心。</p><h3 id="K-2"><a href="#K-2" class="headerlink" title="K=2"></a>K=2</h3><p><img src="/2018/10/20/《人工智能》作业集/4-2.png" alt=""></p><h3 id="K-3"><a href="#K-3" class="headerlink" title="K=3"></a>K=3</h3><p><img src="/2018/10/20/《人工智能》作业集/4-3.png" alt=""></p><h1 id="二叉树的先序遍历与BFS"><a href="#二叉树的先序遍历与BFS" class="headerlink" title="二叉树的先序遍历与BFS"></a>二叉树的先序遍历与BFS</h1><h2 id="题目-4"><a href="#题目-4" class="headerlink" title="题目"></a>题目</h2><p><img src="/2018/10/20/《人工智能》作业集/5-1.jpg" alt=""></p><h2 id="源代码-2"><a href="#源代码-2" class="headerlink" title="源代码"></a>源代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, value)</span>:</span></span><br><span class="line">        self.value = value</span><br><span class="line">        self.left = <span class="keyword">None</span></span><br><span class="line">        self.right = <span class="keyword">None</span></span><br><span class="line">        self.visited = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BinaryTree</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, root_value)</span>:</span></span><br><span class="line">        self.root = Node(root_value)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pre_order_traverse_recursion</span><span class="params">(self, node)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> node:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        print(node.value, end=<span class="string">' '</span>)</span><br><span class="line">        self.pre_order_traverse_recursion(node.left)</span><br><span class="line">        self.pre_order_traverse_recursion(node.right)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pre_order_traverse</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'pre_order_traverse:'</span>)</span><br><span class="line">        self.pre_order_traverse_recursion(self.root)</span><br><span class="line">        print()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bfs</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'bfs:'</span>)</span><br><span class="line">        queue = [self.root, ]</span><br><span class="line">        <span class="keyword">while</span> len(queue) &gt; <span class="number">0</span>:</span><br><span class="line">            now = queue.pop(<span class="number">0</span>)</span><br><span class="line">            print(now.value, end=<span class="string">' '</span>)</span><br><span class="line">            <span class="keyword">if</span> now.left:</span><br><span class="line">                queue.append(now.left)</span><br><span class="line">            <span class="keyword">if</span> now.right:</span><br><span class="line">                queue.append(now.right)</span><br><span class="line">        print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tree = BinaryTree(<span class="string">'A'</span>)</span><br><span class="line">tree.root.left = Node(<span class="string">'B'</span>)</span><br><span class="line">tree.root.right = Node(<span class="string">'C'</span>)</span><br><span class="line">tree.root.left.left = Node(<span class="string">'D'</span>)</span><br><span class="line">tree.root.left.right = Node(<span class="string">'E'</span>)</span><br><span class="line">tree.root.right.left = Node(<span class="string">'F'</span>)</span><br><span class="line">tree.root.right.right = Node(<span class="string">'G'</span>)</span><br><span class="line">tree.root.left.left.left = Node(<span class="string">'H'</span>)</span><br><span class="line">tree.root.right.left.left = Node(<span class="string">'I'</span>)</span><br><span class="line">tree.root.right.right.right = Node(<span class="string">'J'</span>)</span><br><span class="line">tree.root.left.left.left.right = Node(<span class="string">'K'</span>)</span><br><span class="line"></span><br><span class="line">tree.pre_order_traverse()</span><br><span class="line">tree.bfs()</span><br></pre></td></tr></table></figure><h1 id="最大矩阵和"><a href="#最大矩阵和" class="headerlink" title="最大矩阵和"></a>最大矩阵和</h1><h2 id="题目-5"><a href="#题目-5" class="headerlink" title="题目"></a>题目</h2><p><img src="/2018/10/20/《人工智能》作业集/6-1.jpg" alt=""></p><h2 id="源程序"><a href="#源程序" class="headerlink" title="源程序"></a>源程序</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_in_row</span><span class="params">(row)</span>:</span></span><br><span class="line">    max_num = <span class="number">0</span></span><br><span class="line">    temp = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> row:</span><br><span class="line">        <span class="keyword">if</span> num &gt;= <span class="number">0</span>:</span><br><span class="line">            temp += num</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> temp &gt; max_num:</span><br><span class="line">                max_num = temp</span><br><span class="line">            temp = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> max_num</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_in_matrix</span><span class="params">(matrix)</span>:</span></span><br><span class="line">    num_row = matrix.shape[<span class="number">0</span>]</span><br><span class="line">    max_num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_row):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i + <span class="number">1</span>, num_row+<span class="number">1</span>):</span><br><span class="line">            max_num = max(max_num, max_in_row(np.sum(matrix[i:j, :], axis=<span class="number">0</span>)))</span><br><span class="line">    <span class="keyword">return</span> max_num</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">N = int(input())</span><br><span class="line">matrix = np.array([[int(num) <span class="keyword">for</span> num <span class="keyword">in</span> input().split(<span class="string">' '</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)])</span><br><span class="line">print(matrix)</span><br><span class="line">print(max_in_matrix(matrix))</span><br></pre></td></tr></table></figure><h1 id="中心极限定理"><a href="#中心极限定理" class="headerlink" title="中心极限定理"></a>中心极限定理</h1><h2 id="题目-6"><a href="#题目-6" class="headerlink" title="题目"></a>题目</h2><p><img src="/2018/10/20/《人工智能》作业集/7-1.png" alt=""></p><p><img src="/2018/10/20/《人工智能》作业集/7-2.png" alt=""></p><h2 id="源代码-3"><a href="#源代码-3" class="headerlink" title="源代码"></a>源代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line">x1 = np.random.uniform(<span class="number">-2</span>, <span class="number">2</span>, <span class="number">10000</span>)</span><br><span class="line">x2 = np.random.uniform(<span class="number">-2</span>, <span class="number">2</span>, <span class="number">10000</span>)</span><br><span class="line">z = x1 + x2</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">sns.kdeplot(z, shade=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">z = sum([np.random.uniform(<span class="number">-2</span>, <span class="number">2</span>, <span class="number">10000</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">100</span>)])</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">sns.kdeplot(z, shade=<span class="keyword">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="运行结果-3"><a href="#运行结果-3" class="headerlink" title="运行结果"></a>运行结果</h2><p><img src="/2018/10/20/《人工智能》作业集/7-3.png" alt=""></p><h1 id="推导二元正态后验概率"><a href="#推导二元正态后验概率" class="headerlink" title="推导二元正态后验概率"></a>推导二元正态后验概率</h1><h2 id="题目-7"><a href="#题目-7" class="headerlink" title="题目"></a>题目</h2><p><img src="/2018/10/20/《人工智能》作业集/8-1.png" alt=""></p><h2 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h2><p>  <img src="/2018/10/20/《人工智能》作业集/8-2.png" alt=""></p><h1 id="寻找最优解"><a href="#寻找最优解" class="headerlink" title="寻找最优解"></a>寻找最优解</h1><h2 id="题目-8"><a href="#题目-8" class="headerlink" title="题目"></a>题目</h2><p><img src="/2018/10/20/《人工智能》作业集/9-1.png" alt=""></p><p><img src="/2018/10/20/《人工智能》作业集/9-2.png" alt=""></p><h2 id="源代码-4"><a href="#源代码-4" class="headerlink" title="源代码"></a>源代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">10</span> <span class="keyword">if</span> abs(num - <span class="number">10</span>) &lt; abs(num + <span class="number">10</span>) <span class="keyword">else</span> <span class="number">-10</span></span><br><span class="line"></span><br><span class="line">h0, h1 = <span class="number">0.2</span>, <span class="number">0.1</span></span><br><span class="line">H = np.array([</span><br><span class="line">    [h0, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [h1, h0, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, h1, h0, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, h1, h0, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, h1, h0, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, h1, h0],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, h1],</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 穷举法</span></span><br><span class="line">min_error = []</span><br><span class="line"><span class="comment"># one by one detection</span></span><br><span class="line">error_list = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># 生成数据</span></span><br><span class="line">    s = np.array([random.choice([<span class="number">-10</span>, <span class="number">10</span>]) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">6</span>)]).T</span><br><span class="line">    w = np.random.normal(size=<span class="number">7</span>).T</span><br><span class="line">    y = np.dot(H, s) + w</span><br><span class="line">    <span class="comment"># 穷举法</span></span><br><span class="line">    error = float(<span class="string">'Inf'</span>)</span><br><span class="line">    <span class="keyword">for</span> s0 <span class="keyword">in</span> [<span class="number">-10</span>, <span class="number">10</span>]:</span><br><span class="line">        <span class="keyword">for</span> s1 <span class="keyword">in</span> [<span class="number">-10</span>, <span class="number">10</span>]:</span><br><span class="line">            <span class="keyword">for</span> s2 <span class="keyword">in</span> [<span class="number">-10</span>, <span class="number">10</span>]:</span><br><span class="line">                <span class="keyword">for</span> s3 <span class="keyword">in</span> [<span class="number">-10</span>, <span class="number">10</span>]:</span><br><span class="line">                    <span class="keyword">for</span> s4 <span class="keyword">in</span> [<span class="number">-10</span>, <span class="number">10</span>]:</span><br><span class="line">                        <span class="keyword">for</span> s5 <span class="keyword">in</span> [<span class="number">-10</span>, <span class="number">10</span>]:</span><br><span class="line">                            s_hat = np.array([s0, s1, s2, s3, s4, s5])</span><br><span class="line">                            e = y - np.dot(H, s_hat)</span><br><span class="line">                            new_error = np.dot(e, e.T)</span><br><span class="line">                            error = min(new_error, error)</span><br><span class="line">    min_error.append(error)</span><br><span class="line">    <span class="comment"># one by one detection</span></span><br><span class="line">    s_hat = [f((y[<span class="number">0</span>] - w[<span class="number">0</span>]) / h0), ]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>):</span><br><span class="line">        new_s_hat = f((y[i] - h1 * s_hat[<span class="number">-1</span>] - w[i]) / h0)</span><br><span class="line">        s_hat.append(new_s_hat)</span><br><span class="line">    e = y - np.dot(H, s_hat)</span><br><span class="line">    error_list.append(np.dot(e, e.T))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">e1 = sum(min_error) / len(min_error)</span><br><span class="line">e2 = sum(error_list)/len(error_list)</span><br><span class="line">print(e1, e2)</span><br></pre></td></tr></table></figure><h2 id="运行结果-4"><a href="#运行结果-4" class="headerlink" title="运行结果"></a>运行结果</h2><p>显然E[e1]是大于E[e2]，显然穷举法得到的对于s的估计是最优的，one by one的方法，虽然有较高的error，但是显然速度较快。</p><h1 id="alpha-beta-剪枝法"><a href="#alpha-beta-剪枝法" class="headerlink" title="$\alpha-\beta$剪枝法"></a>$\alpha-\beta$剪枝法</h1><h2 id="题目与求解"><a href="#题目与求解" class="headerlink" title="题目与求解"></a>题目与求解</h2><p><img src="/2018/10/20/《人工智能》作业集/10-1.png" alt=""></p><h1 id="爬山算法与模拟剪枝算法"><a href="#爬山算法与模拟剪枝算法" class="headerlink" title="爬山算法与模拟剪枝算法"></a>爬山算法与模拟剪枝算法</h1><h2 id="题目-9"><a href="#题目-9" class="headerlink" title="题目"></a>题目</h2><p><img src="/2018/10/20/《人工智能》作业集/11-2.png" alt=""></p><h2 id="源代码-5"><a href="#源代码-5" class="headerlink" title="源代码"></a>源代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * np.e ** ((-x ** <span class="number">2</span> - y ** <span class="number">2</span>) / <span class="number">8</span>) + np.e ** (-(x - <span class="number">4</span>) ** <span class="number">2</span>) + np.e ** (-((x + <span class="number">4</span>) ** <span class="number">2</span> + (y - <span class="number">4</span>) ** <span class="number">2</span>) / <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">min_n, max_n = <span class="number">-10</span>, <span class="number">10</span></span><br><span class="line">_inf = <span class="number">0</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = Axes3D(fig)</span><br><span class="line">X = np.linspace(min_n, max_n, <span class="number">100</span>)</span><br><span class="line">Y = np.linspace(min_n, max_n, <span class="number">100</span>)</span><br><span class="line">X, Y = np.meshgrid(X, Y)</span><br><span class="line">Z = f(X, Y)</span><br><span class="line">ax.plot_wireframe(X, Y, Z)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 爬山算法</span></span><br><span class="line">x, y = np.random.uniform(min_n, max_n, <span class="number">2</span>)</span><br><span class="line">delta = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">    ax.scatter(x, y, f(x, y), c=<span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">    values = list()  <span class="comment"># 中，上，下，左，右</span></span><br><span class="line">    values.append(f(x, y))</span><br><span class="line">    values.append(f(x, y + delta) <span class="keyword">if</span> y + delta &lt; max_n <span class="keyword">else</span> _inf)</span><br><span class="line">    values.append(f(x, y - delta) <span class="keyword">if</span> y - delta &gt; min_n <span class="keyword">else</span> _inf)</span><br><span class="line">    values.append(f(x - delta, y) <span class="keyword">if</span> x - delta &gt; min_n <span class="keyword">else</span> _inf)</span><br><span class="line">    values.append(f(x + delta, y) <span class="keyword">if</span> x + delta &lt; max_n <span class="keyword">else</span> _inf)</span><br><span class="line">    max_index = np.argmax(values)</span><br><span class="line">    <span class="keyword">if</span> max_index == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">elif</span> max_index == <span class="number">1</span>:</span><br><span class="line">        y += delta</span><br><span class="line">    <span class="keyword">elif</span> max_index == <span class="number">2</span>:</span><br><span class="line">        y -= delta</span><br><span class="line">    <span class="keyword">elif</span> max_index == <span class="number">3</span>:</span><br><span class="line">        x -= delta</span><br><span class="line">    <span class="keyword">elif</span> max_index == <span class="number">4</span>:</span><br><span class="line">        x += delta</span><br><span class="line">ax.text(x, y, f(x, y), str((x, y, f(x, y))))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟退火</span></span><br><span class="line">x, y = np.random.uniform(min_n, max_n, <span class="number">2</span>)</span><br><span class="line">E_now = f(x, y)</span><br><span class="line">Tk, T0, Tf, r = <span class="number">10</span>, <span class="number">10</span>, <span class="number">0.001</span>, <span class="number">0.95</span></span><br><span class="line">n_Tk = <span class="number">3</span></span><br><span class="line"><span class="keyword">while</span> Tk &gt; Tf:</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_Tk):</span><br><span class="line">        x_delta = np.random.uniform(min_n - x, max_n - x)</span><br><span class="line">        y_delta = np.random.uniform(min_n - y, max_n - y)</span><br><span class="line">        E_new = f(x + x_delta, y + y_delta)</span><br><span class="line">        <span class="comment"># 值变大,一定接受</span></span><br><span class="line">        <span class="keyword">if</span> E_new &gt; E_now:</span><br><span class="line">            ax.plot((x, x + x_delta), (y, y + y_delta), (E_now, E_new), c=<span class="string">'g'</span>)</span><br><span class="line">            E_now = E_new</span><br><span class="line">            x += x_delta</span><br><span class="line">            y += y_delta</span><br><span class="line">        <span class="comment"># 值变小,有一定概率接受</span></span><br><span class="line">        <span class="keyword">elif</span> np.e ** ((E_new - E_now) / Tk) &gt; np.random.uniform(<span class="number">0</span>, <span class="number">1</span>):</span><br><span class="line">                ax.plot((x, x + x_delta), (y, y + y_delta), (E_now, E_new), c=<span class="string">'g'</span>)</span><br><span class="line">                E_now = E_new</span><br><span class="line">                x += x_delta</span><br><span class="line">                y += y_delta</span><br><span class="line">    <span class="comment"># 更新温度</span></span><br><span class="line">    Tk *= r</span><br><span class="line">ax.text(x, y, E_now, str((x, y, E_now)))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="运行结果-5"><a href="#运行结果-5" class="headerlink" title="运行结果"></a>运行结果</h2><p>爬山算法总是能更快地落入极大点，但很容易是局部最大点。模拟退火虽然有时候会走弯路，但只要参数适当，总能接近全局最大点。如下图所示，红色为爬山路径，绿色为模拟退火路径。</p><p><img src="/2018/10/20/《人工智能》作业集/11-1.jpg" alt=""></p><p><img src="/2018/10/20/《人工智能》作业集/11-3.jpg" alt=""></p><h1 id="模拟退火算法求解旅行商问题"><a href="#模拟退火算法求解旅行商问题" class="headerlink" title="模拟退火算法求解旅行商问题"></a>模拟退火算法求解旅行商问题</h1><h2 id="题目-10"><a href="#题目-10" class="headerlink" title="题目"></a>题目</h2><p><img src="/2018/10/20/《人工智能》作业集/12-1.png" alt=""></p><h2 id="源代码-6"><a href="#源代码-6" class="headerlink" title="源代码"></a>源代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distance</span><span class="params">(x1, x2, y1, y2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> ((x1 - x2) ** <span class="number">2</span> + (y1 - y2) ** <span class="number">2</span>) ** <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认从index为0的点出发</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sample</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate_cost</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.cost = self.distances[<span class="number">0</span>][self.sample[<span class="number">0</span>]]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.sample) - <span class="number">1</span>):</span><br><span class="line">            a, b = self.sample[i], self.sample[i + <span class="number">1</span>]</span><br><span class="line">            self.cost += self.distances[a][b]</span><br><span class="line">        self.cost += self.distances[self.sample[<span class="number">-1</span>]][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_points, distances)</span>:</span></span><br><span class="line">        self.num_points = num_points</span><br><span class="line">        self.distances = distances</span><br><span class="line">        self.sample = list(range(<span class="number">1</span>, self.num_points))</span><br><span class="line">        random.shuffle(self.sample)</span><br><span class="line">        self.calculate_cost()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">change_sample</span><span class="params">(self)</span>:</span></span><br><span class="line">        i, j = [random.randint(<span class="number">0</span>, len(self.sample) - <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">2</span>)]</span><br><span class="line">        self.sample[i], self.sample[j] = self.sample[j], self.sample[i]</span><br><span class="line">        self.calculate_cost()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Samples</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, x, y, num_samples=<span class="number">100</span>)</span>:</span></span><br><span class="line">        self.inf = <span class="number">1000</span></span><br><span class="line">        self.x, self.y, self.num_samples = x, y, num_samples</span><br><span class="line">        self.num_points = len(self.x)</span><br><span class="line">        self.points = list(zip(self.x, self.y))</span><br><span class="line">        self.distances = np.array([</span><br><span class="line">            [distance(point1[<span class="number">0</span>], point2[<span class="number">0</span>], point1[<span class="number">1</span>], point2[<span class="number">1</span>]) <span class="keyword">if</span> point1 != point2 <span class="keyword">else</span> self.inf <span class="keyword">for</span> point2 <span class="keyword">in</span></span><br><span class="line">             self.points]</span><br><span class="line">            <span class="keyword">for</span> point1 <span class="keyword">in</span> self.points</span><br><span class="line">        ])</span><br><span class="line">        self.sample = Sample(self.num_points, self.distances)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, Tk=<span class="number">100</span>, Tf=<span class="number">0.001</span>, r=<span class="number">0.99</span>, n_Tk=<span class="number">5</span>)</span>:</span></span><br><span class="line">        <span class="keyword">while</span> Tk &gt; Tf:</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_Tk):</span><br><span class="line">                new_sample = deepcopy(self.sample)</span><br><span class="line">                new_sample.change_sample()</span><br><span class="line">                <span class="keyword">if</span> new_sample.cost &lt; self.sample.cost:</span><br><span class="line">                    self.sample = new_sample</span><br><span class="line">                <span class="keyword">elif</span> np.e ** ((self.sample.cost - new_sample.cost) / Tk) &gt; np.random.uniform(<span class="number">0</span>, <span class="number">1</span>):</span><br><span class="line">                    self.sample = new_sample</span><br><span class="line">            <span class="comment"># 更新温度</span></span><br><span class="line">            Tk *= r</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">show_result</span><span class="params">(self)</span>:</span></span><br><span class="line">        plt.figure()</span><br><span class="line">        plt.scatter(self.x, self.y)</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(self.x, self.y):</span><br><span class="line">            plt.text(x, y, (x, y), fontdict=&#123;<span class="string">'size'</span>: <span class="number">16</span>, <span class="string">'color'</span>: <span class="string">'r'</span>&#125;)</span><br><span class="line">        print(<span class="string">'least cost: '</span>, self.sample.cost)</span><br><span class="line">        plt.plot([self.x[<span class="number">0</span>], self.x[self.sample.sample[<span class="number">0</span>]]], [self.y[<span class="number">0</span>], self.y[self.sample.sample[<span class="number">0</span>]]])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.sample.sample)<span class="number">-1</span>):</span><br><span class="line">            a, b = self.sample.sample[i], self.sample.sample[i+<span class="number">1</span>]</span><br><span class="line">            plt.plot([self.x[a], self.x[b]], [self.y[a], self.y[b]])</span><br><span class="line">        plt.plot([self.x[<span class="number">0</span>], self.x[self.sample.sample[<span class="number">-1</span>]]], [self.y[<span class="number">0</span>], self.y[self.sample.sample[<span class="number">-1</span>]]])</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">points_x = [<span class="number">40</span>, <span class="number">24</span>, <span class="number">17</span>, <span class="number">22</span>, <span class="number">51</span>, <span class="number">87</span>, <span class="number">68</span>, <span class="number">84</span>, <span class="number">66</span>, <span class="number">61</span>, ]</span><br><span class="line">points_y = [<span class="number">44</span>, <span class="number">14</span>, <span class="number">22</span>, <span class="number">76</span>, <span class="number">94</span>, <span class="number">65</span>, <span class="number">52</span>, <span class="number">36</span>, <span class="number">25</span>, <span class="number">26</span>, ]</span><br><span class="line">s = Samples(points_x, points_y)</span><br><span class="line">s.train()</span><br><span class="line">s.show_result()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;虽然王公仆老师布置的作业和考试并没有直接关系，但是实事求是的说，王老师《人工智能》课程的内容还是比较科学合理，能够勾起大家学习兴趣的。以下便是历次作业题及其解答。&lt;/p&gt;
&lt;h1 id=&quot;用遗传算法求解旅行商问题&quot;&gt;&lt;a href=&quot;#用遗传算法求解旅行商问题&quot; class
      
    
    </summary>
    
      <category term="人工智能" scheme="http://wang22ti.com/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
  </entry>
  
  <entry>
    <title>笔记-latex-1-组织文本</title>
    <link href="http://wang22ti.com/2018/10/20/%E7%AC%94%E8%AE%B0-latex-1-%E7%BB%84%E7%BB%87%E6%96%87%E6%9C%AC/"/>
    <id>http://wang22ti.com/2018/10/20/笔记-latex-1-组织文本/</id>
    <published>2018-10-20T02:53:19.000Z</published>
    <updated>2018-11-18T07:37:32.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="latex" scheme="http://wang22ti.com/categories/latex/"/>
    
    
  </entry>
  
  <entry>
    <title>《软件综合实践》课程设计-基于龙芯处理器的C语言编译系统的设计与实现</title>
    <link href="http://wang22ti.com/2018/10/08/%E3%80%8A%E8%BD%AF%E4%BB%B6%E7%BB%BC%E5%90%88%E5%AE%9E%E8%B7%B5%E3%80%8B%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1-%E5%9F%BA%E4%BA%8E%E9%BE%99%E8%8A%AF%E5%A4%84%E7%90%86%E5%99%A8%E7%9A%84C%E8%AF%AD%E8%A8%80%E7%BC%96%E8%AF%91%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/"/>
    <id>http://wang22ti.com/2018/10/08/《软件综合实践》课程设计-基于龙芯处理器的C语言编译系统的设计与实现/</id>
    <published>2018-10-08T03:32:54.000Z</published>
    <updated>2018-11-21T15:33:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>嗯……本来是想写一个超详细的说明的，但是发现细节实在是太多了，所以选一些重要的说一下吧。</p><h1 id="干了啥？"><a href="#干了啥？" class="headerlink" title="干了啥？"></a>干了啥？</h1><p>老师给了我们一款板子，上面有龙芯处理器。我们要做的是，在箱子外面写一个编译器，把C语言代码转化为可执行文件，然后传输到板子上，在板子上执行那个可执行文件。</p><p>整个工作分为PC和板子的通信、编译器、汇编器、链接器，我和<a href="https://www.wchhlbt.cn/" target="_blank" rel="noopener">楚涵</a>一起完成了汇编器的部分，就是将MIPS汇编代码翻译为Linux下可执行的ELF文件。</p><h1 id="关键问题"><a href="#关键问题" class="headerlink" title="关键问题"></a>关键问题</h1><ol><li><p>ELF文件长啥样？</p><p>在干这个项目之前，我以为只要参照MIPS指令的手册，粗暴地把指令翻译为二进制的格式并写到文件里面就行。但实际上，二进制可执行文件里面除了指令以外，还有很多的信息，比如文件一开始有一堆关于文件信息的描述，后面是表头，再后面才是各个具体的表。详细在此不赘述，可以参见参考文献。</p><p>由于不同部分之间存在很多交叉引用，尤其是地址的引用，这给我们带来了很大的困难。</p></li><li><p>MIPS汇编文件的宏指令？</p><p>在看了由标准GCC生成的汇编文件后，我们发现除了标准的MIPS指令，还存在大量其他内容。一个是宏指令，比如<code>li</code>和<code>la</code>，即将多条标准指令封装为一条宏指令，根据参数的不同由汇编器选择具体的指令；另外一种是由标准指令扩展的失灵，比如<code>mov</code>，既可以用<code>or</code>扩展，也可以用<code>addiu</code>来扩展；还有一种是以点开头的指令，比如<code>.set noreorder</code>，是对于汇编器的设置。</p><p>要搞清这三类指令的具体含义并转化为代码，并不是一个小工作。</p></li><li><p>怎么写二进制文件？</p><p>由于x86和龙芯MIPS都是小端机，所以直接将要写的数据结构传入函数<code>fwrite</code>即可，C语言会把其中的数据项以二进制的方式顺次写到文件中。需要注意的是，文件打开的模式一定要使用<code>wb</code>；因为在windows下如果模式是<code>w</code>，会在写<code>0x0A</code>的时候自动添加一个<code>0x0D</code>。一开始没注意这个问题，被耽误了一些时间。</p></li><li><p>怎么实现输入输出？</p><p>本来以为需要从头写，用一些系统调用，但实际上Linux系统存在很多动态连接库，他们具有<code>地址无关性</code>，只要在代码中传入恰当的参数并调用即可。当然，生成调用的汇编代码并不是我们的工作，我们的工作是把这些信息写到ELF文件中，这涉及到多个段表的填写，所以还是有点复杂的。</p></li><li><p>开发的环境？</p><p>由于最后的软件是在Linux上跑，汇编器又是涉及操作系统的部分，很多数据结构已经在Linux库函数里面有了定义，比如<code>elf.h</code>中就对于elf文件中各个段的数据结构有了定义，所以至少汇编器和链接器的开发要在Linux上进行。</p></li></ol><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>相比我们有些拙劣的代码，这些参考文献还是更有价值的：</p><ol><li>《自己动手构造编译系统++编译、汇编与链接》。关于x86和Linux环境下的MIPS编译器，中文且非常详细，对我们的工作有启发价值。此外，该书的配套代码已经在<a href="https://github.com/fanzhidongyzby/cit/tree/x86" target="_blank" rel="noopener">GitHub</a>上开源了，十分良心了。</li><li>·《See MIPS Run》。中英文都有，对于MIPS指令及其编码、函数调用等方面讲解非常详细，和我们工作最相关的是第2、8、9、11、16章。</li><li>《ELF_Format》。中英文都有，对于ELF文件各个段的讲解十分详细，对于文献1是一种有益的补充。</li></ol><h1 id="配套软件"><a href="#配套软件" class="headerlink" title="配套软件"></a>配套软件</h1><p>要理解汇编到ELF的翻译工作，离不开对于ELF文件的直接阅读，自然不能少了很多工具：</p><ol><li>readelf，Linux自带的读取ELF头信息的命令</li><li>BinaryViewer，可以轻松以各种方式查看二进制文件</li><li>IDA_PRO，以更高级的方式阅读ELF文件，比如指出某段数据是哪一个段的，不过阅读方式不如BinaryViewer灵活多样。</li><li>Snipaste，一个优秀的截图贴图软件，可以方便的依靠贴图在各个文件之间比对。</li><li>pyelftools，python阅读elf文件的包，通过以下代码可以输出每个段的具体内容：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> elftools.elf.elffile</span><br><span class="line"></span><br><span class="line">file_name = <span class="string">'**********'</span></span><br><span class="line">stream = open(file_name+<span class="string">'.o'</span>, <span class="string">'rb'</span>)</span><br><span class="line"></span><br><span class="line">elf_file = elftools.elf.elffile.ELFFile(stream)</span><br><span class="line">num_of_section = elf_file.num_sections()</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">1</span>, num_of_section):</span><br><span class="line">    section = elf_file.get_section(n)</span><br><span class="line">    print(n, section.name, section.header.sh_size)</span><br><span class="line">    data = section.data()</span><br><span class="line">    print(data)</span><br></pre></td></tr></table></figure><h1 id="源码链接"><a href="#源码链接" class="headerlink" title="源码链接"></a>源码链接</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;嗯……本来是想写一个超详细的说明的，但是发现细节实在是太多了，所以选一些重要的说一下吧。&lt;/p&gt;
&lt;h1 id=&quot;干了啥？&quot;&gt;&lt;a href=&quot;#干了啥？&quot; class=&quot;headerlink&quot; title=&quot;干了啥？&quot;&gt;&lt;/a&gt;干了啥？&lt;/h1&gt;&lt;p&gt;老师给了我们一款板子
      
    
    </summary>
    
      <category term="软件综合实践" scheme="http://wang22ti.com/categories/%E8%BD%AF%E4%BB%B6%E7%BB%BC%E5%90%88%E5%AE%9E%E8%B7%B5/"/>
    
    
  </entry>
  
  <entry>
    <title>《模式识别》实验2-贝叶斯分类器设计</title>
    <link href="http://wang22ti.com/2018/10/08/%E3%80%8A%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E3%80%8B%E5%AE%9E%E9%AA%8C2-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8%E8%AE%BE%E8%AE%A1/"/>
    <id>http://wang22ti.com/2018/10/08/《模式识别》实验2-贝叶斯分类器设计/</id>
    <published>2018-10-08T03:30:57.000Z</published>
    <updated>2018-10-13T02:02:57.038Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实验目的"><a href="#实验目的" class="headerlink" title="实验目的"></a>实验目的</h1><ol><li>对模式识别有一个初步的理解</li><li>能够根据自己的设计对贝叶斯决策理论算法有一个深刻地认识</li><li>理解两类分类器的设计原理 </li></ol><h1 id="实验原理"><a href="#实验原理" class="headerlink" title="实验原理"></a>实验原理</h1><ol><li><p>已知类别概率和先验概率，根据贝叶斯公式，有后验概率公式</p><p><img src="/2018/10/08/《模式识别》实验2-贝叶斯分类器设计/1.png" alt=""></p></li><li><p>已知后验概率，基于最小错误率准则和最小风险准则设计分类器</p></li></ol><h1 id="实验内容与步骤"><a href="#实验内容与步骤" class="headerlink" title="实验内容与步骤"></a>实验内容与步骤</h1><p>假定某个局部区域细胞识别中正常（w1） 和非正常（w2）两类先验概率分别为：正常状态P（w1）=0.9；异常状态P（w2）=0.1。现有一系列待观察的细胞，其观察值为x：</p><p><img src="/2018/10/08/《模式识别》实验2-贝叶斯分类器设计/2.png" alt=""></p><p>类条件概率分布正态分布分别为N(-2, 0.5)和N(2,2)，试对观察的结果进行分类。<br>首先，根据示例代码和数据完成最小错误率贝叶斯分类器的设计，代码如下。为了加快计算速度、提高代码的简洁性，将原来的循环计算改为矩阵计算。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">clear all;</span><br><span class="line">clc;</span><br><span class="line">x=[ <span class="number">-3.9847</span>  <span class="number">-3.5549</span>  <span class="number">-1.2401</span>  <span class="number">-0.9780</span>  <span class="number">-0.7932</span>  <span class="number">-2.8531</span>...</span><br><span class="line">    <span class="number">-2.7605</span>  <span class="number">-3.7287</span>  <span class="number">-3.5414</span>  <span class="number">-2.2692</span>  <span class="number">-3.4549</span>  <span class="number">-3.0752</span>...</span><br><span class="line">    <span class="number">-3.9934</span>  <span class="number">2.8792</span>  <span class="number">-0.9780</span>  <span class="number">0.7932</span>  <span class="number">1.1882</span>  <span class="number">3.0682</span>...</span><br><span class="line">    <span class="number">-1.5799</span>  <span class="number">-1.4885</span>  <span class="number">0.7431</span>  <span class="number">-0.4221</span>  <span class="number">-1.1186</span>   <span class="number">4.2532</span> ];</span><br><span class="line">pw1=<span class="number">0.9</span>; pw2=<span class="number">0.1</span>;</span><br><span class="line">e1=<span class="number">-2</span>; a1=<span class="number">0.5</span>;</span><br><span class="line">e2=<span class="number">2</span>;a2=<span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">p_x_w1 = normpdf(x, e1, a1) * pw1;</span><br><span class="line">p_x_w2 = normpdf(x, e2, a2) * pw2;</span><br><span class="line">pw1_x = p_x_w1 ./ (p_x_w1 + p_x_w2);        <span class="comment">%计算在w1下的后验概率 </span></span><br><span class="line">pw2_x = p_x_w2 ./ (p_x_w1 + p_x_w2);        <span class="comment">%计算在w2下的后验概率 </span></span><br><span class="line">result = pw1_x &lt;= pw2_x;</span><br><span class="line"></span><br><span class="line">a = [<span class="number">-5</span>:<span class="number">0.05</span>:<span class="number">5</span>];                  <span class="comment">%取样本点以画图</span></span><br><span class="line">p_x_w1 = normpdf(a, e1, a1) * pw1;</span><br><span class="line">p_x_w2 = normpdf(a, e2, a2) * pw2;</span><br><span class="line">pw1_plot = p_x_w1 ./ (p_x_w1 + p_x_w2);         <span class="comment">%计算每个样本点对w1的后验概率以画图</span></span><br><span class="line">pw2_plot = p_x_w2 ./ (p_x_w1 + p_x_w2);         <span class="comment">%计算每个样本点对w2的后验概率以画图</span></span><br><span class="line"></span><br><span class="line">figure(<span class="number">1</span>);</span><br><span class="line">hold on</span><br><span class="line">h1=plot(a,pw1_plot,<span class="string">'b-'</span>);</span><br><span class="line">h2=plot(a,pw2_plot,<span class="string">'r-.'</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k=<span class="number">1</span>:<span class="built_in">numel</span>(x)</span><br><span class="line">    <span class="keyword">if</span> result(k)==<span class="number">0</span> </span><br><span class="line">        h3=plot(x(k),<span class="number">-0.1</span>,<span class="string">'bp'</span>); <span class="comment">%正常细胞用五角星表示</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        h4=plot(x(k),<span class="number">-0.1</span>,<span class="string">'r*'</span>); <span class="comment">%异常细胞用*表示</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">legend([h1,h2,h3,h4],<span class="string">'正常细胞后验概率曲线'</span>,<span class="string">'异常细胞后验概率曲线'</span>,<span class="string">'正常细胞'</span>,<span class="string">'异常细胞'</span>);<span class="comment">%legend添加图例</span></span><br><span class="line">xlabel(<span class="string">'样本细胞的观察值'</span>);</span><br><span class="line">ylabel(<span class="string">'后验概率'</span>);</span><br><span class="line">title(<span class="string">'后验概率分布曲线'</span>);</span><br><span class="line">grid on</span><br><span class="line"></span><br><span class="line">figure(<span class="number">2</span>);</span><br><span class="line">hold on</span><br><span class="line">a1=<span class="number">-2</span>;sigma1=<span class="number">0.5</span>;</span><br><span class="line">x1=<span class="number">-10</span>:<span class="number">0.0001</span>:<span class="number">10</span>;</span><br><span class="line">y1=(<span class="number">1</span>/((<span class="built_in">sqrt</span>(<span class="number">2</span>*<span class="built_in">pi</span>))*sigma1))*<span class="built_in">exp</span>(-((x1-a1).^<span class="number">2</span>)/(<span class="number">2</span>*sigma1.^<span class="number">2</span>));</span><br><span class="line">plot(x1,y1,<span class="string">'r'</span>);</span><br><span class="line">a2=<span class="number">2</span>;sigma2=<span class="number">2</span>;</span><br><span class="line">x2=<span class="number">-10</span>:<span class="number">0.0001</span>:<span class="number">10</span>;</span><br><span class="line">y2=(<span class="number">1</span>/((<span class="built_in">sqrt</span>(<span class="number">2</span>*<span class="built_in">pi</span>))*sigma2))*<span class="built_in">exp</span>(-((x2-a2).^<span class="number">2</span>)/(<span class="number">2</span>*sigma2.^<span class="number">2</span>));</span><br><span class="line">plot(x2,y2,<span class="string">'b'</span>);</span><br><span class="line">legend(<span class="string">'正常细胞类条件概率分布曲线'</span>,<span class="string">'异常细胞类条件概率分布曲线'</span>);</span><br><span class="line">title(<span class="string">'条件概率分布曲线'</span>);</span><br><span class="line">grid on</span><br></pre></td></tr></table></figure><p>然后，根据风险决策表，完成最小风险贝叶斯分类器的设计，同样将循环计算改为矩阵计算，关键代码如下。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">clear all;</span><br><span class="line">clc;</span><br><span class="line">x=[ <span class="number">-3.9847</span>  <span class="number">-3.5549</span>  <span class="number">-1.2401</span>  <span class="number">-0.9780</span>  <span class="number">-0.7932</span>  <span class="number">-2.8531</span>...</span><br><span class="line">    <span class="number">-2.7605</span>  <span class="number">-3.7287</span>  <span class="number">-3.5414</span>  <span class="number">-2.2692</span>  <span class="number">-3.4549</span>  <span class="number">-3.0752</span>...</span><br><span class="line">    <span class="number">-3.9934</span>  <span class="number">2.8792</span>  <span class="number">-0.9780</span>  <span class="number">0.7932</span>  <span class="number">1.1882</span>  <span class="number">3.0682</span>...</span><br><span class="line">    <span class="number">-1.5799</span>  <span class="number">-1.4885</span>  <span class="number">0.7431</span>  <span class="number">-0.4221</span>  <span class="number">-1.1186</span>   <span class="number">4.2532</span> ];</span><br><span class="line">pw1=<span class="number">0.9</span>; pw2=<span class="number">0.1</span>;</span><br><span class="line">e1=<span class="number">-2</span>; a1=<span class="number">0.5</span>;</span><br><span class="line">e2=<span class="number">2</span>;a2=<span class="number">2</span>;</span><br><span class="line">r1 = [<span class="number">0</span> <span class="number">4</span>]; </span><br><span class="line">r2 = [<span class="number">2</span> <span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">p_x_w1 = normpdf(x, e1, a1) * pw1;</span><br><span class="line">p_x_w2 = normpdf(x, e2, a2) * pw2;</span><br><span class="line">pw1_x = p_x_w1 ./ (p_x_w1 + p_x_w2);        <span class="comment">%计算在w1下的后验概率 </span></span><br><span class="line">pw2_x = p_x_w2 ./ (p_x_w1 + p_x_w2);        <span class="comment">%计算在w2下的后验概率 </span></span><br><span class="line"><span class="comment">%风险决策表</span></span><br><span class="line">R1 = r1 * [pw1_x;pw2_x];</span><br><span class="line">R2 = r2 * [pw1_x;pw2_x];</span><br><span class="line">result = R1 &gt; R2;</span><br><span class="line"></span><br><span class="line">a = [<span class="number">-5</span>:<span class="number">0.05</span>:<span class="number">5</span>];                  <span class="comment">%取样本点以画图</span></span><br><span class="line">p_x_w1 = normpdf(a, e1, a1) * pw1;</span><br><span class="line">p_x_w2 = normpdf(a, e2, a2) * pw2;</span><br><span class="line">pw1_plot = p_x_w1 ./ (p_x_w1 + p_x_w2);         <span class="comment">%计算每个样本点对w1的后验概率以画图</span></span><br><span class="line">pw2_plot = p_x_w2 ./ (p_x_w1 + p_x_w2);         <span class="comment">%计算每个样本点对w2的后验概率以画图</span></span><br><span class="line">R1_plot = r1 * [pw1_plot;pw2_plot];</span><br><span class="line">R2_plot = r2 * [pw1_plot;pw2_plot];</span><br><span class="line"></span><br><span class="line">figure(<span class="number">1</span>);</span><br><span class="line">hold on</span><br><span class="line">h1=plot(a,R1_plot,<span class="string">'b-'</span>);</span><br><span class="line">h2=plot(a,R2_plot,<span class="string">'r-.'</span>);</span><br><span class="line"><span class="keyword">for</span> k=<span class="number">1</span>:<span class="built_in">numel</span>(x)</span><br><span class="line">    <span class="keyword">if</span> result(k)==<span class="number">0</span> </span><br><span class="line">        h3=plot(x(k),<span class="number">-0.1</span>,<span class="string">'bp'</span>);<span class="comment">%正常细胞用五角星表示</span></span><br><span class="line">    <span class="keyword">else</span>            </span><br><span class="line">        h4=plot(x(k),<span class="number">-0.1</span>,<span class="string">'r*'</span>);<span class="comment">%异常细胞用*表示</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">legend([h1,h2,h3,h4], <span class="string">'正常细胞条件风险曲线'</span>,<span class="string">'异常细胞条件风险曲线'</span>,<span class="string">'正常细胞'</span>,<span class="string">'异常细胞'</span>);</span><br><span class="line">xlabel(<span class="string">'细胞分类结果'</span>);</span><br><span class="line">ylabel(<span class="string">'条件风险'</span>);</span><br><span class="line">title(<span class="string">'风险判决曲线'</span>);</span><br><span class="line">grid on</span><br><span class="line"></span><br><span class="line">figure(<span class="number">2</span>);</span><br><span class="line">hold on</span><br><span class="line">a1=<span class="number">-2</span>;sigma1=<span class="number">0.5</span>;</span><br><span class="line">x1=<span class="number">-10</span>:<span class="number">0.0001</span>:<span class="number">10</span>;</span><br><span class="line">y1=(<span class="number">1</span>/((<span class="built_in">sqrt</span>(<span class="number">2</span>*<span class="built_in">pi</span>))*sigma1))*<span class="built_in">exp</span>(-((x1-a1).^<span class="number">2</span>)/(<span class="number">2</span>*sigma1.^<span class="number">2</span>));</span><br><span class="line">plot(x1,y1,<span class="string">'r'</span>);</span><br><span class="line">a2=<span class="number">2</span>;sigma2=<span class="number">2</span>;</span><br><span class="line">x2=<span class="number">-10</span>:<span class="number">0.0001</span>:<span class="number">10</span>;</span><br><span class="line">y2=(<span class="number">1</span>/((<span class="built_in">sqrt</span>(<span class="number">2</span>*<span class="built_in">pi</span>))*sigma2))*<span class="built_in">exp</span>(-((x2-a2).^<span class="number">2</span>)/(<span class="number">2</span>*sigma2.^<span class="number">2</span>));</span><br><span class="line">plot(x2,y2,<span class="string">'b'</span>);</span><br><span class="line">legend(<span class="string">'正常细胞类条件概率分布曲线'</span>,<span class="string">'异常细胞类条件概率分布曲线'</span>);</span><br><span class="line">title(<span class="string">'条件概率分布曲线'</span>);</span><br><span class="line">grid on</span><br></pre></td></tr></table></figure><h1 id="实验结果与讨论"><a href="#实验结果与讨论" class="headerlink" title="实验结果与讨论"></a>实验结果与讨论</h1><p>由于实验两部分修改的仅仅是分类器，所以正常细胞和异常细胞的类条件概率分布曲线图是完全一样的，如下图所示：</p><p><img src="/2018/10/08/《模式识别》实验2-贝叶斯分类器设计/3.png" alt=""></p><p>当然，由于分类器的不同，两者的后验概率曲线是完全不同的，如下图所示。可以看出， 样本-3.9934、-3.9847在错误率最小分类器中被分为“正常细胞”，在风险最小分类器中被分为“异常细胞”——风险最小分类器倾向于将细胞分类为异常细胞，这与常识相符。导致这一不同的主导因素显然是“风险”这一重要因素的引入。<br><img src="/2018/10/08/《模式识别》实验2-贝叶斯分类器设计/4.png" alt=""><img src="/2018/10/08/《模式识别》实验2-贝叶斯分类器设计/5.png" alt=""></p><h1 id="实验总结"><a href="#实验总结" class="headerlink" title="实验总结"></a>实验总结</h1><p>在本次实验中，我独立完成了错误率最小贝叶斯分类器和风险最小贝叶斯分类器的设计，并使用矩阵计算的方法进行优化。这一方面提高了我对于matlab的掌握程度，另一方面加深了我对于两种贝叶斯分类器的理解，这必将有利于我进一步的学习。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;实验目的&quot;&gt;&lt;a href=&quot;#实验目的&quot; class=&quot;headerlink&quot; title=&quot;实验目的&quot;&gt;&lt;/a&gt;实验目的&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;对模式识别有一个初步的理解&lt;/li&gt;
&lt;li&gt;能够根据自己的设计对贝叶斯决策理论算法有一个深刻地认识&lt;/li&gt;

      
    
    </summary>
    
      <category term="模式识别" scheme="http://wang22ti.com/categories/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>笔记-Anaconda</title>
    <link href="http://wang22ti.com/2018/09/25/%E7%AC%94%E8%AE%B0-Anaconda/"/>
    <id>http://wang22ti.com/2018/09/25/笔记-Anaconda/</id>
    <published>2018-09-25T15:53:06.000Z</published>
    <updated>2018-11-11T02:36:28.498Z</updated>
    
    <content type="html"><![CDATA[<p>最早学python的时候，他们都说用anaconda配置环境十分方便，适合初学者。然而那时我的电脑辣鸡，打开<code>anaconda navigator</code>要花很长时间 ，而且个人对于“环境”的理解又很糟糕，于是放弃了。</p><p>在浙大实习的时候，学长告诉我：有一些环境（即运行某些程序配套的程序）是相互冲突的，而anaconda采用沙盒的机制，即anaconda中的环境在不激活的情况下是不会被系统检测到的（即不会添加到环境变量），所以可以同时存在多个环境而又不冲突。</p><p>同时，anaconda不仅可以方便地切换环境，还可以对一个环境的python包进行很方便的管理，比如安装、更新、卸载等等。</p><p>嗯，真香！</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>就上<a href="https://www.anaconda.com/download/" target="_blank" rel="noopener">官网</a>咯，值得注意的是，python3和python2对应的anaconda的版本也是不一样的。</p><p>一路默认配置安装。唯一要注意的是，第一个勾是是否把Anaconda加入环境变量，这涉及到能否直接在cmd中使用conda、jupyter、ipython等命令，推荐打勾，如果不打勾话问题也不大，可以在之后使用Anaconda提供的命令行工具进行操作。</p><p><img src="/2018/09/25/笔记-Anaconda/0.png" alt=""></p><h1 id="对包的管理"><a href="#对包的管理" class="headerlink" title="对包的管理"></a>对包的管理</h1><p>安装好了之后，会在文件夹<code>Anaconda3</code>下面出现若干个软件，其中<code>Anaconda Prompt</code>是本体，一个命令行窗口；<code>Anaconda Navigtoer</code>是本体的图形化界面；其他的不用管，都是附带的IDE，而我一直觉得Pycharm是最好用的，也支持Anaconda的环境。</p><p>打开<code>Anaconda Prompt</code>后，会发现和普通的命令行相比，用户目录前面多了一个<code>base</code>，这就是在提示用户：你现在用的python环境叫做base！</p><p><img src="/2018/09/25/笔记-Anaconda/1.jpg" alt=""></p><p>base是anaconda默认配置的python环境，里面已经又很多包可以用了，使用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda list</span><br></pre></td></tr></table></figure><p>就可以查看有哪些。当然，以下的命令想来不需要解释了：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conda remove 包名</span><br><span class="line">conda update 包名</span><br><span class="line">conda env export &gt; environment.yaml // 导出当前环境的包信息</span><br><span class="line">conda env create -f environment.yaml // 用配置文件创建新的虚拟环境</span><br></pre></td></tr></table></figure><h1 id="在Pycharm中使用Anaconda的环境"><a href="#在Pycharm中使用Anaconda的环境" class="headerlink" title="在Pycharm中使用Anaconda的环境"></a>在Pycharm中使用Anaconda的环境</h1><p>在Pycharm工具栏<code>File-Settings for new project-Project Interpreter</code>界面点击<code>齿轮</code>按钮，再点击<code>Add</code>按钮会出现如下界面：</p><p><img src="/2018/09/25/笔记-Anaconda/2.jpg" alt=""></p><p>此时点击<code>Conda Environment</code>就会看到</p><p><img src="/2018/09/25/笔记-Anaconda/3.jpg" alt=""></p><p>要选择<code>Existing Environment</code>，并点击<code>...</code>，再选择编译器所在的位置就好啦，默认如下：</p><p><img src="/2018/09/25/笔记-Anaconda/4.jpg" alt=""></p><p>之后在Pycharm中新建的Project的编译器就是Anaconda默认的那个了~</p><h1 id="对环境的管理"><a href="#对环境的管理" class="headerlink" title="对环境的管理"></a>对环境的管理</h1><p>这个功能有什么用？为什么要安装多个python环境？首先，某些包可能在某些特定的python版本才存在；其次，某些包会相互冲突，比如CPU版本和GPU版本的tensorflow；最后，别人的代码可能需要某特定的环境才能运行。</p><p>第一步当然是新建一个环境：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n 环境名称 [必备包的列表] [python=版本号]</span><br></pre></td></tr></table></figure><p>之后是激活环境，成功后base会变成新的环境名：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">%</span><span class="bash"> windows</span></span><br><span class="line">activate 环境名</span><br><span class="line"><span class="meta">%</span><span class="bash"> Linux</span></span><br><span class="line">source activate 环境名</span><br></pre></td></tr></table></figure><p>离开环境也是很类似：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">%</span><span class="bash"> windows</span></span><br><span class="line">deactivate</span><br><span class="line"><span class="meta">%</span><span class="bash"> Linux</span></span><br><span class="line">source deactivate</span><br></pre></td></tr></table></figure><p>这两个不需要解释：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda env list</span><br><span class="line">conda env remove -n 环境名</span><br></pre></td></tr></table></figure><p>此外，环境可以导出和读取，从而确保项目在其他电脑上一定能跑：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda env export &gt; 文件名.yaml</span><br><span class="line">conda env create -f 文件名.yaml</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最早学python的时候，他们都说用anaconda配置环境十分方便，适合初学者。然而那时我的电脑辣鸡，打开&lt;code&gt;anaconda navigator&lt;/code&gt;要花很长时间 ，而且个人对于“环境”的理解又很糟糕，于是放弃了。&lt;/p&gt;
&lt;p&gt;在浙大实习的时候，学长告
      
    
    </summary>
    
      <category term="anaconda" scheme="http://wang22ti.com/categories/anaconda/"/>
    
    
  </entry>
  
  <entry>
    <title>笔记-《数学之美》</title>
    <link href="http://wang22ti.com/2018/09/02/%E7%AC%94%E8%AE%B0-%E3%80%8A%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E%E3%80%8B/"/>
    <id>http://wang22ti.com/2018/09/02/笔记-《数学之美》/</id>
    <published>2018-09-02T00:34:30.000Z</published>
    <updated>2018-09-02T02:43:12.955Z</updated>
    
    <content type="html"><![CDATA[<p>本来买这本书只是因为名字——“数学是美的！是有趣的！”袁岚峰老师在讲蓝眼睛岛问题（<a href="https://www.bilibili.com/video/av29527174" target="_blank" rel="noopener">上</a>）（<a href="https://www.bilibili.com/video/av30113899" target="_blank" rel="noopener">下</a>）的时候就这么呼吁。翻了翻目录，却是讲自然语言处理的，而且几乎以Google作为模板，然而这并不服算妨碍啦！</p><h1 id="文字和语言vs数字和信息"><a href="#文字和语言vs数字和信息" class="headerlink" title="文字和语言vs数字和信息"></a>文字和语言vs数字和信息</h1><blockquote><p>语言和数学的产生都是为了记录和传播信息，但是直到香农提出信息论，人们才将两者自觉的联系起来。</p></blockquote><h1 id="自然语言处理——从规则到统计"><a href="#自然语言处理——从规则到统计" class="headerlink" title="自然语言处理——从规则到统计"></a>自然语言处理——从规则到统计</h1><h1 id="统计语言模型"><a href="#统计语言模型" class="headerlink" title="统计语言模型"></a>统计语言模型</h1><h1 id="谈谈分词"><a href="#谈谈分词" class="headerlink" title="谈谈分词"></a>谈谈分词</h1><h1 id="隐含马尔可夫模型"><a href="#隐含马尔可夫模型" class="headerlink" title="隐含马尔可夫模型"></a>隐含马尔可夫模型</h1><h1 id="信息的度量和作用"><a href="#信息的度量和作用" class="headerlink" title="信息的度量和作用"></a>信息的度量和作用</h1><h1 id="贾里尼克和现代语言处理"><a href="#贾里尼克和现代语言处理" class="headerlink" title="贾里尼克和现代语言处理"></a>贾里尼克和现代语言处理</h1><h1 id="简单之美——布尔代数和搜索引擎"><a href="#简单之美——布尔代数和搜索引擎" class="headerlink" title="简单之美——布尔代数和搜索引擎"></a>简单之美——布尔代数和搜索引擎</h1><h1 id="图论和网络爬虫"><a href="#图论和网络爬虫" class="headerlink" title="图论和网络爬虫"></a>图论和网络爬虫</h1><h1 id="PageRank——Google的民主表决式网页排名技术"><a href="#PageRank——Google的民主表决式网页排名技术" class="headerlink" title="PageRank——Google的民主表决式网页排名技术"></a>PageRank——Google的民主表决式网页排名技术</h1><h1 id="如何确定网页和查询的相关性"><a href="#如何确定网页和查询的相关性" class="headerlink" title="如何确定网页和查询的相关性"></a>如何确定网页和查询的相关性</h1><h1 id="有限状态机和动态规划——地图与本地搜索的核心技术"><a href="#有限状态机和动态规划——地图与本地搜索的核心技术" class="headerlink" title="有限状态机和动态规划——地图与本地搜索的核心技术"></a>有限状态机和动态规划——地图与本地搜索的核心技术</h1><h1 id="Google-AK-47的设计者——阿米特·辛格博士"><a href="#Google-AK-47的设计者——阿米特·辛格博士" class="headerlink" title="Google AK-47的设计者——阿米特·辛格博士"></a>Google AK-47的设计者——阿米特·辛格博士</h1><h1 id="余弦定理与新闻分类"><a href="#余弦定理与新闻分类" class="headerlink" title="余弦定理与新闻分类"></a>余弦定理与新闻分类</h1><h1 id="矩阵运算和文本处理中的两个分类问题"><a href="#矩阵运算和文本处理中的两个分类问题" class="headerlink" title="矩阵运算和文本处理中的两个分类问题"></a>矩阵运算和文本处理中的两个分类问题</h1><h1 id="信息指纹及其应用"><a href="#信息指纹及其应用" class="headerlink" title="信息指纹及其应用"></a>信息指纹及其应用</h1><h1 id="由电视剧《暗算》所想到的——谈谈密码学的数学原理"><a href="#由电视剧《暗算》所想到的——谈谈密码学的数学原理" class="headerlink" title="由电视剧《暗算》所想到的——谈谈密码学的数学原理"></a>由电视剧《暗算》所想到的——谈谈密码学的数学原理</h1><h1 id="闪光的不一定是金子——谈谈搜索引擎反作弊问题和搜索结果的权威性问题"><a href="#闪光的不一定是金子——谈谈搜索引擎反作弊问题和搜索结果的权威性问题" class="headerlink" title="闪光的不一定是金子——谈谈搜索引擎反作弊问题和搜索结果的权威性问题"></a>闪光的不一定是金子——谈谈搜索引擎反作弊问题和搜索结果的权威性问题</h1><h1 id="谈谈数学模型的重要性"><a href="#谈谈数学模型的重要性" class="headerlink" title="谈谈数学模型的重要性"></a>谈谈数学模型的重要性</h1><h1 id="不要把鸡蛋装在一个笼子里——谈谈最大熵模型"><a href="#不要把鸡蛋装在一个笼子里——谈谈最大熵模型" class="headerlink" title="不要把鸡蛋装在一个笼子里——谈谈最大熵模型"></a>不要把鸡蛋装在一个笼子里——谈谈最大熵模型</h1><h1 id="拼音输入法的数学原理"><a href="#拼音输入法的数学原理" class="headerlink" title="拼音输入法的数学原理"></a>拼音输入法的数学原理</h1><h1 id="自然语言处理的教父马库斯和他的优秀弟子们"><a href="#自然语言处理的教父马库斯和他的优秀弟子们" class="headerlink" title="自然语言处理的教父马库斯和他的优秀弟子们"></a>自然语言处理的教父马库斯和他的优秀弟子们</h1><h1 id="布隆过滤器"><a href="#布隆过滤器" class="headerlink" title="布隆过滤器"></a>布隆过滤器</h1><h1 id="马尔可夫链的扩展——贝叶斯网络"><a href="#马尔可夫链的扩展——贝叶斯网络" class="headerlink" title="马尔可夫链的扩展——贝叶斯网络"></a>马尔可夫链的扩展——贝叶斯网络</h1><h1 id="条件随机场、文法分析和其他"><a href="#条件随机场、文法分析和其他" class="headerlink" title="条件随机场、文法分析和其他"></a>条件随机场、文法分析和其他</h1><h1 id="维比特和他的维比特算法"><a href="#维比特和他的维比特算法" class="headerlink" title="维比特和他的维比特算法"></a>维比特和他的维比特算法</h1><h1 id="上帝的算法——期望最大化算法"><a href="#上帝的算法——期望最大化算法" class="headerlink" title="上帝的算法——期望最大化算法"></a>上帝的算法——期望最大化算法</h1><h1 id="逻辑回归和搜索广告"><a href="#逻辑回归和搜索广告" class="headerlink" title="逻辑回归和搜索广告"></a>逻辑回归和搜索广告</h1><h1 id="各个击破算法和Google云计算基础"><a href="#各个击破算法和Google云计算基础" class="headerlink" title="各个击破算法和Google云计算基础"></a>各个击破算法和Google云计算基础</h1><h1 id="Google大脑和人工神经网络"><a href="#Google大脑和人工神经网络" class="headerlink" title="Google大脑和人工神经网络"></a>Google大脑和人工神经网络</h1><h1 id="大数据的威力——谈谈数据的重要性"><a href="#大数据的威力——谈谈数据的重要性" class="headerlink" title="大数据的威力——谈谈数据的重要性"></a>大数据的威力——谈谈数据的重要性</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本来买这本书只是因为名字——“数学是美的！是有趣的！”袁岚峰老师在讲蓝眼睛岛问题（&lt;a href=&quot;https://www.bilibili.com/video/av29527174&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;上&lt;/a&gt;）（&lt;a hre
      
    
    </summary>
    
      <category term="自然语言处理" scheme="http://wang22ti.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
  </entry>
  
  <entry>
    <title>关于保研的一些总结</title>
    <link href="http://wang22ti.com/2018/09/02/%E5%85%B3%E4%BA%8E%E4%BF%9D%E7%A0%94%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%BB%E7%BB%93/"/>
    <id>http://wang22ti.com/2018/09/02/关于保研的一些总结/</id>
    <published>2018-09-02T00:34:07.000Z</published>
    <updated>2018-10-12T15:49:39.268Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>北京交通大学计算机与信息技术学院2015级</p><p>2年半排名11/198，保研排名3/198，六级526</p><p>无国奖，美赛H奖，一堆校级奖项，</p><p>做过几个项目，一篇EI在投</p></blockquote><p>转眼已经是2018年10月12日，一切早已在9月28日尘埃落定，我也终于安下心情，记录一下这段时光。如果想了解一些细节，尽管找我吧。</p><h1 id="未名湖"><a href="#未名湖" class="headerlink" title="未名湖"></a>未名湖</h1><p>5月，我忽然意识到要联系老师了。</p><p>现在来看，那时已经很迟了，很多同学不经意间已经实习了很久。托徐老师的福，我很幸运的联系到了北大计算所的冯老师，专业也是我一直很感兴趣的自然语言处理。然而这之后就犯了一个战略性的大错误。</p><p>咨询过学长学姐（感谢何、吴、苏、才、顼、由、宋、高、叶几位学长学姐），要机试要面试，却盯着第11不放，一直死扣几门课程的大作业。</p><p>很快审判日到了，经过期末考试后几天的仓促准备（感谢佳丽），北大叉院的机试仅仅做出3题（poj难度），虽然勉勉强强进了50%，但是面试又是一塌糊涂，关于自然语言处理的最基本问题都无法回答，自然就GG了。其中凌晨3点发的面试通知邮件，几乎一宿没睡，内心的挣扎在此不述（后来才知道一般来说做出来1道就可以进）。</p><p>很惨淡的是，虽然进了软件所的夏令营，但是和北大信科又冲突了。虽然知道没什么机会，但是还是没有打开那心结，挣扎着身心，又去数媒组试了一下。马老师是我在北大见过少数慈善的老师，但是后来才知道，没有提前联系老师的我，实力不够强劲的我，在硕士名额减少60%的北大，是没有什么机会的。</p><p>忽然有一种理想幻灭的感觉。</p><h1 id="桑基韬"><a href="#桑基韬" class="headerlink" title="桑基韬"></a>桑基韬</h1><p>完全失去了夏令营的机会，我开始考虑本校，第一次认真的了解一些基本常识。</p><p><a href="https://www.zhihu.com/question/23679630/answer/233282268" target="_blank" rel="noopener">求详细介绍下院士、长江、百人、千人、万人、青千、杰青、各类学者人才计划等等？</a></p><p><a href="https://www.zhihu.com/question/66251104/answer/239964441" target="_blank" rel="noopener">现在科研项目中的重点项目、重大项目、重大研究计划项目，重点研发计划有什么区别和联系？</a></p><p>意外地发现了使我成为迷弟的桑老师。85年的教授，高高瘦瘦的，给我一种意气风发的学长的感觉。他很开心的告诉我是一个“相信眼缘的人”。其实我也是，要不是父母对于平台的要求，以及后来的际遇，我真的不愿意离开他。</p><h1 id="紫金港"><a href="#紫金港" class="headerlink" title="紫金港"></a>紫金港</h1><p>没几天，我就踏上了前往浙大实习的路，这只是当时偶然的报名。</p><p>一个月的时间，学到了很多东西，也认识了很多好朋友（舍友兼知己倪、一同实习的李、谢、金、宁、赵），当然也很感谢刘新国老师的指导（还有学长学姐们的帮助）。当然，还有友鑫的款待。</p><p>老师推荐我和金参加实习直博面试。英语口语一直是我的弱项，对实习项目的介绍当然是很糟糕了，还有自我介绍部分，完全俗套，没有足够的突出综合素质。总之，老师只能招收一个人，我怀着极端复杂的心情回了趟家。</p><p>垓下一战的感觉。</p><h1 id="九推"><a href="#九推" class="headerlink" title="九推"></a>九推</h1><p>第一天上午，计算所VIPL笔试，90%的淘汰率让大家望而生畏，一共就来了20人。但是数学是真的全忘了，，</p><p>下午，机试，虽然vs环境很麻烦，但幸好题目难度并不是很大，马马虎虎吧</p><p>晚上，做面试ppt，真的很匆忙，，</p><p>第二天上午，参加计算所体检</p><p>下午面试，这次总结了前几次的教训，突出个人品质并结合实例说明，，</p><p>王树徽老师和我开玩笑：照片那么精神，为啥真人那么憔悴？（那可是99元的照片啊）</p><p>之傍晚火急火燎地赶上去杭州的火车，终于在12点半躺在湿漉漉的青旅床单上。</p><p>中途收到卿老师的电话，问我去不去国科大计算机学院。</p><p>第三天上午，准备面试；下午倒是比较顺利，选的2个英文题目应该还算可以，对老师的方向也很了解，不过课程相关的问题依旧一塌糊涂。</p><p>下午又火急火燎地回到北京，京城初秋的午夜真是冷冷清清。</p><p>第4填上午，去校医院体检。心想，应该结束了吧？</p><h1 id="转机"><a href="#转机" class="headerlink" title="转机"></a>转机</h1><p>吃完午饭准备上楼睡觉，许老师忽然打电话：</p><p>黄老师在信工所有一个名额，不过以后都是在计算所学习，你来不来？</p><p>当然！火急火燎地前往香山，火急火燎地面试，火急火燎地见黄老师，</p><p>大佬的笑容，慈祥的神色，可能这就是期待已久的场景吧。</p><p>无心插柳柳成荫。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;北京交通大学计算机与信息技术学院2015级&lt;/p&gt;
&lt;p&gt;2年半排名11/198，保研排名3/198，六级526&lt;/p&gt;
&lt;p&gt;无国奖，美赛H奖，一堆校级奖项，&lt;/p&gt;
&lt;p&gt;做过几个项目，一篇EI在投&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;转
      
    
    </summary>
    
      <category term="杂记" scheme="http://wang22ti.com/categories/%E6%9D%82%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>联想miix5pro连接技嘉GamingBox1080</title>
    <link href="http://wang22ti.com/2018/08/28/%E8%81%94%E6%83%B3miix5pro%E8%BF%9E%E6%8E%A5%E6%8A%80%E5%98%89GamingBox1080/"/>
    <id>http://wang22ti.com/2018/08/28/联想miix5pro连接技嘉GamingBox1080/</id>
    <published>2018-08-28T12:30:50.000Z</published>
    <updated>2018-09-02T02:49:01.019Z</updated>
    
    <content type="html"><![CDATA[<h1 id="购买"><a href="#购买" class="headerlink" title="购买"></a>购买</h1><p>想用GPU加速机器学习任务，但是买1060及以上的笔记本又大又重，而且很多钱花在144Hz的屏幕、机械键盘等零件上；如果买台式机也得1万多，而且宿舍没有什么地方放，毫无便携的可能，思前想后，想起来miix5pro是有一个满血的雷电3接口，有着40Gbps的吞吐率，正好买一个外接显卡。</p><p>主要考虑<a href="https://item.jd.com/5685757.html?dist=jd" target="_blank" rel="noopener">惠普的外接盒子</a>和<a href="https://detail.tmall.com/item.htm?id=559309220532&amp;ns=1&amp;abbucket=14" target="_blank" rel="noopener">技嘉的Gaming box</a>（还有<a href="http://cn.razerzone.com/razer-core-v2" target="_blank" rel="noopener">雷蛇的盒子</a>但是太贵了）。两者价格差不多，技嘉的优势在于1080、便携、送雷电3数据线和鼠标，惠普的优势在于盒子和接口的扩展性。于是查了不少相关的测评：</p><p><a href="https://www.bilibili.com/video/av12205290?from=search&amp;seid=12560254674047576198" target="_blank" rel="noopener">【开箱拆解】史上最小外置显卡盒AORUS GTX 1070 GAMING BOX全网首发开箱拆解，评测稍后发~</a></p><p><a href="https://www.zhihu.com/question/63245941" target="_blank" rel="noopener">如何评价技嘉的gaming box显卡拓展？</a></p><p><a href="https://www.zhihu.com/question/39218012" target="_blank" rel="noopener">如何评价雷蛇的外置显卡Razer Core？</a></p><p><a href="https://post.smzdm.com/p/724215/" target="_blank" rel="noopener">HP 惠普 GA1-1007cl OMEN 暗影精灵 显卡扩展坞简单开箱</a></p><p><a href="https://www.chiphell.com/forum.php?mod=viewthread&amp;tid=1837389&amp;page=1#pid38624312" target="_blank" rel="noopener">【MacBook Pro 2016外接 GTX 1080显卡】- 技嘉 AORUS GTX 1080 Gaming Box</a></p><p><a href="https://zhuanlan.zhihu.com/p/33071814" target="_blank" rel="noopener">MacbookPro搭配技嘉外置显卡拓展坞AORUS GTX 1070 GamingBox使用感受，神经网络训练和简明安装教程</a></p><p>另外一个评论总结比较到位</p><p><img src="/2018/08/28/联想miix5pro连接技嘉GamingBox1080/6.png" alt=""></p><p>正纠结着，又看到一个评论</p><p><img src="/2018/08/28/联想miix5pro连接技嘉GamingBox1080/1.png" alt=""></p><p>嗯……那就买技嘉的吧，已经1080了，2、3年内并不会不够用。</p><h1 id="开箱"><a href="#开箱" class="headerlink" title="开箱"></a>开箱</h1><p><img src="/2018/08/28/联想miix5pro连接技嘉GamingBox1080/2.jpg" alt=""><br><img src="/2018/08/28/联想miix5pro连接技嘉GamingBox1080/3.jpg" alt=""><br><img src="/2018/08/28/联想miix5pro连接技嘉GamingBox1080/4.jpg" alt=""><br><img src="/2018/08/28/联想miix5pro连接技嘉GamingBox1080/5.jpg" alt=""></p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>由于重装过系统，所以直接插上并不能识别，所以要先安装雷电3的驱动，这是<a href="http://driverdl.lenovo.com.cn/lenovo/DriverFilesUploadFloder/52684/ThunderboltDriver-CZME06AF.exe" target="_blank" rel="noopener">miix5pro的</a>。实际上还有一个更好的方案更新所有驱动，就是安装<a href="https://downloadcenter.intel.com/" target="_blank" rel="noopener">Intel的升级助手</a></p><p>之后就可是识别雷电3啦，但是要识别显卡，还去<a href="https://www.nvidia.com/zh-cn/geforce/geforce-experience/" target="_blank" rel="noopener">英伟达官网</a>下载GEFORCE EXPERIENCE软件，它会提示安装对应的驱动并保持更新。安装之后可以使用控制面板设置在1080上跑的程序：</p><p><img src="/2018/08/28/联想miix5pro连接技嘉GamingBox1080/7.png" alt=""></p><p>注意不要安装技嘉的软件！！！我也不知道为什么，总之安装了之后，不仅设置显卡很慢，还会经常掉盘，体验极差，emmmmm</p><p>为了跑tensorflow，又折腾了一会，见<a href="http://wang22ti.com/2018/07/25/%E7%AC%94%E8%AE%B0-tensorflow/">tensorflow的笔记</a>。</p><h1 id="体验"><a href="#体验" class="headerlink" title="体验"></a>体验</h1><p>懒得跑分了，直接开了一局守望先锋，在极高的画质下人比较少时稳定在55帧以上，人很多很混乱的时候在45帧以上，作为二合一的本子，这样的性能已经让人很满意了，有着质的飞跃。</p><p>当然，由于低压CPU的限制，GPU图形部分并没有能够跑满，大概60-70%吧。此外如果作为鼠标的拓展坞，似乎流畅度不如直接插在电脑上。</p><p>装上CUDA之后跑tensorflow也没有问题，速度有明显的提高，没有亏本。</p><h1 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h1><p>不知道是显卡还是电脑的问题，主要出现了三个问题：</p><ol><li>连接上显卡后经常掉盘，即使是在跑神经网络的时候</li><li>有时连接不上显示器</li><li>鼠标接在显卡上会间歇性卡顿</li></ol><p>很扎心，于是联系客服寄过去修了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;购买&quot;&gt;&lt;a href=&quot;#购买&quot; class=&quot;headerlink&quot; title=&quot;购买&quot;&gt;&lt;/a&gt;购买&lt;/h1&gt;&lt;p&gt;想用GPU加速机器学习任务，但是买1060及以上的笔记本又大又重，而且很多钱花在144Hz的屏幕、机械键盘等零件上；如果买台式机也得1万多，
      
    
    </summary>
    
      <category term="杂记" scheme="http://wang22ti.com/categories/%E6%9D%82%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>mooc-Git实用教程</title>
    <link href="http://wang22ti.com/2018/08/13/mooc-Git%E5%AE%9E%E7%94%A8%E6%95%99%E7%A8%8B/"/>
    <id>http://wang22ti.com/2018/08/13/mooc-Git实用教程/</id>
    <published>2018-08-13T06:02:27.000Z</published>
    <updated>2018-08-14T02:41:45.404Z</updated>
    
    <content type="html"><![CDATA[<p>注册GitHub有一段时间了，一直用它托管博客，觉得还是应该学一下git的原理，至少是最基础的部分吧！看的是小甲鱼的教程，<a href="https://www.bilibili.com/video/av27780400?zw" target="_blank" rel="noopener">哔哩哔哩</a>和<a href="http://study.163.com/course/courseMain.htm?courseId=1003109018" target="_blank" rel="noopener">网易云课堂</a>都有。</p><h1 id="git是个什么玩意"><a href="#git是个什么玩意" class="headerlink" title="git是个什么玩意"></a>git是个什么玩意</h1><p>讲了什么是版本管理，git的历史，不做赘述</p><h1 id="git理论基础"><a href="#git理论基础" class="headerlink" title="git理论基础"></a>git理论基础</h1><h2 id="git的安装"><a href="#git的安装" class="headerlink" title="git的安装"></a>git的安装</h2><p>Windows/mac去<a href="https://git-scm.com/" target="_blank" rel="noopener">官网</a>下载，ubuntu使用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install git</span><br></pre></td></tr></table></figure><h2 id="git全局初始化"><a href="#git全局初始化" class="headerlink" title="git全局初始化"></a>git全局初始化</h2><p>git使用之前需要配置，让它知道主人是谁，注意不要使用中文</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name <span class="string">"****"</span></span><br><span class="line">git config --global user.email <span class="string">"****"</span></span><br></pre></td></tr></table></figure><p>之后使用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --list</span><br></pre></td></tr></table></figure><p>就可以查看相关信息了。</p><h2 id="git记录的是什么"><a href="#git记录的是什么" class="headerlink" title="git记录的是什么"></a>git记录的是什么</h2><p>常规版本的思路是记录每一次的更改：</p><p><img src="https://xxx.ilovefishc.com/forum/201604/21/190850u33aqh9ku93k3kz3.png" alt=""></p><p>但是git记录的是每个版本的：</p><p><img src="https://xxx.ilovefishc.com/forum/201604/21/190850p05ig3nhe56eh0gp.png" alt=""></p><p>本地仓库有 Git 维护的三棵“树”组成——工作区域、暂存区域和 Git 仓库 ，这是 Git 的核心框架。</p><p><code>工作区域（Working Directory）</code>就是本地存放项目代码的地方。    </p><p><code>暂存区域（Stage）</code>事实上只是一个文件（比如隐藏文件<code>.git</code>），保存即将提交的文件信息。    </p><p><code>仓库（Repository）</code>就是安全存放数据的位置，其中有提交的所有版本的数据。其中，HEAD 指向最新放入仓库的版本。   </p><p>Git 的工作流程一般是：    </p><ol><li>在工作目录中添加、修改文件；    </li><li>将需要进行版本管理的文件放入暂存区域；    </li><li>将暂存区域的文件提交到 Git 仓库。    </li></ol><p>因此，Git 管理的文件有三种状态：<code>已修改（modified）</code>、<code>已暂存（staged）</code>和<code>已提交（committed）</code>，依次对应上边的每一个流程。 </p><p><img src="https://xxx.ilovefishc.com/forum/201604/21/185430j73kd854krr3p58d.png" alt=""> </p><h2 id="git局部初始化"><a href="#git局部初始化" class="headerlink" title="git局部初始化"></a>git局部初始化</h2><p>在项目的根目录执行初始化后，就会出现暂存区域文件夹<code>.git</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure><p>在该目录新建一个README.txt文件后将之加入暂存区域</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add README.md</span><br></pre></td></tr></table></figure><p>之后将新的版本提交，其中<code>-m</code>表示注释信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m <span class="string">"add a readme file"</span></span><br></pre></td></tr></table></figure><p>在操作的过程中，显示提交成功如下</p><p><img src="/2018/08/13/mooc-Git实用教程/1.png" alt=""></p><p>然而我在GitHub的两个仓库中并没有发现文件的影子！仔细一想，这和GitHub应该还没啥关系，但究竟去哪里了呢？先占一个坑</p><p>其实还在本机，，终于明白了，，commit和push不是一个意思，，</p><h1 id="查看工作状态和历史提交"><a href="#查看工作状态和历史提交" class="headerlink" title="查看工作状态和历史提交"></a>查看工作状态和历史提交</h1><p>使用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git status</span><br></pre></td></tr></table></figure><p>可以查看当前状态如下，在默认的分支master中，没有需要提交的，工作树是干净的。</p><p><img src="/2018/08/13/mooc-Git实用教程/2.png" alt=""></p><p>加入一个MIT协议文件<code>LIENSE</code>如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Copyright (C) &lt;year&gt; &lt;copyright holders&gt;</span><br><span class="line"></span><br><span class="line">　　Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the &quot;Software&quot;), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</span><br><span class="line">　　</span><br><span class="line">　　The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</span><br><span class="line">THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</span><br></pre></td></tr></table></figure><p>这时再查看状态会提示有个一文件更改了但是还没有放入暂存区域</p><p><img src="/2018/08/13/mooc-Git实用教程/3.png" alt=""></p><p>将之add到缓存区域后再查看状态就会提示有一个新的文件可以被提交</p><p><img src="/2018/08/13/mooc-Git实用教程/4.png" alt=""></p><p>这时可以使用rest命令反悔，从而回到更改了但是没有放到暂存区域的状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset LISENCE</span><br></pre></td></tr></table></figure><p>重新add并commit后，又回到了没有需要提交的，工作树是干净的状态。修改LISENCE再查看状态，因为此时是工作目录文件和暂存区域文件不同，所以输出有所不同，第一个建议和之前相同，第二个会将暂存区域的文件覆盖掉工作目录修改后的文件。</p><p><img src="/2018/08/13/mooc-Git实用教程/5.png" alt=""></p><p>将之add到暂存区域后并commit，可以查看所有的版本信息，黄色的是唯一的版本ID。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span></span><br></pre></td></tr></table></figure><p><img src="/2018/08/13/mooc-Git实用教程/6.png" alt=""></p><h1 id="回到过去"><a href="#回到过去" class="headerlink" title="回到过去"></a>回到过去</h1><p>在3课树之间的转换有如下的示意图</p><p><img src="/2018/08/13/mooc-Git实用教程/7.png" alt=""></p><p>先退回到上一个版本，再查看日志就只有2个版本了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset HEAD~1</span><br></pre></td></tr></table></figure><p>该命令的选项如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git rest --mix HEAD~</span><br><span class="line">git rest --soft HEAD~</span><br><span class="line">git rest --hard HEAD~</span><br></pre></td></tr></table></figure><p>其中soft表示只影响仓库；mix是缺省值，表示仓库回到以前的版本后并用以前版本覆盖暂存区域；hard则更进一步，还要覆盖工作目录。</p><p>进一步地，还可以通过指定ID切换版本（一般只要输入前几位），比如回到最后一个版本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset --hard 82811</span><br></pre></td></tr></table></figure><h1 id="版本对比"><a href="#版本对比" class="headerlink" title="版本对比"></a>版本对比</h1><p>新建一个项目test2，其中有2个文件game.py和README.md，提交后修改两个文件。运行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git diff</span><br></pre></td></tr></table></figure><p>可以查看工作目录和暂存区域之间的区别（似乎有点崩），其中a是暂存区临时目录，b是工作区临时目录。</p><p><img src="/2018/08/13/mooc-Git实用教程/8.png" alt=""></p><p>绿色表示新增加的，白色表示共有的。如果文件很长，会有交互命令，用<code>h</code>可以查看帮助，在此不赘述。当然也可以比较两个版本的快照：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git diff id1 id2</span><br></pre></td></tr></table></figure><p>当前工作目录和某个版本的快照：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>当前工作目录和当前版本的快照：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git diff HEAD</span><br></pre></td></tr></table></figure><p>当前暂存区域和某个版本的快照：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git diff --cached [id]</span><br></pre></td></tr></table></figure><h1 id="修改最后一次提交、删除重命名文件"><a href="#修改最后一次提交、删除重命名文件" class="headerlink" title="修改最后一次提交、删除重命名文件"></a>修改最后一次提交、删除重命名文件</h1><p>如果提交后发现这个版本漏了一些东西，又不想制造一个新的版本，怎么办？使用如下命令即可，不需要回滚</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit --amend [-m <span class="string">"****"</span>]</span><br></pre></td></tr></table></figure><p>删除了已经add的文件后，可以恢复</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -- [filename]</span><br></pre></td></tr></table></figure><p>那如果想彻底删除已经提交的文件呢？因为有可能加入了不想加入的东西</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add *</span><br></pre></td></tr></table></figure><p>那么使用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git rm filename</span><br></pre></td></tr></table></figure><p>删除工作目录和暂存区内容后再软回滚版本。不过，如果工作目录和暂存区文件不一样，就会提示错误，这是要用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git rm -f filename</span><br></pre></td></tr></table></figure><p>当然也可以只删除暂存区域的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git rm --cached filename</span><br></pre></td></tr></table></figure><p>直接在系统中修改文件名会出错，需要使用命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git mv/ren oldfile newfile</span><br></pre></td></tr></table></figure><h1 id="创建和切换分支"><a href="#创建和切换分支" class="headerlink" title="创建和切换分支"></a>创建和切换分支</h1><p>优秀的分支管理是git的灵魂！分支的意义如下</p><p><img src="/2018/08/13/mooc-Git实用教程/9.png" alt=""></p><p>创建一个叫feature的分支</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch feature</span><br></pre></td></tr></table></figure><p>此时查看带有分支的日志</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --decorate --oneline</span><br></pre></td></tr></table></figure><p>切换分支名称</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout feature</span><br></pre></td></tr></table></figure><p>当然也可以直接创建并切换</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -b feature2</span><br></pre></td></tr></table></figure><p>在feature分支下做出修改并提交，再切换回master分支后，工作区和暂存区都回到master的状态！可以以图形化的方法看一下所有分支</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --decoreate --oneline --graph --all</span><br></pre></td></tr></table></figure><h1 id="合并和删除分支"><a href="#合并和删除分支" class="headerlink" title="合并和删除分支"></a>合并和删除分支</h1><p>将指定的分支合并到本分支</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git merge branchname</span><br></pre></td></tr></table></figure><p>但是如果有2个同名文件但内容不同，自动合并就会失败，git会在冲突的文件中打入标记，修改后提交就会完成自动合并。删除分支也很简单，其实只是删除了一个指针</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch -d brachname</span><br></pre></td></tr></table></figure><h1 id="匿名分支与checkout"><a href="#匿名分支与checkout" class="headerlink" title="匿名分支与checkout"></a>匿名分支与checkout</h1><p>创建匿名分支</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout HEAD~</span><br></pre></td></tr></table></figure><p>实际上就是把HEAD指针向前指了，但是又没有告诉名字，所以再切换到其他分支的时候，会丢掉所有的更改，这可以用来做实验。切换到别的分支的时候如果想保留改匿名分支，可以立刻用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch new_branch_name id</span><br></pre></td></tr></table></figure><p>如果错过了又忘记了id，就再也找不到了。</p><p>check和reset命令都可以恢复指定快照的指定文件，并且不影响HEAD指针。区别是reset只恢复到暂存区（不允许和soft、hard使用），checkout同时覆盖暂存区和工作目录。因此reset要比checkout安全一些。</p><p>check和reset命令都可以恢复指定快照，都是通过移动指针和覆盖文件产生的。第一个不同在于checkout会检查工作状态是不是clean，因此比较reset —hard更加安全。另一个区别是reset会移动HEAD及其分支指向，而checkout只会移动HEAD。</p><h1 id="创建GitHub账户"><a href="#创建GitHub账户" class="headerlink" title="创建GitHub账户"></a>创建GitHub账户</h1><p>小甲鱼的免费教程只有这些，可是又发现了<a href="https://blog.csdn.net/qq_36974281/article/details/81427893" target="_blank" rel="noopener">大佬的教程</a>，结合起来应该没问题了。我尝试进行了一次操作：</p><ol><li>用git clone url把我再GitHub创建的仓库克隆下来</li><li>用<code>git remote add repository_name url</code>将本地库和远程库关联</li><li>对工作目录进行一定修改，add并commit</li><li>用<code>git push</code>将本地仓库提交到GitHub，这时会弹出窗口让输入GitHub的账号密码，输入后就正确地提交了！</li></ol><p>可能还有情况没有遇到，之后再琢磨啦，何况还有一本git的书再吃灰呢。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;注册GitHub有一段时间了，一直用它托管博客，觉得还是应该学一下git的原理，至少是最基础的部分吧！看的是小甲鱼的教程，&lt;a href=&quot;https://www.bilibili.com/video/av27780400?zw&quot; target=&quot;_blank&quot; rel=&quot;
      
    
    </summary>
    
      <category term="git" scheme="http://wang22ti.com/categories/git/"/>
    
    
  </entry>
  
  <entry>
    <title>mooc-深度学习工程师-5-序列模型</title>
    <link href="http://wang22ti.com/2018/08/12/mooc-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%B8%88-5-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/"/>
    <id>http://wang22ti.com/2018/08/12/mooc-深度学习工程师-5-序列模型/</id>
    <published>2018-08-12T11:43:15.000Z</published>
    <updated>2018-08-25T07:43:51.411Z</updated>
    
    <content type="html"><![CDATA[<h1 id="循环序列模型"><a href="#循环序列模型" class="headerlink" title="循环序列模型"></a>循环序列模型</h1><h2 id="什么是序列数据"><a href="#什么是序列数据" class="headerlink" title="什么是序列数据"></a>什么是序列数据</h2><p><img src="/2018/08/12/mooc-深度学习工程师-5-序列模型/1.png" alt=""></p><h2 id="符号说明"><a href="#符号说明" class="headerlink" title="符号说明"></a>符号说明</h2><p>用$T_x^{(i)}$表示第$i$个样本的长度，$x^{(i)<t>}$表示第$i$个序列中的第$t$个单元。对于一个句子，首先用所有的单词制作一个一维词典，之后对于每一个词采用one-hot的方式编码。</t></p><h2 id="循环网络模型"><a href="#循环网络模型" class="headerlink" title="循环网络模型"></a>循环网络模型</h2><p>为什么不使用标准的神经网络呢？因为</p><ol><li>不同序列的长度不一定相同，即便使用填充的方法，这显然不是一个好的编码方法</li><li>无法共享在不同位置提取的特征，参数数量过多</li></ol><p>RNN解决了这个问题，</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;循环序列模型&quot;&gt;&lt;a href=&quot;#循环序列模型&quot; class=&quot;headerlink&quot; title=&quot;循环序列模型&quot;&gt;&lt;/a&gt;循环序列模型&lt;/h1&gt;&lt;h2 id=&quot;什么是序列数据&quot;&gt;&lt;a href=&quot;#什么是序列数据&quot; class=&quot;headerlink&quot; ti
      
    
    </summary>
    
      <category term="机器学习" scheme="http://wang22ti.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>mooc-深度学习工程师-4-卷积神经网络</title>
    <link href="http://wang22ti.com/2018/08/09/mooc-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%B8%88-4-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://wang22ti.com/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/</id>
    <published>2018-08-09T04:02:01.000Z</published>
    <updated>2018-08-24T09:29:32.246Z</updated>
    
    <content type="html"><![CDATA[<p>总的来说，这门课程比CS229容易一些，也可能是神经网络发展时间的问题，很多内容是经验上的，所以在差不多一周的学习之后，来到了最重要的部分啦。</p><h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><h2 id="计算机视觉"><a href="#计算机视觉" class="headerlink" title="计算机视觉"></a>计算机视觉</h2><p>由于神经网络的发展，<code>计算机视觉（computer vision）</code>的研究再近几年取得了突飞猛进的发展，也给其他与计算机交叉的领域带来很多灵感。计算机视觉的任务主要有：图像分类（image classification）、物体检测（object detection）、图像风格转换等等。计算机视觉的挑战主要来自于巨大的输入，一张1000*1000*3的图像有3百万维度的输入特征，如果神经网络的第一层是大小为1000全连接层，那么第一层权重就有30亿个，很难有足够的数据防止不过拟合，对硬件的要求也很高——而这并不能算很大的图片。</p><p>所以，计算机视觉必须进行卷积操作。</p><h2 id="卷积的定义"><a href="#卷积的定义" class="headerlink" title="卷积的定义"></a>卷积的定义</h2><p>卷积是一个应用广泛的定义，在图像处理中实际上就是——以<code>过滤器（filter）</code>/<code>核（kernel）</code>为权重，对原图像相同大小的区域进行加权求和；一次卷积实际只生成了一个像素点的值，对图像的卷积操作实际上是多次卷积操作结果的拼接。</p><p>网上有很多用信号等概念对卷积进行解释，比如<a href="https://blog.csdn.net/bitcarmanlee/article/details/54729807" target="_blank" rel="noopener">最容易理解的对卷积(convolution)的解释</a>和<a href="https://blog.csdn.net/panglinzhuo/article/details/75207855" target="_blank" rel="noopener">深度学习中的卷积与反卷积</a>。实际上他们可能并没有什么必然的实际意义上的联系，只是数学上的形式相同，毕竟不同的卷积核作用大相径庭；他们为了佐证实际意义上的联系都选用了恰好有对应意义的核。用于竖直边缘检测的核如下图所示，在数学上就是图片对应区域左边减去右边。在非边缘的区域，像素的值比较接近，在完全相同（比如都是255）的的情况下卷积的计算结果就为0；在边缘区域，像素值相差较大，左边减去右边的绝对值就很大——因此可以检测竖直边缘。</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/4.png" alt="1"></p><p>同理使用旋转180°的核就可以检测水平边缘。下面左图叫做Sobel filter，健壮性好于上面的那个；右图叫做Scharr filter；全是$\frac{1}{9}$的核可以对图片进行平滑处理。但是在卷积神经网络中，我们做的不是手动选择卷积核，而是通过梯度等手段让计算机选择卷积核，即选择提取的特征，这在之后的课程中会提到。</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/2.png" alt="1"><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/3.png" alt="1"></p><h2 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h2><p>对于大小为$n^2$的图像和核$f^2$的卷积核，得到的是大小为$(n-f+1)^2$的图片，这样有两个缺点，一个是每做一次卷积操作图片就会变小一点，另一个是边缘的像素点没有被充分采样。所以通过对原图像用0进行扩充到$(n+f-1)^2$维，即外层增加$p=\frac{f-1}{2},f=2k-1,k\in \N_+$圈。之所以是奇数主要是计算机视觉中的惯例，这样更方便自然的padding和用中心点标记核的位置。</p><p>根据padding的情况，可以定义两种卷积。<code>valid convolution</code>是完全没有padding，而<code>same convolution</code>保证了卷积后的图像大小和原图相同。</p><h2 id="步长（stride）"><a href="#步长（stride）" class="headerlink" title="步长（stride）"></a>步长（stride）</h2><p>步长$s$就是每次卷积核在图上移动的距离，最终输出图像的大小为$\lfloor\frac{n+2p-f}{s}+1\rfloor^2$，向下取整是为了所有的卷积核都是完全在图像上的。</p><p>此外，吴老师在这里解释了计算机的卷积和其他地方的卷积的区别。数学或信号学定义的卷积需要把卷积核做竖直和水平的反转，再进行之前操作，目的是可以使用结合律，但是这在神经网络中并不重要。在数学上，之前介绍的卷积常常被称为<code>互相换（cross correlation）</code>，不过在计算机领域还是习惯叫做卷积。</p><h2 id="3D卷积"><a href="#3D卷积" class="headerlink" title="3D卷积"></a>3D卷积</h2><p>所谓3D卷积是指对具有多个通道（又称深度）的图像进行卷积，此时卷积核的通道数和图像的通道数相同，卷积过程如下图所示</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/5.png" alt="1"></p><p>实际和2d的并没有什么不同，只不过每次计算量变大了一些。</p><h2 id="多个卷积核"><a href="#多个卷积核" class="headerlink" title="多个卷积核"></a>多个卷积核</h2><p>如果想要同时提取多个特征，就需要使用多个卷积核，分别对图像做卷积操作后堆叠在一起即可，如下图所示</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/6.png" alt="1"></p><p>即卷积得到的图像如果是有第3维的，第3维的大小表示的是提取的特征的数量，而非其他！</p><h2 id="单层卷积网络"><a href="#单层卷积网络" class="headerlink" title="单层卷积网络"></a>单层卷积网络</h2><p>有了上面的铺垫，终于可以搭建神经网络啦，如下图所示是一个单层神经网络，其中原图是输入$a^{[0]}$，过滤器是参数$W^{[1]}$，卷积操作相当于传统神经网络的矩阵乘法，通过激活函数后各个特征堆叠在一起的就是$a^{[1]}$。</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/7.png" alt="1"></p><p>举个例子，如果一层10个3×3×3的卷积核，一共又(3×3×3+1)×10个参数，这是输入图片的大小无关！这样就可以避免过拟合。于是，为了表示整个网络，就有超多的记号如下图</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/8.png" alt="1"></p><p>其中$m$是mini-batch的大小。</p><h2 id="简单CNN示例"><a href="#简单CNN示例" class="headerlink" title="简单CNN示例"></a>简单CNN示例</h2><p>一个简单的、完全由卷积层构成的神经网络如下图所示，在最后一个卷积层后通过平坦化提取到了1960个特征，再后面就是传统的神经网络了！注意到经过几层神经网络后，图片的大小逐渐减小，深度逐渐增大，一般是有这个趋势的。</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/9.png" alt="1"></p><p>除了卷积层，CNN通常还有<code>池化层（pooling layers）</code>和<code>全连接层（full connecting layers）</code>，可以使得网络更加强大。</p><h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><p>池化层通常用来缩减模型大小，提高计算速度，同时提高模型特征的健壮性。一般的池化层有两种，一个是<code>最大池化层（max pooling layer）</code>，如下图所示，和卷积很类似，不过是对选定范围内的值直接取最大值，可以理解为选择该范围中最突出的特征，过滤掉不重要的特征。</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/10.png" alt="1"></p><p>还有一个是<code>平均池化层（average pooling layer）</code>，即将取最大值的操作改为取平均值。平均池化层很少使用，比如将一个具有很深的输入转化为只有$1\times1\times n_c$的形式。</p><p>当然，池化层的深度要和输入的通道数相同。最重要的是池化层并没有任何要学习的参数，只是一个固定的转换。至于池化层的padding，基本上都是0，极少的特例会在之后讲解。</p><h2 id="典型CNN示例"><a href="#典型CNN示例" class="headerlink" title="典型CNN示例"></a>典型CNN示例</h2><p>以下的神经网络基于经典神经网络<code>LeNet-5</code>，注意到由于池化层没有超参数，所以连同前面的卷积层被认为是同一层。这里有大量的超参数，设计的思路会在之后讲解，不过常规的做法不是自己设计，而是从别人的论文中汲取经验。</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/11.png" alt="1"></p><p>观察这个典型神经网络的参数，可以发现：参数主要集中在全连接层；activation大小减小的速度不能太快；在卷积部分activation的长和宽逐渐减小而深度逐渐增大。很多CNN都有这样的模式。</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/12.png" alt="1"></p><h2 id="为什么使用CNN"><a href="#为什么使用CNN" class="headerlink" title="为什么使用CNN"></a>为什么使用CNN</h2><p>和全连接层相比，为什么卷积在神经网络中很有效呢？因为它有着<code>参数共享（parameter sharing）</code>和<code>稀疏连接（sparsity of connections）</code>两个特性，使得它减少了参数，在防止过拟合的同时加快了训练速度。参数共享是指一个卷积核中的参数被图片中的不同区域反复使用，从而用很少的参数对图片所有区域实现了同一特征的提取；稀疏连接是指卷积得到的每一个像素点都只和对应区域的值有关，和其他区域的值没有任何关系。此外，CNN很擅长捕捉<code>平移不变性（translation invariance）</code>，即相同物体在图片中平移几个像素，CNN仍然可以提取出十分相似的特征。</p><p>了解CNN的结构后，如何训练它也就很简单啦，在此不做赘述。</p><h1 id="卷积神经网络实例探究"><a href="#卷积神经网络实例探究" class="headerlink" title="卷积神经网络实例探究"></a>卷积神经网络实例探究</h1><h2 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h2><p>Lecun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to  document recognition[J]. Proceedings of the IEEE, 1998,  86(11):2278-2324. </p><p>那时没有padding的概念，大家更喜欢使用平均池化，激活函数用的是sigmoid和tanh，最后的输出用了一个很古老的分类器而非softmax，整个网络规模也不大。同时，由于当时计算机很慢，所以论文中用很复杂的推导使得过滤器的通道数和输入通道数可能并不一样——现在已经不使用了。此外，还在池化层增加了激活函数，这是很难理解的地方。吴老师给的示意图如下</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/14.png" alt="1"></p><p>吴老师推荐精读文章的第二部分，有网络结构的详细说明；泛读第三部分，有有趣的实验结果。论文中的示意图如下</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/13.png" alt="1"></p><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep  convolutional neural networks[C] International Conference on Neural  Information Processing Systems. Curran Associates Inc.  2012:1097-1105. </p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/15.png" alt="1"></p><p>AlexNet和LeNet非常相似，但是效果好得多，这篇论文也是深度学习在计算机视觉领域大规模应用的开端。效果好一方面是因为规模大得多，另一方面是因为使用了Relu函数。同时在几年前，GPU还没有那么快，文章中介绍了将模型拆分到两个GPU同时训练的方法。此外，文章中的网络还有<code>局部响应归一化层（local response normalization）</code>，不过后来发现对于效果的提升并不明显，所以现在很少使用了。原文的示意图如下</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/16.png" alt="1"></p><h2 id="VGG-16Net"><a href="#VGG-16Net" class="headerlink" title="VGG-16Net"></a>VGG-16Net</h2><p>Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition[J]. Computer Science, 2014. </p><p>这个网络十分地深邃，之所以名字中有16就是一共有16个卷积层核全连接层，一共有1.38亿个参数，这是最大的缺点。网络中所有的卷积层都是3×3的核，步长为1，same；所有的池化层都是2×2，步长为2的最大池化层；所以没有那么多超参数，最大的优势是简化了网络的结构。</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/17.png" alt="1"></p><p>此外还有VGG-19，比VGG16更加深邃，但是性能核VGG-16不相上下，所以大家通常使用VGG-16。</p><h2 id="残差网络（ResNet）"><a href="#残差网络（ResNet）" class="headerlink" title="残差网络（ResNet）"></a>残差网络（ResNet）</h2><p>He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[J].  2015:770-778. </p><p>很深的网络是很难训练的，因为存在梯度消失或者梯度爆炸，普通的网络训练误差和层数的关系如下图所示。</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/19.png" alt="1"></p><p>所以何大佬提出残差网络，其思想是加入直接向更深层传导梯度的路径。从此，巨深的网络变为可能，甚至可以超过100层。其基本结构<code>残差块（residual block）</code>如下图所示，其中的弧线被称为<code>short cut</code>或者<code>skip connection</code></p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/18.png" alt="1"></p><p>即有</p><script type="math/tex; mode=display">a^{[l+2]}=g(z^{[l+2]}+w_sa^{[l]})=g(w^{[l+2]}a^{[l+1]}+b^{[l+2]}+w_sa^{[l]})</script><p>其中$w_s$为了保证，相加的矩阵维度相同；不过由于残差网络基本使用same卷积，所以常常为单位矩阵。假设$l+1$层和$l+2$层是在原网络基础上增加的；如果他们的参数均为0，即它们什么都没有学习到，那么考虑到Relu函数的特性与$a$基本大于0，$a^{[l]}$直接传导到最后，并没有什么危害；如果新加的两层学习到了一些特征，那么网络的效果就会增强，这是残差块效果的基本理解。典型的plain网络转化为残差网络如下图所示，很明显是在VGG的基础上改进的。</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/20.png" alt="1"></p><h2 id="1×1的卷积核"><a href="#1×1的卷积核" class="headerlink" title="1×1的卷积核"></a>1×1的卷积核</h2><p>Lin M, Chen Q, Yan S. Network In Network[J]. Computer Science, 2013. </p><p>1×1卷积（又称<code>network in network</code>）看起来没什么作用，不过从另外一个角度来看，可以被认为是共用卷积核为权重的全连接网络。虽然在林敏的论文里的架构并没有得到广泛应用，但是1×1卷积却很有影响力，包括下节的Inception都受到它的启发。它的作用在于可以压缩或者增加通道的数量！</p><h2 id="Inception网络"><a href="#Inception网络" class="headerlink" title="Inception网络"></a>Inception网络</h2><p>Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C]// IEEE  Conference on Computer Vision and Pattern Recognition. IEEE, 2015:1-9. </p><p>当构建卷积层的时候，要设计卷积核的大小、需不需要池化层，而Inception的作用在于代替人做决定。虽然导致了网络更加复杂，但是表现却非常好。其核心原理如下图所示</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/21.png" alt="1"></p><p>它将左边的输入分别用不同大小的same卷积层和same池化层进行操作后拼接在一起，从而获得一个256通道的输出，由计算机决定各个卷积核的大小以及是否需要池化层。然而如果之后直接对深度为256的输入进行卷积操作，比如用32个5×5的same卷积核，那么浮点乘法计算量将到达1.2亿次！这是不能忍受的，所以还会加入一层1×1的卷积层，称为<code>瓶颈层（bootleneck layer）</code>从而减小计算量，如下图所示</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/22.png" alt="1"></p><p>浮点计算量下降到了1.24千万次。事实证明，只要合理地设计好1×1的卷积层，并不会降低网络的性能。一个典型的Inception组件如下图所示。</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/23.png" alt="1"></p><p>最后，典型的Inception网络十分地庞大。其中，黄色的分支是让隐藏层的输出也参与训练，从而避免过拟合。</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/24.png" alt="1"></p><h2 id="使用经典模型的建议"><a href="#使用经典模型的建议" class="headerlink" title="使用经典模型的建议"></a>使用经典模型的建议</h2><h3 id="使用开源的解决方案"><a href="#使用开源的解决方案" class="headerlink" title="使用开源的解决方案"></a>使用开源的解决方案</h3><p>由于神经网络的复杂性，它们很难被复制，即便是顶尖大学的学生也很难通过阅读他人的论文复制研究成果（而复制是进行进一步研究的第一步）。幸运的是很多深度学习研究者都习惯吧自己的成果开源到Github之类的网站上，所以吴老师也推荐我们把代码放到上面。而在我们阅读文献后，吴老师建议我们去网上找一个开源的实现（实际上吴老师自己也常常这么做），通常比从头开始要容易得多。</p><p>下面吴老师竟然实操如何下载GitHub的代码！他用了一个更高端的方法如下。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> URL</span><br></pre></td></tr></table></figure><h3 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h3><p>迁移学习的概念此前已经有所叙述。将开源网络下载下来后，根据自己的需要对其进行修改，建议是冻结保留的开源网络的参数，只训练自己加入的参数，这样可能用很小的数据集就可以获得很强性能——幸运的是很多数据集都支持这种操作，常见的是设置<code>trainableParamer=0</code>或<code>freeze=1</code>。另外一个方法是先用保留的网络计算训练集，将得到的特征存入硬盘，再用这些训练比如一个简单的softmax网络。</p><p>如果自己的数据集比较大，可以冻结更少的层，没冻结的层可以接着用，也可以去掉换为自己的网络。如果自己的数据集特别大，那就不要冻结啦。计算机视觉应该是用迁移学习最多的领域了。</p><h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>和其他领域不同，计算机视觉的数据集的大小总显得不足，因此需要扩充数据集。最简单的是镜像对称和随机裁剪，使用旋转、局部扭曲等方法也没有坏处，只是比较复杂。第二类方法是色彩转换，对RGB各通道进行一些操作，比较专业的有PCA采样。</p><p>一般是一个或多个现场进程数据的读取与增强，另一个线程进行训练。</p><h2 id="计算机视觉现状"><a href="#计算机视觉现状" class="headerlink" title="计算机视觉现状"></a>计算机视觉现状</h2><p>由于计算机视觉的复杂性，所以即便已经有很多数据集也显得不够，于是计算机视觉就更加依赖于hand-engineering，收集更多的数据、设计更复杂的模型等等。幸运的是我们还有迁移学习可以使用。</p><p>在竞赛中，为了在benchmark中脱颖而出，有以下两个建议，虽然不建议在工业界使用：</p><ol><li><code>集成（ensembling）</code>，独立训练多个神经网络并平均它们的输出。</li><li>在测试的时候使用<code>muti-crop</code>，即将一幅图及其镜面图取上下左右中共10个作为数据增强。</li></ol><p>使用开源资源的3个阶段：论文架构、架构的开源实现、迁移学习</p><h1 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h1><h2 id="目标定位"><a href="#目标定位" class="headerlink" title="目标定位"></a>目标定位</h2><p><code>图像分类（Image Classification）</code>是指给每张图片打上一个标签，比如车；<code>目标分类定位（Object Classification with localization）</code>是指不仅打上一个标签，还要用方框指出，比如车的具体位置；<code>目标检测（Object Detection）</code>是本周的重点，也是近几年得益于自动驾驶研究快速发展的技术，是指识别图片中的多个物体并指出其位置，例如图中所有的车、人、路灯等等。由于分类与定位是检测的基础，所以从前两者开始讲起。</p><p>我们已经很熟悉图像分类问题了，先卷积再全连接最后用softmax输出结果。在目标定位中，需要在结果中加入位置信息，可以使用$b_x,b_y,b_h,b_w$来描述目标位置，其含义如下图所示。</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/25.png" alt="1"></p><p>如果一共要分为3类，输出常见的形式如下所示</p><script type="math/tex; mode=display">y=\begin{bmatrix}  p_c \\ b_x\\ b_y\\ b_h\\ b_w \\ c_1 \\c_2 \\ c_3\end{bmatrix}</script><p>其中$p_c$指示是否有合理的分类，若$p_c=0$则其余的参数都没有任何意义。于是损失函数可以有如下的形式</p><script type="math/tex; mode=display">L(\hat{y},y)=y_1\sum_{i=1}^n{(\hat{y}_i-y_i)^2} + (1-y_1)(\hat{y}_1-y_1)^2</script><p>当然这只是举例，比如$p_c$也可以使用逻辑回归的形式。</p><h2 id="特征点检测"><a href="#特征点检测" class="headerlink" title="特征点检测"></a>特征点检测</h2><p>上一部分用方框表示物体的位置，完全可以推广为<code>特征点（landmark）</code>，比如人脸识别和<code>人体姿态检测（people pose detection）</code>：</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/26.png" alt="1"></p><h2 id="基于滑动窗口的目标检测"><a href="#基于滑动窗口的目标检测" class="headerlink" title="基于滑动窗口的目标检测"></a>基于滑动窗口的目标检测</h2><p>有了图像识别乃至目标定位的神经网络，我们可以使用不同大小的矩形作为窗口并选择一定的步长进行滑动操作，使用神经网络对每一次分割得到的图片进行分类或定位，从而实现目标检测。如下图所示</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/27.png" alt="1"></p><p>显然，这种方法计算量巨大。传统使用的简单的线性分类器，而在神经网络的时代，这似乎是无法忍受的。幸运的是，计算成本的问题已经有了很好的解决解决方案，大大提高了在CNN上应用滑动窗口方法的效率。</p><h2 id="滑动窗口的卷积实现"><a href="#滑动窗口的卷积实现" class="headerlink" title="滑动窗口的卷积实现"></a>滑动窗口的卷积实现</h2><p>Sermanet P, Eigen D, Zhang X, et al. OverFeat: Integrated Recognition,  Localization and Detection using Convolutional Networks[J]. Eprint  Arxiv, 2013. </p><p>全连接层是可以被卷积层代替的，只要卷积核和输入的大小完全相同即可：</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/28.png" alt="1"></p><p>于是将上面的神经网络训练好后输入一张较大的图：</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/29.png" alt="1"></p><p>同时输出了所有滑动窗口的结果！这就实现了加速。</p><h2 id="YOLO算法"><a href="#YOLO算法" class="headerlink" title="YOLO算法"></a>YOLO算法</h2><h3 id="边界矩形预测"><a href="#边界矩形预测" class="headerlink" title="边界矩形预测"></a>边界矩形预测</h3><p>Redmon J, Divvala S, Girshick R, et al. You only look once: Unified,  real-time object detection[C] Proceedings of the IEEE conference on  computer vision and pattern recognition. 2016: 779-788. </p><p>有了滑动窗口的卷积实现，结合目标定位的方法，就可以预测目标的<code>边界矩形（bounding box）</code>了，输出是一个3维的矩阵，通道数就是每个窗口的特征数，第一个通道$p_c$表示为某一分类的概率。这个算法被称为YOLO算法，由于采用卷积的方式，所以快到可以实时识别。此外，此时$b_x,b_y\in[0,1],b_h,b_w&gt;0$，因为边界矩阵可能不局限于一个窗口内。</p><p>实际上，原论文中的实现更加复杂，这里只是一个合理的方法。不过原论文的难度相当大，吴老师也是和其他大佬讨论后才搞明白的。</p><h3 id="交并比"><a href="#交并比" class="headerlink" title="交并比"></a>交并比</h3><p><code>交并比（intersection over union）</code>函数，即交集比上并集，可以用来改善上述网络的结果：</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/30.png" alt="1"></p><p>通常认为交并比大于0.5就是一个正确的预测，当然也可以另外设定。更广泛的，交并比可以衡量任意两个box的重叠程度。</p><h3 id="非极大值抑制"><a href="#非极大值抑制" class="headerlink" title="非极大值抑制"></a>非极大值抑制</h3><p>目前为止学习的算法都存在一个问题：同一个物体会被多次检测。典型的情况如下图所示：</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/31.png" alt="1"></p><p><code>非极大值抑制（non-max suppression）</code>可以解决这个问题其思路是：去除所有$p_c$过小（比如0.6以下）的边界矩形，然后重复选择$p_c$最大的矩形并删去与之有着较大（比如0.5以上）交并比的矩形直到所有矩形都被操作过。如果要检测多类物体，则需要对每一类物体单独执行一次算法。</p><h3 id="Anchor-boxes"><a href="#Anchor-boxes" class="headerlink" title="Anchor boxes"></a>Anchor boxes</h3><p>目前为止学习的算法都存在一个问题：一个窗口中只能检测出一个目标。典型的情况如下图所示：</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/32.png" alt="1"></p><p>设置anchor box可以在一定程度上解决这个问题。其思想是预先定义多个不同的anchor box，比如一个站着的（anchor box 1），一个躺着的（anchor box 2），然后将特征按照anchor box的数量复制，并将训练集重新组合如下所示</p><script type="math/tex; mode=display">y=\begin{bmatrix}  p_c \\ b_x\\ b_y\\ b_h\\ b_w \\ c_1 \\c_2 \\ c_3 \\ p_c \\ b_x\\ b_y\\ b_h\\ b_w \\ c_1 \\c_2 \\ c_3\end{bmatrix}=\begin{bmatrix}  1 \\ b_x\\ b_y\\ b_h\\ b_w \\ 1 \\ 0 \\ 0 \\ 1 \\ b_x\\ b_y\\ b_h\\ b_w \\ 0 \\ 1 \\ 0\end{bmatrix}</script><p>不过如果只定义2个anchor box，并不能处理1个窗口中出现3个及以上目标的情况，也不能处理有相同anchor box的2个目标。不过考虑到当每个窗口设置得足够小的时候，这些情况发生的概率还是蛮低的。设置anchor box的意义更大程度上在于指导网络box的形状。</p><p>怎么选择anchor box呢？一般是手动指定，多达5到10个。还有一个在论文中给出的更高级的版本，使用了k-means算法对box进行聚类。</p><h3 id="整合到一起"><a href="#整合到一起" class="headerlink" title="整合到一起"></a>整合到一起</h3><p>将上述内容整合到一起就是完整的YOLO算法，再次不做赘述。YOLO几乎是目前最好的检测算法，整合了很多模型中精妙的部分。</p><h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h2><p>Girshick R, Donahue J, Darrell T, et al. Rich feature hierarchies for  accurate object detection and semantic segmentation[C]//Proceedings of  the IEEE conference on computer vision and pattern recognition. 2014:  580-587. </p><p>在进行滑动窗口的时候，检测了很多其实并不会有什么的大块区域，所以<code>R-CNN（区域分割CNN，Region proposal CNN）</code>先将图像进行分割，之后再对相应的Window进行检测：</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/33.png" alt="1"></p><p>因为整个过程有两步，所以R-CNN是很慢的，于是有很多的改进版本，包括以下的Fast R-CNN和Faster R-CNN：</p><p>Girshick R. Fast r-cnn[C] Proceedings of the IEEE international conference on computer vision. 2015: 1440-1448. </p><p>Ren S, He K, Girshick R, et al. Faster r-cnn: Towards real-time object  detection with region proposal networks[C] Advances in neural  information processing systems. 2015: 91-99. </p><p>吴老师还是认为YOLO算法更有前景，所以这里只是简要的介绍了一下。但是R-CNN在学界还是有很大影响力的，所以还是需要关注的。</p><h1 id="人脸识别"><a href="#人脸识别" class="headerlink" title="人脸识别"></a>人脸识别</h1><h2 id="什么是人脸识别"><a href="#什么是人脸识别" class="headerlink" title="什么是人脸识别"></a>什么是人脸识别</h2><p>展示了百度的刷脸进入的视频（去京东的时候也有使用），其中涉及到了<code>人脸识别（face recognition）</code>和<code>活体检测（liveness detection）</code>。活体检测也可以使用监督学习完成，但不是本部分的重点。</p><p><code>人脸验证（face verification）</code>输入图片和ID，判断是否为一个人。人脸识别较之困难多，数据库中有K个人，对于给定的输入图片，要输出图片对应人的ID或者输出没有匹配信息。如果通过人脸验证实现人脸识别，那么就对验证的准确率有很高的要求。</p><h2 id="One-shot-学习问题"><a href="#One-shot-学习问题" class="headerlink" title="One-shot 学习问题"></a>One-shot 学习问题</h2><p>人脸识别难度很大程度来自于One-shot学习问题，即训练某个人的样本很可能只有一张。一种简单的方法是用softmax网络输出，但是由于样本实在太少所以效果并不好，而且有新人加入就要重新学习整个网络吗？这显然不是一个好方法。</p><p>比较好的解决方法是学习一个“相似”函数，即输入时两张图片，输出为两者的相似度或者差异度。当两张图片的差异度小于某个阈值$\tau$，就认为时同一个人；反之则不是。</p><h2 id="Siamese-network"><a href="#Siamese-network" class="headerlink" title="Siamese network"></a>Siamese network</h2><p>Taigman Y, Yang M, Ranzato M A, et al. Deepface: Closing the gap to  human-level performance in face verification[C] Proceedings of the IEEE conference on computer vision and pattern recognition. 2014: 1701-1708. </p><p>正常的神经网络会通过softmax输出图片分类，在Siamese网络中并不会这样做，其输出就是一个，比如128维的向量（embedding）。可以理解为这个神经网络是一个巨大的函数，它的作用就是将图片转化为向量，相似的图片对应的向量间的距离比较小，不相似的图片对应的向量间距离远。</p><h2 id="triplet-loss"><a href="#triplet-loss" class="headerlink" title="triplet loss"></a>triplet loss</h2><p>Schroff F, Kalenichenko D, Philbin J. Facenet: A unified embedding for  face recognition and clustering[C]//Proceedings of the IEEE conference  on computer vision and pattern recognition. 2015: 815-823. </p><p>如何定义上述网络的损失，才能将神经网络训练好呢？通常使用三元组损失，即对于任意一个人A，再找一张ta的图片P，和一张不是ta的图片N，如果将神经网络看作函数f，我们希望有</p><script type="math/tex; mode=display">||f(A)-f(P)||^2 + \alpha \leq ||f(A)-f(N)||^2</script><p>其中$\alpha$是一个超参数，为了避免所有的输出均为$\vec{0}$。于是就可以定义损失函数：</p><script type="math/tex; mode=display">L(A,P,N)=\max(||f(A)-f(P)||^2  - ||f(A)-f(N)||^2 + \alpha,0) \\ J=\sum_{i=1}^m{L(A^{(i)},P^{(i)},N^{(i)})}</script><p>不过，随机选择的三元组是没有用的，要选择足够有难度分辩的才行：</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/34.png" alt="1"></p><h2 id="人脸认证与二分类问题"><a href="#人脸认证与二分类问题" class="headerlink" title="人脸认证与二分类问题"></a>人脸认证与二分类问题</h2><p>上述模型可以改进为一个二分类问题，即将两幅图片分别输入网络，得到两个embedding后输入到一个逻辑回归的网络判别是否为同一个人。逻辑网络的训练用的损失函数很简单，比如</p><script type="math/tex; mode=display">\hat{y}=\sigma(w_i\sum_{k=1}^{128}{|f(x^{(i)})_k-f(x^{(j)})_k|}+b)</script><p>其中$x^i$是匹配的图片，$x^j$是数据库中的图片。或者是是$\kappa^2$相似度:</p><script type="math/tex; mode=display">\hat{y}=\sigma(w_i\sum_{k=1}^{128}{\frac{(f(x^{(i)})_k-f(x^{(j)})_k)^2}{f(x^{(i)})_k+f(x^{(j)})_k}}+b)</script><p>当然，为了计算的实时性，数据库中存储的不一定是图片，而是对应计算好的bedding。数据集如下所示：</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/35.png" alt="1"></p><h1 id="风格迁移"><a href="#风格迁移" class="headerlink" title="风格迁移"></a>风格迁移</h1><h2 id="什么是风格迁移"><a href="#什么是风格迁移" class="headerlink" title="什么是风格迁移"></a>什么是风格迁移</h2><p>如下图所示，不做赘述。</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/36.png" alt="1"></p><h2 id="深度学习网络究竟在学习什么"><a href="#深度学习网络究竟在学习什么" class="headerlink" title="深度学习网络究竟在学习什么"></a>深度学习网络究竟在学习什么</h2><p>Zeiler M D, Fergus R. Visualizing and  understanding convolutional networks[C] European conference on computer vision. Springer, Cham, 2014: 818-833. </p><p>通过对激活程度最高单元的可视化展示可知，神经网络的浅层往往学习的是线条、颜色、明暗等局部特征，而深层学习到的则是例如车轮、狗、人脸等高级的、全局的特征，如下图所示</p><p><img src="/2018/08/09/mooc-深度学习工程师-4-卷积神经网络/37.png" alt="1"></p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>Gatys L A, Ecker A S, Bethge M. A neural algorithm of artistic style[J]. arXiv preprint arXiv:1508.06576, 2015. </p><p>最初生成网络生成的图片是一副随机初始化的早点，之后使用梯度下降优化损失函数，使得Generated图片的内容逐渐靠近Content图片，而风格逐渐靠近Style图片，所以损失函数有如下的形式：</p><script type="math/tex; mode=display">J(G)=\alpha J_{content}(C,G)+\beta J_{style}(S,G)</script><p>虽然说一般只会用一个参数，但是为了和论文保持一致用了2个。</p><h3 id="内容损失函数"><a href="#内容损失函数" class="headerlink" title="内容损失函数"></a>内容损失函数</h3><p>根据深度学习网络学习的内容，我们假设在一个预训练好的网络，比如VGG网络中第$l$层表达了图形的内容，令$a^{<a href="C">l</a>}$和$a^{<a href="G">l</a>}$是第$l$层的输出，则令</p><script type="math/tex; mode=display">J_{content}(C,G)=\frac{1}{2}||a^{[l](C)}-a^{[l](G)}||^2</script><h3 id="风格损失函数"><a href="#风格损失函数" class="headerlink" title="风格损失函数"></a>风格损失函数</h3><p>内容比较好定义，那如何定义风格呢？我们把通道数总是称为特征数，如果通道A与通道B总是同时被激活，即某两种特征总是一起出现，那么就可以认为是一种风格。令$a_{i,j,k}^{[l]}$是第$l$层在通道$k$上位于$(i,j)$的输出，则定义第$l$层的风格矩阵$G^{[l]}(n_c^{[l]}\times n_c^{[l]})$：</p><script type="math/tex; mode=display">G_{kk'}^{[l]}=\sum_{i=1}^{n_H^{[l]}}\sum_{j=1}^{n_W^{[l]}}a_{ijk}^{[l]}a_{ijk'}^{[l]}</script><p>则有</p><script type="math/tex; mode=display">J_{style}^{[l]}(S,G)=||G^{[l](S)}-G^{[l](G)}||_F^2 = \sum_{k}\sum_{k'}(G_{kk'}^{[l](S)}-G_{kk'}^{[l](G)})^2</script><p>由于加入的层数越多模拟的风格越好，所以</p><script type="math/tex; mode=display"> J_{style}(S,G)=\sum_l{\lambda^{[l]}J_{style}^{[l]}(S,G)}</script><p>综述就可以完成风格迁移。</p><h1 id="一维和三维的卷积"><a href="#一维和三维的卷积" class="headerlink" title="一维和三维的卷积"></a>一维和三维的卷积</h1><p>其实理解了2维之后，1维和3维的情况就很容易啦。1维又称序列模型，比如心电图峰值数据，典型使用RNN处理，但是仍然有人在使用CNN处理。3维由骨骼CT图像和电影。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;总的来说，这门课程比CS229容易一些，也可能是神经网络发展时间的问题，很多内容是经验上的，所以在差不多一周的学习之后，来到了最重要的部分啦。&lt;/p&gt;
&lt;h1 id=&quot;卷积神经网络&quot;&gt;&lt;a href=&quot;#卷积神经网络&quot; class=&quot;headerlink&quot; title=&quot;卷
      
    
    </summary>
    
      <category term="机器学习" scheme="http://wang22ti.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>mooc-深度学习工程师-3-结构化机器学习项目</title>
    <link href="http://wang22ti.com/2018/08/06/mooc-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%B8%88-3-%E7%BB%93%E6%9E%84%E5%8C%96%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE/"/>
    <id>http://wang22ti.com/2018/08/06/mooc-深度学习工程师-3-结构化机器学习项目/</id>
    <published>2018-08-06T14:14:41.000Z</published>
    <updated>2018-08-09T03:54:57.965Z</updated>
    
    <content type="html"><![CDATA[<p>这一部分可能和<a href="http://wang22ti.com/2018/06/05/mooc-%E5%90%B4%E6%81%A9%E8%BE%BE%E8%80%81%E5%B8%88%E5%9C%A8%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%80%E8%AF%BE2%E2%80%94%E2%80%94%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/">学习理论</a>比较接近，但是听完后会发现十分的工程化、经验化。但是由于神经网络的复杂性，当模型表现不好时，有相当多可能的做法：</p><p><img src="/2018/08/06/mooc-深度学习工程师-3-结构化机器学习项目/1.png" alt="1"></p><p>如何选择呢？</p><h1 id="正交化"><a href="#正交化" class="headerlink" title="正交化"></a>正交化</h1><p>所谓<code>正交化（orthogonalization）</code>，就是要求改变参数的时候只有一项性能指标发生改变，例如在汽车中方向盘只改变方向，油门只提高速度，刹车只减速。在机器学习中，有四个性能指标：</p><div class="table-container"><table><thead><tr><th style="text-align:center">性能指标</th><th style="text-align:center">正交化方法</th></tr></thead><tbody><tr><td style="text-align:center">fit training set well on cost function</td><td style="text-align:center">更复杂的神经网络、更好的优化算法、换一个网络架构</td></tr><tr><td style="text-align:center">fit dev set well on cost function</td><td style="text-align:center">正则化、更大的训练集、换一个网络架构</td></tr><tr><td style="text-align:center">fit test set on cost function</td><td style="text-align:center">更大的开发集</td></tr><tr><td style="text-align:center">performs well in real world</td><td style="text-align:center">改变开发集或者损失函数</td></tr></tbody></table></div><p>由于early stopping不是一个正交化的方法，所以吴老师很少使用。</p><h1 id="单一数字评估指标"><a href="#单一数字评估指标" class="headerlink" title="单一数字评估指标"></a>单一数字评估指标</h1><p>当开始机器学习项目的时候，设置单一的评估指标是很有效的，只要观察该指标就知道模型是变好了还是变坏了。比如<code>查全率（precision）</code>和<code>查准率（recall）</code>的调和平均数，比如各个分指标的加权平均数。另外一个方法是将某一个指标作为优化目标，其他指标作为约束条件，建立一个简单的优化模型。</p><p>其中精确率和召回率的定义如下</p><p><img src="/2018/08/06/mooc-深度学习工程师-3-结构化机器学习项目/2.png" alt="1"></p><script type="math/tex; mode=display">Precision = \frac{TP}{TP+FP} \\ Recall = \frac{TP}{TP+FN}</script><h1 id="训练-开发-测试集划分（一）"><a href="#训练-开发-测试集划分（一）" class="headerlink" title="训练/开发/测试集划分（一）"></a>训练/开发/测试集划分（一）</h1><p>除了单一的指标，开发集的选择也是十分重要的，要保证训练集的分布和开发集是基本相同的。在机器学习时代，训练集和测试集的比例是7:3或者三者的比例是6:2:2。当数据比较少的时候是合理的，比如100到10000。但是在深度学习的时代需要更大的数据集，比例可能为98:1:1。由于测试集是用在系统开发完之后用以对系统性能进行评估的，所以大小要使得评估结果有足够的置信度。</p><p>如果发现评估指标和预期效果不一致的时候，需要改变开发/测试集或者指标。不一致的原因可能是有意想不到的额外因素，比如在对猫的分类中的色情图片；还有可能是测试集和开发集的分布意外的不同，比如一个是精美的专业的，另外一个是随意的业余的。</p><h1 id="和人的水平进行对比"><a href="#和人的水平进行对比" class="headerlink" title="和人的水平进行对比"></a>和人的水平进行对比</h1><p>比较机器和人的表现的原因为：机器学习的效果变好了，甚至在一些领域接近了人的水平；当机器的水平超过人类时，得益于其效率会有很好的产出。更重要的是，人在很多领域已经做得很好，十分接近理论最优（Bayes optimal），如下图所示</p><p><img src="/2018/08/06/mooc-深度学习工程师-3-结构化机器学习项目/3.png" alt="1"></p><p>之所以机器在超过人之后进步会变慢，一方面是因为人的水平已经十分接近贝叶斯误差了，另一方面更重要的是在此之后又很多本来可以使用的方法变得无效了，比如通过人的努力获得标注的训练数据、依靠人的观察力分析哪里还可以提高，方便的误差方差分析……根本来说，此前是依靠人的基础的、感性的智慧取得进步，而后的进步则来自人更加抽象、高级的智慧了。</p><p>其中，什么是方便的误差方差分析呢？如果将人的水平作为贝叶斯误差的近似，则人小于训练集的误差称为<code>可避免误差（avoidable error）</code>，将训练集小于开发集上的误差称为方差，则当可避免误差大于方差的时候，可以认为模型还不够好，误差大于方差；反之则方差大于误差。这在很多场合比直接将贝叶斯误差设为0要好得多。</p><p>要知道常言道，人与人的差距比人与狗的差距还大，人的哪一种水平可以作为贝叶斯误差的估计呢？一般人的业余水平？普通职业水平？高的职业水平？高水平职业团队水平？比如在医学骨骼鉴别中分别为3%以上、1%、0.7%、0.5%，于是我们知道贝叶斯误差一定小于0.5%，所以尽可能使用最小的误差作为人类水平。当然，这样的训练样本也很难得，很多论文中对于人类水平的定义也不一样，一般只要超过普通职业水平，就也可以被认为也是人类水平，这样的系统也具备了部署的价值。</p><p>如果机器已经超过了人了，该怎么办呢？此时无法得知是模型过拟合了还是人类水平没有达到贝叶斯误差的水平。如果此前规定的人类水平是高水平职业团队的，那么很难依靠直觉判断进一步优化的方向。这样的问题包括在线广告推荐、产品推荐、物流预测、偿款预测等等，而这些问题都是有海量结构化的数据的，没有自然感知。而在自然感知领域，机器还是很难超过人类的，实现的包括部分的语音识别、图像识别、医学任务。</p><h1 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h1><p>当模型还没有达到人的水平的时候，应该人工检查算法中犯的错误，这就是<code>误差分析（error analysis）</code>，它可以告诉我们这个错误是否值得被改进。比如一个识别猫的程序，在预测值为猫的图片里存在一定数量的狗，要不要花很长时间去设计排除狗的算法呢？我们需要手动地一张一张看开发集中被标记为猫的图片里有多少的狗，如果狗占了50%，那么是可能值得的，因为可以使误差一下下降一半；如果是5%，那么就要很慎重地考虑了。这类似于体系结构中的Amdahl定律，把精力花在最重要的事情上。</p><p>有时在做误差分析的时候可以同时并行评估几个想法。比如有多个改进识别猫的程序，剔除狗的影响、剔除大型猫科动物的影响、更好地处理模糊图像，可以画出下面的这张表：</p><p><img src="/2018/08/06/mooc-深度学习工程师-3-结构化机器学习项目/4.png" alt="1"></p><p>中途也可以加入新的列比如滤镜。最后很容易地，在这个例子中，应该优先优化模糊和大型猫科动物。</p><p>在误差分析中，如果发现有些训练样本中的标签是错误的该怎么办呢？由于深度学习算法对训练集中的随机误差具有相当好的健壮性，所以如果确认训练样本中的错误足够随机，比如偶然地敲错键盘，那么不管他可能也没什么问题。但是算法对于<code>系统性（systematic）</code>的误差就没有那么健壮了，比如某个人一直把白色的狗标记为猫。</p><p>如果错误是在开发集或者测试集上呢？那么就在上图中增加一列统计错误标记的比例，并增加一定的备注。如果发现这个比例已经影响到结果的评估了，即错误标记带来的误差在所有误差中的比例已经很大了，那么就去修正它。修正要同时在开发和测试集合进行，以确保它们是同分布的；同时无论算法是否预测正确的样本，都应该被修正，否则偏差可能更大；此外保持开发和测试集合同分布是很重要的，训练集是可以略有不同的，也是很常见的。</p><p>总之，要承认这种方法是有重大意义的，虽然没有什么技术含量，虽然十分无趣繁琐，但是确实要这么做，也不用羞于承认自己这么做。</p><h1 id="用快速原型-迭代的方法开发系统"><a href="#用快速原型-迭代的方法开发系统" class="headerlink" title="用快速原型+迭代的方法开发系统"></a>用快速原型+迭代的方法开发系统</h1><p>对于一个采用神经网络解决的问题，改进的方法实在是太多了，不可能面面俱到。所以一般的策略是</p><ol><li>设计开发/测试集以及单一的数字评估指标</li><li>建立快速简陋的模型</li><li>使用偏差/方差权衡以及误差分析确定主要因素</li><li>根据主要因素迭代改进</li></ol><p>如果在该领域十分有经验，那么原型可以稍微复杂一些；如果在该领域，准确的说是该问题已经有相当成熟的论文，那么可以更复杂一些。但是无论如何吴老师都不推荐第一次处理某问题就使用十分复杂的模型。大多数团队都是想得太复杂导致浪费了时间。</p><p>当然，如果目标不是解决某个问题，而是提出新的机器学习算法，就另当别论了，这是完全不同的目标。</p><h1 id="训练-开发-测试集划分（二）"><a href="#训练-开发-测试集划分（二）" class="headerlink" title="训练/开发/测试集划分（二）"></a>训练/开发/测试集划分（二）</h1><p>开发测试集的分布必须相同，而训练集和它们基本相同即可，并不需要完全相同。比如有20万张爬虫得到的图片，1万张用户上传的图片，正确的做法不是将它们完全混合打散后进行划分，而是留一部分用户的图片，比如5000张单独作为开发集和训练集，剩下的和所有爬虫得到的混合后作为训练集。</p><p>需不需要把收集到的数据都用掉？不一定。当在训练集表现良好而在开发集反之时，由于训练集的分布和开发集的分布不同，比如训练集都是清晰容易识别的图片而开发集都是模糊难识别的，所以此时并不能断言模型过拟合了。要弄清楚是过拟合还是分布不同，需要再定义一个<code>训练-开发集（Training-dev set）</code>，它的分布和训练集相同（即从训练集中随机抽取），但是不会用于训练，而是用来作为和开发集的对照组。在方差很大的情况下，如果训练-开发误差和训练误差相差很大，那么可以断言是过拟合了；反之则说明方差来自于训练集和开发集的分布不同，或者叫<code>数据不匹配（data mismatch）</code>。</p><p>所以最终关注的数据有人类水平误差、训练集误差、训练-开发集误差、开发误差、测试误差，在大多数情况下它们是递增的，相邻两项的差距分别表示了可避免偏差、方差、数据不匹配、开发集过拟合几个问题。在不同的情况下，牢记它们的定义和分布这个概念，分析起来不会吃力的。还可以结合下面这张表</p><p><img src="/2018/08/06/mooc-深度学习工程师-3-结构化机器学习项目/5.png" alt="1"></p><h1 id="处理数据不匹配"><a href="#处理数据不匹配" class="headerlink" title="处理数据不匹配"></a>处理数据不匹配</h1><p>如果误差主要是数据不匹配导致的，该如何处理呢？其实并无完全系统的方法，不过可以手动查看进行误差分析，搞清楚训练集和开发集之间的不同。比如语音后视镜的输入可能比一般语音输入有更多的地址，比室内的输入有更多的噪音。这样可以用各种方法，比如用<code>人工合成数据（artificial data synthesis）</code>加入噪音，使用生成对抗模型，有意识地收集更多相似数据，使得训练集更像开发集。</p><p>合成数据的时候需要注意的是，如果有1万小时的纯净语言和1小时的噪声，虽然人听起来没什么问题，但是模型很容易对噪音过拟合。同样的，用模型生成的车，或者一个游戏中出现的车来训练车的识别网络，也很有可能对那些车过拟合。</p><h1 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h1><p>当在开发智能后视镜的时候，如果已经有1万小时一般语音输入训练好的网络，而专门用于后视镜的语音只有1小时，怎么办呢？有人提出了<code>迁移学习（transfer learning）</code>，即利用已经训练好的网络对底层特征的把握，对其作修改后利用少量数据就可以完成模型的训练。如下图所示，将最后一层网络更换为了随机初始化的多层网络。</p><p><img src="/2018/08/06/mooc-深度学习工程师-3-结构化机器学习项目/6.png" alt="1"></p><p>如果新的数据很少，那么只训练新的最后的网络；反之可以对所有参数重新训练，对原网络的训练称为<code>预训练（pre-training）</code>，对新网络中原网络部分的训练称为微调<code>（fine tuning）</code>。</p><p>什么时候迁移学习是有效的呢？当然，任务A和任务B必须有相同的输入，比如图片或语音，其次任务A的数据比任务B的多得多，最后是任务A的特征对任务B是有意义的。由于单个任务A的数据对于任务B的价值一定小于任务B本身的数据，所以当没有满足第二个条件的时候，就不必如此劳神了。</p><h1 id="多任务学习"><a href="#多任务学习" class="headerlink" title="多任务学习"></a>多任务学习</h1><p>迁移学习的过程是串行的，在<code>多任务学习（muti-task learning）</code>中，单个神经网络同时进行多个任务，从每个任务学习到的知识都可以帮助其他任务的应用。比如在图像识别中要辨别有没有车、人、路牌、信号灯，正常是训练4个网络，但也可以只训练一个。该网络的输出是一个4维0-1向量，分别指示有没有对应的物体，和softmax不同的是可以判定同时出现了多个物体。其损失函数为</p><script type="math/tex; mode=display">\frac{1}{m}\sum_{i=1}^{m}{ \sum_{j=1}^4{L(\hat{y}_j^{(i)},y_j^{(i)})} }</script><p>其中$L$通常是用逻辑回归的交叉熵。由于识别这几个物体都需要使用几个十分底层的特征，所以同时训练4个任务反而比分别训练效果要好。如果一张图片只有部分标签，如有人无车，而不知道有没有路牌和信号灯怎么办呢？并不影响，求和中只对有标记的操作即可。总结起来，多任务学习使用的条件有：</p><ol><li>多个任务之间共享相同的底层特征</li><li>通常情况下，每个任务的数据量是相似的</li><li>可以训练一个可以处理所有任务的大型网络</li></ol><p>多任务学习和迁移学习相比并不那么常用，不过物体检测是个例外。</p><h1 id="端到端学习"><a href="#端到端学习" class="headerlink" title="端到端学习"></a>端到端学习</h1><p>近期神经网络最大的进展就是<code>端到端学习（end-to-end learning）</code>的兴起，简而言之以前一些任务需要多个环节才能完成，而现在用一个神经网络去代替（这让很多从事多年中间件的人很头大）。端到端学习的一个挑战是需要大量的数据。所以在数据量小的时候，传统的方法也是很好的。比如门禁系统先识别脸的位置，之后再用所有员工的脸与之比对；机器翻译中需要先提取文本特征，之后逐个转换并组合；从图片判断骨骼年龄需要分割每块骨骼，然后根据每一块的结果得到结论。</p><p>如何判断端到端学习是否适用呢？首先要理解端到端学习的优缺点。其优点包括</p><ol><li>真正的“让数据说话”，即完全依靠神经网络去提取统计特征，不加入任何人类的知识，尤其是成见。比如早期语音识别系统中强迫机器识别音节——这可能对人类语言学家来说是有意义的，但是可能对机器来说是没有意义的。</li><li>减少了组件的设计，简化了工作流程。</li></ol><p>其缺点包括：</p><ol><li>需要大量的端到端数据。</li><li>排除了一些的确很有用的人工设计的组件，即没有利用已有的人的智慧，虽然机器学习研究者都很鄙视手动设计的东西。但是神经网络的知识一方面来自数据，另一方面来自人；当有成吨的数据的时候，人的知识当然并不重要，不过在数据不大的时候，精心设计的组件确实可以将知识直接注入网络。</li></ol><p>于是，决定时候使用端到端学习的因素就是：是否有足够多的端到端数据，使得网络能够学到足够复杂的端到端映射。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这一部分可能和&lt;a href=&quot;http://wang22ti.com/2018/06/05/mooc-%E5%90%B4%E6%81%A9%E8%BE%BE%E8%80%81%E5%B8%88%E5%9C%A8%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%
      
    
    </summary>
    
      <category term="机器学习" scheme="http://wang22ti.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
</feed>
